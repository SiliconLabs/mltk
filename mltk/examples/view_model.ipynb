{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Visualizer API Examples\n",
    "\n",
    "This demonstrates how to use the [view_model](https://siliconlabs.github.io/mltk/docs/python_api/operations/view.html) API.\n",
    "\n",
    "Refer to the [Model Visualizer](https://siliconlabs.github.io/mltk/docs/guides/model_visualizer.html) guide for more details.\n",
    "\n",
    "__NOTES:__  \n",
    "- Refer to the [Notebook Examples Guide](https://siliconlabs.github.io/mltk/docs/guides/notebook_examples_guide.html) for how to run this example locally in VSCode \n",
    "- These APIs will _not_ work on a remote server as a local Python HTTP server is used to serve the interactive webpage\n",
    "- Alternatively, drag & drop your model into [http://netron.app](http://netron.app)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install MLTK Python Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the MLTK Python package (if necessary)\n",
    "!pip install --upgrade silabs-mltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary MLTK APIs\n",
    "from mltk.core import view_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: View Keras model\n",
    "\n",
    "In this example, we view the trained `.h5` model file in the \n",
    "[image_example1](https://siliconlabs.github.io/mltk/docs/python_api/models/examples/image_example1.html) model's [model archive](https://siliconlabs.github.io/mltk/docs/guides/model_archive.html).\n",
    "\n",
    "__NOTE:__ The model graph will appear in your web-browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'E:/reed/mltk/models/image_example1/extracted_archive/image_example1.h5' at http://localhost:8080\n",
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "view_model('image_example1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: View Tensorflow-Lite model\n",
    "\n",
    "In this example, we view the trained `.tflite` model file in the \n",
    "[image_example1](https://siliconlabs.github.io/mltk/docs/python_api/models/examples/image_example1.html) model's [model archive](https://siliconlabs.github.io/mltk/docs/guides/model_archive.html).\n",
    "\n",
    "__NOTE:__ The model graph will appear in your web-browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'E:/reed/mltk/models/image_example1/extracted_archive/image_example1.tflite' at http://localhost:8080\n",
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "view_model('image_example1', tflite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: View external Tensorflow-Lite model\n",
    "\n",
    "The given model need _not_ be generated by the MLTK. \n",
    "External models are also supported by the `view_model` API.\n",
    "\n",
    "__NOTE:__ The model graph will appear in your web-browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tempfile\n",
    "import urllib\n",
    "import shutil\n",
    "\n",
    "# Use .tflite mode found here:\n",
    "# https://github.com/mlcommons/tiny/tree/master/benchmark/training/keyword_spotting/trained_models\n",
    "# NOTE: Update this URL to point to your model if necessary\n",
    "TFLITE_MODEL_URL = 'https://github.com/mlcommons/tiny/raw/master/benchmark/training/keyword_spotting/trained_models/kws_ref_model.tflite'\n",
    "\n",
    "# Download the .tflite file and save to the temp dir\n",
    "external_tflite_path = os.path.normpath(f'{tempfile.gettempdir()}/kws_ref_model.tflite')\n",
    "with open(external_tflite_path, 'wb') as dst:\n",
    "    with urllib.request.urlopen(TFLITE_MODEL_URL) as src:\n",
    "        shutil.copyfileobj(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'E:/kws_ref_model.tflite' at http://localhost:8080\n",
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "view_model(external_tflite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: View model before training\n",
    "\n",
    "Training a model can be very time-consuming, and it is useful to view a \n",
    "model before investing time and energy into training it.  \n",
    "For this reason, the MLTK `view_model` API features a `build` argument to build a model\n",
    "and view it _before_ the model is fully trained.\n",
    "\n",
    "In this example, the [image_example1](https://siliconlabs.github.io/mltk/docs/python_api/models/examples/image_example1.html) model is built\n",
    "at api-execution-time and this file is opened in the viewer.  \n",
    "Note that _only_ the [model specification](https://siliconlabs.github.io/mltk/docs/guides/model_specification.html) script is required, \n",
    "it does _not_ need to be trained first.\n",
    "\n",
    "__NOTE:__ The model graph will appear in your web-browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting GPU : NVIDIA GeForce RTX 2060 (id=0)\n",
      "Enabling test mode\n",
      "training is using 1 subprocesses\n",
      "validation is using 1 subprocesses\n",
      "Model: \"image_example1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 24)        240       \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (None, 24, 24, 24)       0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 16)        3472      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 9, 9, 24)          3480      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 9, 9, 24)         96        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 9, 9, 24)          0         \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (None, 4, 4, 24)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 384)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 1155      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,443\n",
      "Trainable params: 8,395\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "\n",
      "Total MACs: 1.197 M\n",
      "Total OPs: 2.528 M\n",
      "Name: image_example1\n",
      "Version: 1\n",
      "Description: Image classifier example for detecting Rock/Paper/Scissors hand gestures in images\n",
      "Classes: rock, paper, scissor\n",
      "Training dataset: Found 9 samples belonging to 3 classes:\n",
      "      rock = 3\n",
      "     paper = 3\n",
      "   scissor = 3\n",
      "Validation dataset: Found 9 samples belonging to 3 classes:\n",
      "      rock = 3\n",
      "     paper = 3\n",
      "   scissor = 3\n",
      "Running cmd: c:\\Users\\reed\\workspace\\silabs\\mltk\\.venv\\Scripts\\python.exe -m pip install -U tensorflow-addons\n",
      "(This may take awhile, please be patient ...)\n",
      "Requirement already satisfied: tensorflow-addons in c:\\users\\reed\\workspace\\silabs\\mltk\\.venv\\lib\\site-packages (0.18.0)\n",
      "\n",
      "Requirement already satisfied: packaging in c:\\users\\reed\\workspace\\silabs\\mltk\\.venv\\lib\\site-packages (from tensorflow-addons) (21.3)\n",
      "\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\reed\\workspace\\silabs\\mltk\\.venv\\lib\\site-packages (from tensorflow-addons) (2.13.3)\n",
      "\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\reed\\workspace\\silabs\\mltk\\.venv\\lib\\site-packages (from packaging->tensorflow-addons) (3.0.9)\n",
      "\n",
      "WARNING: You are using pip version 21.2.3; however, version 22.2.2 is available.\n",
      "\n",
      "You should consider upgrading via the 'c:\\Users\\reed\\workspace\\silabs\\mltk\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n",
      "\n",
      "Forcing epochs=3 since test=true\n",
      "Class weights:\n",
      "   rock = 1.00\n",
      "  paper = 1.00\n",
      "scissor = 1.00\n",
      "Starting model training ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565336a826ef45579c4e9d629c52c544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|           0/3 ETA: ?s,  ?epochs/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f375c7807f4d198e784076ac346386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/3           ETA: ?s - "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692ce7cc4543465481ead5beeeb301dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/3           ETA: ?s - "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582da097aa804192872aa594831210fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/3           ETA: ?s - "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating C:/Users/reed/.mltk/models/image_example1-test/image_example1.test.h5\n",
      "\n",
      "\n",
      "*** Best training val_accuracy = 0.333\n",
      "\n",
      "\n",
      "Training complete\n",
      "Training logs here: C:/Users/reed/.mltk/models/image_example1-test\n",
      "validation is using 1 subprocesses\n",
      "Generating E:/reed/mltk/tmp_models/image_example1.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: E:\\tmpsa1z1ouz\\assets\n",
      "Using Tensorflow-Lite Micro version: b13b48c (2022-06-08)\n",
      "Searching for optimal runtime memory size ...\n",
      "Determined optimal runtime memory size to be 72320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reed\\workspace\\silabs\\mltk\\.venv\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'E:/reed/mltk/tmp_models/image_example1.tflite' at http://localhost:8080\n",
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "view_model('image_example1', tflite=True, build=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "600e22ae316f8c315f552eaf99bb679bc9438a443c93affde9ac001991b79c8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
