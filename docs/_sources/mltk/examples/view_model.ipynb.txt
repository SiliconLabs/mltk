{

 "cells": [

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "# Model Visualizer API Examples\n",

    "\n",

    "This demonstrates how to use the [view_model](../../docs/python_api/operations/view.md) API.\n",

    "\n",

    "Refer to the [Model Visualizer](../../docs/guides/model_visualizer.md) guide for more details.\n",

    "\n",

    "__NOTES:__  \n",

    "- Refer to the [Notebook Examples Guide](../../docs/guides/notebook_examples_guide.md) for how to run this example locally in VSCode \n",

    "- These APIs will _not_ work on a remote server as a local Python HTTP server is used to serve the interactive webpage\n",

    "- Alternatively, drag & drop your model into [http://netron.app](http://netron.app)  "

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Install MLTK Python Package"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 1,

   "metadata": {},

   "outputs": [],

   "source": [

    "# Install the MLTK Python package (if necessary)\n",

    "!pip install --upgrade silabs-mltk"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Import Python Packages"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 1,

   "metadata": {},

   "outputs": [],

   "source": [

    "# Import the necessary MLTK APIs\n",

    "from mltk.core import view_model"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Example 1: View Keras model\n",

    "\n",

    "In this example, we view the trained `.h5` model file in the \n",

    "[image_example1](../../docs/python_api/models/examples/image_example1.md) model's [model archive](../../docs/guides/model_archive.md).\n",

    "\n",

    "__NOTE:__ The model graph will appear in your web-browser."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 3,

   "metadata": {},

   "outputs": [

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Serving 'E:/reed/mltk/models/image_example1/extracted_archive/image_example1.h5' at http://localhost:8080\n",

      "Stopping http://localhost:8080\n"

     ]

    }

   ],

   "source": [

    "view_model('image_example1')"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Example 2: View Tensorflow-Lite model\n",

    "\n",

    "In this example, we view the trained `.tflite` model file in the \n",

    "[image_example1](../../docs/python_api/models/examples/image_example1.md) model's [model archive](../../docs/guides/model_archive.md).\n",

    "\n",

    "__NOTE:__ The model graph will appear in your web-browser."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 4,

   "metadata": {},

   "outputs": [

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Serving 'E:/reed/mltk/models/image_example1/extracted_archive/image_example1.tflite' at http://localhost:8080\n",

      "Stopping http://localhost:8080\n"

     ]

    }

   ],

   "source": [

    "view_model('image_example1', tflite=True)"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Example 3: View external Tensorflow-Lite model\n",

    "\n",

    "The given model need _not_ be generated by the MLTK. \n",

    "External models are also supported by the `view_model` API.\n",

    "\n",

    "__NOTE:__ The model graph will appear in your web-browser."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 5,

   "metadata": {},

   "outputs": [],

   "source": [

    "import os \n",

    "import tempfile\n",

    "import urllib\n",

    "import shutil\n",

    "\n",

    "# Use .tflite mode found here:\n",

    "# https://github.com/mlcommons/tiny/tree/master/benchmark/training/keyword_spotting/trained_models\n",

    "# NOTE: Update this URL to point to your model if necessary\n",

    "TFLITE_MODEL_URL = 'https://github.com/mlcommons/tiny/raw/master/benchmark/training/keyword_spotting/trained_models/kws_ref_model.tflite'\n",

    "\n",

    "# Download the .tflite file and save to the temp dir\n",

    "external_tflite_path = os.path.normpath(f'{tempfile.gettempdir()}/kws_ref_model.tflite')\n",

    "with open(external_tflite_path, 'wb') as dst:\n",

    "    with urllib.request.urlopen(TFLITE_MODEL_URL) as src:\n",

    "        shutil.copyfileobj(src, dst)"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 6,

   "metadata": {},

   "outputs": [

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Serving 'E:/kws_ref_model.tflite' at http://localhost:8080\n",

      "Stopping http://localhost:8080\n"

     ]

    }

   ],

   "source": [

    "view_model(external_tflite_path)"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Example 4: View model before training\n",

    "\n",

    "Training a model can be very time-consuming, and it is useful to view a \n",

    "model before investing time and energy into training it.  \n",

    "For this reason, the MLTK `view_model` API features a `build` argument to build a model\n",

    "and view it _before_ the model is fully trained.\n",

    "\n",

    "In this example, the [image_example1](../../docs/python_api/models/examples/image_example1.md) model is built\n",

    "at api-execution-time and this file is opened in the viewer.  \n",

    "Note that _only_ the [model specification](../../docs/guides/model_specification.md) script is required, \n",

    "it does _not_ need to be trained first.\n",

    "\n",

    "__NOTE:__ The model graph will appear in your web-browser."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 7,

   "metadata": {},

   "outputs": [

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Selecting GPU : NVIDIA GeForce RTX 2060 (id=0)\n",

      "Enabling test mode\n",

      "training is using 1 subprocesses\n",

      "validation is using 1 subprocesses\n",

      "Model: \"image_example1\"\n",

      "_________________________________________________________________\n",

      " Layer (type)                Output Shape              Param #   \n",

      "=================================================================\n",

      " conv2d (Conv2D)             (None, 48, 48, 24)        240       \n",

      "                                                                 \n",

      " average_pooling2d (AverageP  (None, 24, 24, 24)       0         \n",

      " ooling2D)                                                       \n",

      "                                                                 \n",

      " conv2d_1 (Conv2D)           (None, 11, 11, 16)        3472      \n",

      "                                                                 \n",

      " conv2d_2 (Conv2D)           (None, 9, 9, 24)          3480      \n",

      "                                                                 \n",

      " batch_normalization (BatchN  (None, 9, 9, 24)         96        \n",

      " ormalization)                                                   \n",

      "                                                                 \n",

      " activation (Activation)     (None, 9, 9, 24)          0         \n",

      "                                                                 \n",

      " average_pooling2d_1 (Averag  (None, 4, 4, 24)         0         \n",

      " ePooling2D)                                                     \n",

      "                                                                 \n",

      " flatten (Flatten)           (None, 384)               0         \n",

      "                                                                 \n",

      " dense (Dense)               (None, 3)                 1155      \n",

      "                                                                 \n",

      " activation_1 (Activation)   (None, 3)                 0         \n",

      "                                                                 \n",

      "=================================================================\n",

      "Total params: 8,443\n",

      "Trainable params: 8,395\n",

      "Non-trainable params: 48\n",

      "_________________________________________________________________\n",

      "\n",

      "Total MACs: 1.197 M\n",

      "Total OPs: 2.528 M\n",

      "Name: image_example1\n",

      "Version: 1\n",

      "Description: Image classifier example for detecting Rock/Paper/Scissors hand gestures in images\n",

      "Classes: rock, paper, scissor\n",

      "Training dataset: Found 9 samples belonging to 3 classes:\n",

      "      rock = 3\n",

      "     paper = 3\n",

      "   scissor = 3\n",

      "Validation dataset: Found 9 samples belonging to 3 classes:\n",

      "      rock = 3\n",

      "     paper = 3\n",

      "   scissor = 3\n",

      "Running cmd: c:\\Users\\reed\\workspace\\silabs\\mltk\\.venv\\Scripts\\python.exe -m pip install -U tensorflow-addons\n",

      "(This may take awhile, please be patient ...)\n",

      "Requirement already satisfied: tensorflow-addons in c:\\users\\reed\\workspace\\silabs\\mltk\\.venv\\lib\\site-packages (0.18.0)\n",

      "\n",

      "Requirement already satisfied: packaging in c:\\users\\reed\\workspace\\silabs\\mltk\\.venv\\lib\\site-packages (from tensorflow-addons) (21.3)\n",

      "\n",

      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\reed\\workspace\\silabs\\mltk\\.venv\\lib\\site-packages (from tensorflow-addons) (2.13.3)\n",

      "\n",

      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\reed\\workspace\\silabs\\mltk\\.venv\\lib\\site-packages (from packaging->tensorflow-addons) (3.0.9)\n",

      "\n",

      "WARNING: You are using pip version 21.2.3; however, version 22.2.2 is available.\n",

      "\n",

      "You should consider upgrading via the 'c:\\Users\\reed\\workspace\\silabs\\mltk\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n",

      "\n",

      "Forcing epochs=3 since test=true\n",

      "Class weights:\n",

      "   rock = 1.00\n",

      "  paper = 1.00\n",

      "scissor = 1.00\n",

      "Starting model training ...\n"

     ]

    },

    {

     "data": {

      "application/vnd.jupyter.widget-view+json": {

       "model_id": "565336a826ef45579c4e9d629c52c544",

       "version_major": 2,

       "version_minor": 0

      },

      "text/plain": [

       "Training:   0%|           0/3 ETA: ?s,  ?epochs/s"

      ]

     },

     "metadata": {},

     "output_type": "display_data"

    },

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Epoch 1/3\n"

     ]

    },

    {

     "data": {

      "application/vnd.jupyter.widget-view+json": {

       "model_id": "92f375c7807f4d198e784076ac346386",

       "version_major": 2,

       "version_minor": 0

      },

      "text/plain": [

       "0/3           ETA: ?s - "

      ]

     },

     "metadata": {},

     "output_type": "display_data"

    },

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Epoch 2/3\n"

     ]

    },

    {

     "data": {

      "application/vnd.jupyter.widget-view+json": {

       "model_id": "692ce7cc4543465481ead5beeeb301dc",

       "version_major": 2,

       "version_minor": 0

      },

      "text/plain": [

       "0/3           ETA: ?s - "

      ]

     },

     "metadata": {},

     "output_type": "display_data"

    },

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Epoch 3/3\n"

     ]

    },

    {

     "data": {

      "application/vnd.jupyter.widget-view+json": {

       "model_id": "582da097aa804192872aa594831210fb",

       "version_major": 2,

       "version_minor": 0

      },

      "text/plain": [

       "0/3           ETA: ?s - "

      ]

     },

     "metadata": {},

     "output_type": "display_data"

    },

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Generating C:/Users/reed/.mltk/models/image_example1-test/image_example1.test.h5\n",

      "\n",

      "\n",

      "*** Best training val_accuracy = 0.333\n",

      "\n",

      "\n",

      "Training complete\n",

      "Training logs here: C:/Users/reed/.mltk/models/image_example1-test\n",

      "validation is using 1 subprocesses\n",

      "Generating E:/reed/mltk/tmp_models/image_example1.tflite\n"

     ]

    },

    {

     "name": "stderr",

     "output_type": "stream",

     "text": [

      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"

     ]

    },

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "INFO:tensorflow:Assets written to: E:\\tmpsa1z1ouz\\assets\n",

      "Using Tensorflow-Lite Micro version: b13b48c (2022-06-08)\n",

      "Searching for optimal runtime memory size ...\n",

      "Determined optimal runtime memory size to be 72320\n"

     ]

    },

    {

     "name": "stderr",

     "output_type": "stream",

     "text": [

      "c:\\Users\\reed\\workspace\\silabs\\mltk\\.venv\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",

      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"

     ]

    },

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Serving 'E:/reed/mltk/tmp_models/image_example1.tflite' at http://localhost:8080\n",

      "Stopping http://localhost:8080\n"

     ]

    }

   ],

   "source": [

    "view_model('image_example1', tflite=True, build=True)"

   ]

  }

 ],

 "metadata": {

  "kernelspec": {

   "display_name": "Python 3.9.7 ('.venv': venv)",

   "language": "python",

   "name": "python3"

  },

  "language_info": {

   "codemirror_mode": {

    "name": "ipython",

    "version": 3

   },

   "file_extension": ".py",

   "mimetype": "text/x-python",

   "name": "python",

   "nbconvert_exporter": "python",

   "pygments_lexer": "ipython3",

   "version": "3.9.7"

  },

  "orig_nbformat": 4,

  "vscode": {

   "interpreter": {

    "hash": "600e22ae316f8c315f552eaf99bb679bc9438a443c93affde9ac001991b79c8f"

   }

  }

 },

 "nbformat": 4,

 "nbformat_minor": 2

}

