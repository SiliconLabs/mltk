{

 "cells": [

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "# Model Visualizer API Examples\n",

    "\n",

    "This demonstrates how to use the [view_model](../../docs/python_api/operations.html#view-model) API.\n",

    "\n",

    "Refer to the [Model Visualizer](../../docs/guides/model_visualizer.md) guide for more details.\n",

    "\n",

    "__NOTES:__  \n",

    "- Refer to the [Notebook Examples Guide](../../docs/guides/notebook_examples_guide.md) for how to run this example locally in VSCode \n",

    "- These APIs will _not_ work on a remote server as a local Python HTTP server is used to serve the interactive webpage\n",

    "- Alternatively, drag & drop your model into [http://netron.app](http://netron.app)  "

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Install MLTK Python Package"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 1,

   "metadata": {},

   "outputs": [],

   "source": [

    "# Install the MLTK Python package (if necessary)\n",

    "!pip install --upgrade silabs-mltk"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Import Python Packages"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 2,

   "metadata": {},

   "outputs": [],

   "source": [

    "# Import the necessary MLTK APIs\n",

    "from mltk.core import view_model"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Example 1: View Keras model\n",

    "\n",

    "In this example, we view the trained `.h5` model file in the \n",

    "[image_example1](../../docs/python_api/models/examples/image_example1.md) model's [model archive](../../docs/guides/model_archive.md).\n",

    "\n",

    "__NOTE:__ The model graph will appear in your web-browser."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 3,

   "metadata": {},

   "outputs": [

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Serving 'E:/reed/mltk/models/image_example1/extracted_archive/image_example1.h5' at http://localhost:8080\n",

      "Stopping http://localhost:8080\n"

     ]

    }

   ],

   "source": [

    "view_model('image_example1')"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Example 2: View Tensorflow-Lite model\n",

    "\n",

    "In this example, we view the trained `.tflite` model file in the \n",

    "[image_example1](../../docs/python_api/models/examples/image_example1.md) model's [model archive](../../docs/guides/model_archive.md).\n",

    "\n",

    "__NOTE:__ The model graph will appear in your web-browser."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 4,

   "metadata": {},

   "outputs": [

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Serving 'E:/reed/mltk/models/image_example1/extracted_archive/image_example1.tflite' at http://localhost:8080\n",

      "Stopping http://localhost:8080\n"

     ]

    }

   ],

   "source": [

    "view_model('image_example1', tflite=True)"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Example 3: View external Tensorflow-Lite model\n",

    "\n",

    "The given model need _not_ be generated by the MLTK. \n",

    "External models are also supported by the `view_model` API.\n",

    "\n",

    "__NOTE:__ The model graph will appear in your web-browser."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 5,

   "metadata": {},

   "outputs": [],

   "source": [

    "import os \n",

    "import tempfile\n",

    "import urllib\n",

    "import shutil\n",

    "\n",

    "# Use .tflite mode found here:\n",

    "# https://github.com/mlcommons/tiny/tree/master/benchmark/training/keyword_spotting/trained_models\n",

    "# NOTE: Update this URL to point to your model if necessary\n",

    "TFLITE_MODEL_URL = 'https://github.com/mlcommons/tiny/raw/master/benchmark/training/keyword_spotting/trained_models/kws_ref_model.tflite'\n",

    "\n",

    "# Download the .tflite file and save to the temp dir\n",

    "external_tflite_path = os.path.normpath(f'{tempfile.gettempdir()}/kws_ref_model.tflite')\n",

    "with open(external_tflite_path, 'wb') as dst:\n",

    "    with urllib.request.urlopen(TFLITE_MODEL_URL) as src:\n",

    "        shutil.copyfileobj(src, dst)"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 6,

   "metadata": {},

   "outputs": [

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Serving 'E:/kws_ref_model.tflite' at http://localhost:8080\n",

      "Stopping http://localhost:8080\n"

     ]

    }

   ],

   "source": [

    "view_model(external_tflite_path)"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Example 4: View model before training\n",

    "\n",

    "Training a model can be very time-consuming, and it is useful to view a \n",

    "model before investing time and energy into training it.  \n",

    "For this reason, the MLTK `view_model` API features a `build` argument to build a model\n",

    "and view it _before_ the model is fully trained.\n",

    "\n",

    "In this example, the [image_example1](../../docs/python_api/models/examples/image_example1.md) model is built\n",

    "at api-execution-time and this file is opened in the viewer.  \n",

    "Note that _only_ the [model specification](../../docs/guides/model_specification.md) script is required, \n",

    "it does _not_ need to be trained first.\n",

    "\n",

    "__NOTE:__ The model graph will appear in your web-browser."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 7,

   "metadata": {},

   "outputs": [

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Enabling test mode\n",

      "Model: \"image_example1\"\n",

      "_________________________________________________________________\n",

      "Layer (type)                 Output Shape              Param #   \n",

      "=================================================================\n",

      "conv2d (Conv2D)              (None, 48, 48, 24)        240       \n",

      "_________________________________________________________________\n",

      "average_pooling2d (AveragePo (None, 24, 24, 24)        0         \n",

      "_________________________________________________________________\n",

      "conv2d_1 (Conv2D)            (None, 11, 11, 16)        3472      \n",

      "_________________________________________________________________\n",

      "conv2d_2 (Conv2D)            (None, 9, 9, 24)          3480      \n",

      "_________________________________________________________________\n",

      "batch_normalization (BatchNo (None, 9, 9, 24)          96        \n",

      "_________________________________________________________________\n",

      "activation (Activation)      (None, 9, 9, 24)          0         \n",

      "_________________________________________________________________\n",

      "average_pooling2d_1 (Average (None, 4, 4, 24)          0         \n",

      "_________________________________________________________________\n",

      "flatten (Flatten)            (None, 384)               0         \n",

      "_________________________________________________________________\n",

      "dense (Dense)                (None, 3)                 1155      \n",

      "_________________________________________________________________\n",

      "activation_1 (Activation)    (None, 3)                 0         \n",

      "=================================================================\n",

      "Total params: 8,443\n",

      "Trainable params: 8,395\n",

      "Non-trainable params: 48\n",

      "_________________________________________________________________\n",

      "\n",

      "Total MACs: 1.197 M\n",

      "Total OPs: 2.528 M\n",

      "Name: image_example1\n",

      "Version: 1\n",

      "Description: Image classifier example for detecting Rock/Paper/Scissors hand gestures in images\n",

      "Classes: rock, paper, scissor\n",

      "hash: None\n",

      "date: None\n",

      "Test mode enabled, forcing max_samples_per_class=3, batch_size=3\n",

      "NOTE: ProcessPoolManager using ThreadPool (instead of ProcessPool)\n",

      "ProcessPoolManager using 1 of 24 CPU cores\n",

      "NOTE: You may need to adjust the \"cores\" parameter of the data generator if you're experiencing performance issues\n",

      "Training dataset: Found 9 samples belonging to 3 classes:\n",

      "      rock = 3\n",

      "     paper = 3\n",

      "   scissor = 3\n",

      "Validation dataset: Found 9 samples belonging to 3 classes:\n",

      "      rock = 3\n",

      "     paper = 3\n",

      "   scissor = 3\n",

      "Forcing epochs=1 since test=true\n",

      "Class weights:\n",

      "- rock = 1.00\n",

      "- paper = 1.00\n",

      "- scissor = 1.00\n",

      "Starting model training ...\n",

      "WARNING:tensorflow:From c:\\Users\\reed\\workspace\\silabs\\mltk\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",

      "Instructions for updating:\n",

      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"

     ]

    },

    {

     "data": {

      "application/vnd.jupyter.widget-view+json": {

       "model_id": "6b05d5a0cc17481186c24aa2a4ebeda4",

       "version_major": 2,

       "version_minor": 0

      },

      "text/plain": [

       "Training:   0%|           0/1 ETA: ?s,  ?epochs/s"

      ]

     },

     "metadata": {},

     "output_type": "display_data"

    },

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Epoch 1/1\n"

     ]

    },

    {

     "data": {

      "application/vnd.jupyter.widget-view+json": {

       "model_id": "1f3c150d77554892aac6ca163a268b2f",

       "version_major": 2,

       "version_minor": 0

      },

      "text/plain": [

       "0/3           ETA: ?s - "

      ]

     },

     "metadata": {},

     "output_type": "display_data"

    },

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Generating C:/Users/reed/.mltk/models/image_example1-test/image_example1.test.h5\n",

      "\n",

      "\n",

      "*** Best training val_accuracy = 0.667\n",

      "\n",

      "\n",

      "Training complete\n",

      "Training logs here: C:/Users/reed/.mltk/models/image_example1-test\n",

      "Trained model files here: c:/users/reed/workspace/silabs/mltk/mltk/models/examples/image_example1-test.mltk.zip\n",

      "ProcessPoolManager using 7 of 24 CPU cores\n",

      "NOTE: You may need to adjust the \"cores\" parameter of the data generator if you're experiencing performance issues\n",

      "Generating E:/reed/mltk/tmp_models/image_example1.tflite\n",

      "INFO:tensorflow:Assets written to: E:\\tmp2xfrszzl\\assets\n",

      "Serving 'E:/reed/mltk/tmp_models/image_example1.tflite' at http://localhost:8080\n",

      "Stopping http://localhost:8080\n"

     ]

    }

   ],

   "source": [

    "view_model('image_example1', tflite=True, build=True)"

   ]

  }

 ],

 "metadata": {

  "interpreter": {

   "hash": "d2cfb25ea30f37ddda2085817c91f6bd2a4a914387b5b179eb21bf4600b69cf8"

  },

  "kernelspec": {

   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",

   "name": "python3"

  },

  "language_info": {

   "codemirror_mode": {

    "name": "ipython",

    "version": 3

   },

   "file_extension": ".py",

   "mimetype": "text/x-python",

   "name": "python",

   "nbconvert_exporter": "python",

   "pygments_lexer": "ipython3",

   "version": "3.9.7"

  },

  "orig_nbformat": 4

 },

 "nbformat": 4,

 "nbformat_minor": 2

}

