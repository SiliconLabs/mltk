{

 "cells": [

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "# Keyword Spotting - Pac-Man\n",

    "\n",

    "This tutorial describes how to use the MLTK to develop a \"Pac-Man\" keyword spotting demo.\n",

    "\n",

    "The basic setup for this demo is as follows:  \n",

    "![](../../docs/img/pacman_demo_overview.png)\n",

    "\n",

    "In the demo, embedded machine learning is used to detect the keywords:\n",

    "- __Left__\n",

    "- __Right__\n",

    "- __Up__\n",

    "- __Down__\n",

    "- __Stop__\n",

    "- __Go__\n",

    "\n",

    "When a keyword is detected, its corresponding ID is sent to a webpage via Bluetooth Low-Energy. The webpage uses Javascript to process keyword ID to move the Pac-Man accordingly."

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Live Demo\n",

    "\n",

    "A live demo for this tutorial is available online:  \n",

    "[https://mltk-pacman.web.app](https://mltk-pacman.web.app)\n",

    "\n",

    "\n",

    "__NOTE:__ To use this demo, you must have a [BRD2601](../../docs/other/supported_hardware.md#brd2601) development board."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Quick Links\n",

    "\n",

    "- [GitHub Source](https://github.com/SiliconLabs/mltk/blob/master/mltk/tutorials/keyword_spotting_pacman.ipynb) - View this tutorial on Github\n",

    "- [Run on Colab](https://colab.research.google.com/github/siliconlabs/mltk/blob/master/mltk/tutorials/keyword_spotting_pacman.ipynb) - Run this tutorial on Google Colab\n",

    "- [Train in the \"Cloud\"](../../mltk/tutorials/cloud_training_with_vast_ai.md) - _Vastly_ improve training times by training this model in the \"cloud\"\n",

    "- [C++ Example Application](../../docs/cpp_development/examples/ble_audio_classifier.md) - View this tutorial's associated C++ example application\n",

    "- [Pac-Man Webpage Source](https://github.com/SiliconLabs/mltk/blob/master/cpp/shared/apps/ble_audio_classifier/web/pacman) - View the Pac-Man webpage's source code on Github\n",

    "- [Machine Learning Model](../../docs/python_api/models/siliconlabs/keyword_spotting_pacman_v3.md) - View this tutorial's associated machine learning model\n",

    "- [Live Demo](https://mltk-pacman.web.app) - Play Pac-Man using the keywords: Left, Right, Up, Down \n",

    "- [Presentation PDF](https://cms.tinyml.org/wp-content/uploads/talks2022/tinyML_Talks_Dan_Riedler_221025.pdf) - Presentation describing how this demo was created\n",

    "- [Presentation Video](https://www.youtube.com/watch?v=xhiFMDOyA0g) - YouTube video of the presentation given to TinyML.org for this tutorial"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Overview\n",

    "\n",

    "### Objectives\n",

    "\n",

    "After completing this tutorial, you will have:\n",

    "1. A better understanding of how audio classification machine learning models work\n",

    "2. All of the tools needed to develop your own keyword spotting model\n",

    "3. A better understanding of how to issue commands to a webpage from an embedded MCU via Bluetooth Low Energy\n",

    "4. A working demo to play the game Pac-Man using the keywords: \"Left\", \"Right\", \"Up\", \"Down\", \"Stop\", \"Go\"\n",

    "\n",

    "### Content\n",

    "\n",

    "This tutorial is divided into the following sections:\n",

    "- [Prerequisite reading](#prerequisite-reading)\n",

    "- [Creating the machine learning model](#creating-the-machine-learning-model)\n",

    "- [Creating the firmware application](#creating-the-firmware-application)\n",

    "- [Creating the Pac-Man webpage](#creating-the-pac-man-webpage)\n",

    "- [Running the demo](#running-the-demo)\n",

    "\n",

    "### Running this tutorial from a notebook\n",

    "\n",

    "For documentation purposes, this tutorial was designed to run within a [Jupyter Notebook](https://jupyter.org). \n",

    "The notebook can either run locally on your PC _or_ on a remote server like [Google Colab](https://colab.research.google.com/notebooks/welcome.ipynb).  \n",

    "\n",

    "- Refer to the [Notebook Examples Guide](../../docs/guides/notebook_examples_guide.md) for more details\n",

    "- Click here: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/siliconlabs/mltk/blob/master/mltk/tutorials/keyword_spotting_pacman.ipynb) to run this tutorial interactively in your browser\n",

    "\n",

    "__NOTE:__ Some of the following sections require this tutorial to be running locally with a supported embedded platform connected.\n",

    "\n",

    "### Running this tutorial from the command-line\n",

    "\n",

    "While this tutorial uses a [Jupyter Notebook](https://jupyter.org), \n",

    "the recommended approach is to use your favorite text editor and standard command terminal, no Jupyter Notebook required.  \n",

    "\n",

    "See the [Standard Python Package Installation](https://siliconlabs.github.io/mltk/docs/installation.html#standard-python-package) guide for more details on how to enable the `mltk` command in your local terminal.\n",

    "\n",

    "In this mode, when you encounter a `!mltk` command in this tutorial, the command should actually run in your local terminal (excluding the `!`)"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Required Hardware\n",

    "\n",

    "To play this tutorial's game using machine learning + keyword spotting, the [BRD2601](../../docs/other/supported_hardware.md#brd2601) development board is required."

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Install MLTK Python Package\n",

    "\n",

    "Before using the MLTK, it must first be installed.  \n",

    "See the [Installation Guide](../../docs/installation.md) for more details."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "metadata": {},

   "outputs": [],

   "source": [

    "!pip install --upgrade silabs-mltk"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "All MLTK modeling operations are accessible via the `mltk` command.  \n",

    "Run the command `mltk --help` to ensure it is working.  \n",

    "__NOTE:__ The exclamation point `!` tells the Notebook to run a shell command, it is not required in a [standard terminal](https://siliconlabs.github.io/mltk/docs/installation.html#standard-python-package)"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 1,

   "metadata": {},

   "outputs": [

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Usage: mltk [OPTIONS] COMMAND [ARGS]...\n",

      "\n",

      "  Silicon Labs Machine Learning Toolkit\n",

      "\n",

      "  This is a Python package with command-line utilities and scripts to aid the\n",

      "  development of machine learning models for Silicon Lab's embedded platforms.\n",

      "\n",

      "Options:\n",

      "  --version         Display the version of this mltk package and exit\n",

      "  --gpu / --no-gpu  Disable usage of the GPU. \n",

      "                    This does the same as defining the environment variable: CUDA_VISIBLE_DEVICES=-1\n",

      "                    Example:\n",

      "                    mltk --no-gpu train image_example1\n",

      "  --help            Show this message and exit.\n",

      "\n",

      "Commands:\n",

      "  build               MLTK build commands\n",

      "  classify_audio      Classify keywords/events detected in a microphone's...\n",

      "  classify_image      Classify images detected by a camera connected to...\n",

      "  commander           Silab's Commander Utility\n",

      "  compile             Compile a model for the specified accelerator\n",

      "  custom              Custom Model Operations\n",

      "  evaluate            Evaluate a trained ML model\n",

      "  fingerprint_reader  View/save fingerprints captured by the fingerprint...\n",

      "  profile             Profile a model\n",

      "  quantize            Quantize a model into a .tflite file\n",

      "  summarize           Generate a summary of a model\n",

      "  train               Train an ML model\n",

      "  update_params       Update the parameters of a previously trained model\n",

      "  utest               Run the all unit tests\n",

      "  view                View an interactive graph of the given model in a...\n",

      "  view_audio          View the spectrograms generated by the...\n"

     ]

    }

   ],

   "source": [

    "!mltk --help"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Prerequisite Reading\n",

    "\n",

    "Before continuing with this tutorial, it is recommended to review the following documentation:\n",

    "- [Keyword Spotting Overview](../../docs/audio/keyword_spotting_overview.md) - Provides overview of how embedded keyword spotting works\n",

    "- [Keyword Spotting Tutorial](../../mltk/tutorials/keyword_spotting_on_off.md) - Provides an in-depth tutorial on how to create a keyword spotting model"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Creating the Machine Learning Model\n",

    "\n",

    "The pre-defined [Model Specification](../../docs/guides/model_specification.md) used by the tutorial may be found on [Github](https://github.com/SiliconLabs/mltk/tree/master/mltk/models/siliconlabs/keyword_spotting_pacman_v3.py).\n",

    "\n",

    "This model is a standard audio classification model designed to detect the classes:  \n",

    "- Left\n",

    "- Right\n",

    "- Up\n",

    "- Down\n",

    "- Stop\n",

    "- Go\n",

    "- _unknown_\n",

    "\n",

    "Additionally, this model augments the training samples by adding audio recorded while playing the Pac-Man game. In this way, the model can be more robust to the background noise generated while playing the game.\n",

    "\n",

    "Refer to the model, [keyword_spotting_pacman_v3](../../docs/python_api/models/siliconlabs/keyword_spotting_pacman_v3.md) for more details."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Select the dataset\n",

    "\n",

    "This model was trained using several different datasets:  \n",

    "- [mltk.datasets.audio.direction_commands](../../docs/python_api/datasets/audio/direction_commands.md) - Synthetically generated keywords: left, right, up, down, stop, go\n",

    "- [mltk.datasets.audio.speech_commands_v2](../../docs/python_api/datasets/audio/speech_commands_v2.md) - Human generated keywords: left, right, up, down, stop, go\n",

    "- [mltk.datasets.audio.mlcommons.ml_commons_keyword](../../docs/python_api/datasets/audio/ml_commons/keywords.md) - Large collection of keywords, random subset used for *unknown* class\n",

    "- [mltk.datasets.audio.background_noise.esc50](../../docs/python_api/datasets/audio/background_noise/esc50.md) - Collection of various noises, random subset used for *unknown* class\n",

    "- [mltk.datasets.audio.background_noise.ambient](../../docs/python_api/datasets/audio/background_noise/ambient.md) - Collection of various background noises, mixed into other samples for augmentation\n",

    "- [mltk.datasets.audio.background_noise.brd2601](../../docs/python_api/datasets/audio/background_noise/brd2601.md) - \"Silence\" recorded by BRD2601 microphone, mixed into other samples to make them \"sound\" like they came from the BRD2601's microphone\n",

    "- Pac-Man game noise - Recording from Pac-Man game play, mixed into other samples for augmentation\n",

    "\n"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Model Parameter Tradeoffs\n",

    "\n",

    "We have two main requirements when choosing the model parameters:\n",

    "- We want the spectrogram resolution and convolutional filters to be as high as possible so that the model can make accurate predictions\n",

    "- We want the model's computational complexity to be as small as possible so that inference latency is small and keywords are quickly detected while playing the game\n",

    "\n",

    "Note that the larger the spectrogram resolution, the larger the model's input size and thus the larger the model's computational complexity. Likewise, more convolution filters also increases the model's computational complexity. As such, we need to find a middle ground for these parameters.\n",

    "\n",

    "The MLTK offers two tools that can help when choosing these parameters:  \n",

    "- [Model Profiler](../../docs/guides/model_profiler.md) - This allows for profiling the model on the embedded device and determining the inference latency __before__ fully training the model\n",

    "- [Audio Visualizer Utility](https://siliconlabs.github.io/mltk/docs/audio/audio_feature_generator.html#audio-visualizer-utility) - This allows for visualizing the generated spectrograms in real-time"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Audio Feature Generator Settings\n",

    "\n",

    "This model uses the following [Audio Feature Generator](../../docs/audio/audio_feature_generator.md) settings:"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 2,

   "metadata": {},

   "outputs": [],

   "source": [

    "from mltk.core.preprocess.audio.audio_feature_generator import AudioFeatureGeneratorSettings\n",

    "\n",

    "frontend_settings = AudioFeatureGeneratorSettings()\n",

    "\n",

    "frontend_settings.sample_rate_hz = 16000\n",

    "frontend_settings.sample_length_ms = 1000                       # A 1s buffer should be enough to capture the keywords\n",

    "frontend_settings.window_size_ms = 30\n",

    "frontend_settings.window_step_ms = 10\n",

    "frontend_settings.filterbank_n_channels = 104                   # We want this value to be as large as possible\n",

    "                                                                # while still allowing for the ML model to execute efficiently on the hardware\n",

    "frontend_settings.filterbank_upper_band_limit = 7500.0\n",

    "frontend_settings.filterbank_lower_band_limit = 125.0           # The dev board mic seems to have a lot of noise at lower frequencies\n",

    "\n",

    "frontend_settings.noise_reduction_enable = True                 # Enable the noise reduction block to help ignore background noise in the field\n",

    "frontend_settings.noise_reduction_smoothing_bits = 10\n",

    "frontend_settings.noise_reduction_even_smoothing =  0.025\n",

    "frontend_settings.noise_reduction_odd_smoothing = 0.06\n",

    "frontend_settings.noise_reduction_min_signal_remaining = 0.40   # This value is fairly large (which makes the background noise reduction small)\n",

    "                                                                # But it has been found to still give good results\n",

    "                                                                # i.e. There is still some background noise reduction,\n",

    "                                                                # but the actual signal is still (mostly) untouched\n",

    "\n",

    "frontend_settings.dc_notch_filter_enable = True                 # Enable the DC notch filter, to help remove the DC signal from the dev board's mic\n",

    "frontend_settings.dc_notch_filter_coefficient = 0.95\n",

    "\n",

    "frontend_settings.quantize_dynamic_scale_enable = True          # Enable dynamic quantization, this dynamically converts the uint16 spectrogram to int8\n",

    "frontend_settings.quantize_dynamic_scale_range_db = 40.0"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "\n",

    "This uses a 16kHz sample rate which was found to give better performance at the expense of more RAM.\n",

    "\n",

    "```python\n",

    "frontend_settings.sample_rate_hz = 16000\n",

    "```\n",

    "\n",

    "To help reduce the model computational complexity, only a 1000ms sample length is used. \n",

    "\n",

    "```python\n",

    "frontend_settings.sample_length_ms = 1000\n",

    "```\n",

    "\n",

    "The idea here is that it only takes at most ~1000ms to say any of the keywords (i.e. the audio buffer needs to be large enough to hold the entire keyword but no larger). \n",

    "\n",

    "This model uses a window size of 30ms and a step of 10ms.\n",

    "\n",

    "```python\n",

    "frontend_settings.window_size_ms = 30\n",

    "frontend_settings.window_step_ms = 10\n",

    "```\n",

    "\n",

    "These values were found experimentally using the [Audio Visualizer Utility](https://siliconlabs.github.io/mltk/docs/audio/audio_feature_generator.html#audio-visualizer-utility).\n",

    "\n",

    "104 frequency bins are used to generate the spectrogram:\n",

    "\n",

    "```python\n",

    "frontend_settings.filterbank_n_channels = 104\n",

    "```\n",

    "\n",

    "Increasing this value improves the resolution of spectrogram at the cost of model computational complexity (i.e. inference latency).\n",

    "\n",

    "The noise reduction block is enabled but uses a fairly large `min_signal_remaining`:\n",

    "```python\n",

    "frontend_settings.noise_reduction_enable = True\n",

    "frontend_settings.noise_reduction_smoothing_bits = 10\n",

    "frontend_settings.noise_reduction_even_smoothing =  0.025\n",

    "frontend_settings.noise_reduction_odd_smoothing = 0.06\n",

    "frontend_settings.noise_reduction_min_signal_remaining = 0.40\n",

    "```\n",

    "\n",

    "This helps to reduce background noise in the field.  \n",

    "__NOTE:__ We also add padding to the audio samples during training to \"warm up\" the noise reduction block when generating the spectrogram using the \n",

    "[Audio Feature Generator](../../docs/audio/audio_feature_generator.md). See the `audio_pipeline_with_augmentations()`\n",

    "function in [keyword_spotting_pacman_v3.py](https://siliconlabs.github.io/mltk/docs/python_api/models/siliconlabs/keyword_spotting_pacman_v3.html#model-specification) for more details.\n",

    "\n",

    "\n",

    "\n",

    "The DC notch filter was enabled to help remove the DC component from the development board's microphone:\n",

    "```python\n",

    "frontend_settings.dc_notch_filter_enable = True # Enable the DC notch filter\n",

    "frontend_settings.dc_notch_filter_coefficient = 0.95\n",

    "```\n",

    "\n",

    "Dynamic quantization was enabled to convert the generated spectrogram from uint16 to int8\n",

    "\n",

    "```python\n",

    "frontend_settings.quantize_dynamic_scale_enable = True # Enable dynamic quantization\n",

    "frontend_settings.quantize_dynamic_scale_range_db = 40.0\n",

    "```"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Module Architecture\n",

    "\n",

    "\n",

    "The model is based on the [Temporal efficient neural network (TENet)](https://arxiv.org/pdf/2010.09960.pdf) model architecture.  \n",

    "> A network for processing spectrogram data using temporal and depthwise convolutions. The network treats the [T, F] spectrogram as a timeseries shaped [T, 1, F].\n",

    "\n",

    "More details at [mltk.models.shared.tenet.TENet](../../docs/python_api/models/common_models.md#tenet)\n"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "metadata": {},

   "outputs": [],

   "source": [

    "def my_model_builder(model: MyModel) -> tf.keras.Model:\n",

    "    \"\"\"Build the Keras model\n",

    "    \"\"\"\n",

    "    input_shape = model.input_shape\n",

    "    # NOTE: This model requires the input shape: <time, 1, features>\n",

    "    #       while the embedded device expects: <time, features, 1>\n",

    "    #       Since the <time> axis is still row-major, we can swap the <features> with 1 without issue\n",

    "    time_size, feature_size, _ = input_shape\n",

    "    input_shape = (time_size, 1, feature_size)\n",

    "\n",

    "    keras_model = tenet.TENet12(\n",

    "        input_shape=input_shape,\n",

    "        classes=model.n_classes,\n",

    "        channels=40,\n",

    "        blocks=5,\n",

    "    )\n",

    "\n",

    "    keras_model.compile(\n",

    "        loss='categorical_crossentropy',\n",

    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=1e-8),\n",

    "        metrics= ['accuracy']\n",

    "    )\n",

    "\n",

    "    return keras_model"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "The main parameters to modify are:\n",

    "\n",

    "```python\n",

    "channels = 40\n",

    "blocks = 5\n",

    "```\n",

    "\n",

    "`channels` sets the base number of channels in the network.  \n",

    "`block` set the number of `(StridedIBB -> IBB -> ...)` blocks in the networks.\n",

    "\n",

    "The larger these values are, the more trainable parameters the model will have which should allow for it to have better accuracy. \n",

    "However, increasing this value also increases the model's computational complexity which increases the model inference latency."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Audio Data Generator\n",

    "\n",

    "This model has an additional requirement that the keywords need to be said while the Pac-Man video game noises are generated in the background. As such, the model is trained by taking each keyword sample and adding a snippet of background noise to the sample. In this way, the model learns to pick out the keywords from the Pac-Man video game's noises.\n",

    "\n",

    "The Pac-Man game audio was acquired by recording during game play (using the arrows on the keyboard). Recording was done using the MLTK command:\n",

    "\n",

    "```\n",

    "mltk classify_audio keyword_spotting_pacman_v3 --dump-audio --device\n",

    "```\n",

    "\n",

    "This command uses the microphone on the development board to record the video game's generated audio. The recorded audio is saved to the local PC as a `.wav` file.\n",

    "\n",

    "The [model specification](https://github.com/SiliconLabs/mltk/tree/master/mltk/models/siliconlabs/keyword_spotting_pacman_v3.py) file was then modified to apply random augmentations to the dataset samples and then [generate spectrograms](https://siliconlabs.github.io/mltk/docs/python_api/data_preprocessing/audio.html#mltk.core.preprocess.utils.audio.apply_frontend) from the augmented samples.\n",

    "The spectrograms are given to the model for training.\n",

    "\n",

    "__NOTE:__ The spectrogram generation algorithm [source code](../../docs/audio/audio_feature_generator.md) is shared between the model training script and embedded runtime. This way, the generated spectrograms \"look\" the same during training and inference which should make the model more robust in the field."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "metadata": {},

   "outputs": [],

   "source": [

    "def audio_pipeline_with_augmentations(\n",

    "    path_batch:np.ndarray,\n",

    "    label_batch:np.ndarray,\n",

    "    seed:np.ndarray\n",

    ") -> np.ndarray:\n",

    "    \"\"\"Augment a batch of audio clips and generate spectrograms\n",

    "\n",

    "    This does the following, for each audio file path in the input batch:\n",

    "    1. Read audio file\n",

    "    2. Adjust its length to fit within the specified length\n",

    "    3. Apply random augmentations to the audio sample using audiomentations\n",

    "    4. Convert to the specified sample rate (if necessary)\n",

    "    5. Generate a spectrogram from the augmented audio sample\n",

    "    6. Dump the augmented audio and spectrogram (if necessary)\n",

    "\n",

    "    NOTE: This will be execute in parallel across *separate* subprocesses.\n",

    "\n",

    "    Arguments:\n",

    "        path_batch: Batch of audio file paths\n",

    "        label_batch: Batch of corresponding labels\n",

    "        seed: Batch of seeds to use for random number generation,\n",

    "            This ensures that the \"random\" augmentations are reproducible\n",

    "\n",

    "    Return:\n",

    "        Generated batch of spectrograms from augmented audio samples\n",

    "    \"\"\"\n",

    "    batch_length = path_batch.shape[0]\n",

    "    height, width = frontend_settings.spectrogram_shape\n",

    "    x_shape = (batch_length, height, 1, width)\n",

    "    x_batch = np.empty(x_shape, dtype=np.int8)\n",

    "\n",

    "    # This is the amount of padding we add to the beginning of the sample\n",

    "    # This allows for \"warming up\" the noise reduction block\n",

    "    padding_length_ms = 1000\n",

    "    padded_frontend_settings = frontend_settings.copy()\n",

    "    padded_frontend_settings.sample_length_ms += padding_length_ms\n",

    "\n",

    "    # For each audio sample path in the current batch\n",

    "    for i, (audio_path, labels) in enumerate(zip(path_batch, label_batch)):\n",

    "        class_id = np.argmax(labels)\n",

    "        np.random.seed(seed[i])\n",

    "\n",

    "        rn = np.random.random()\n",

    "        # 3% of the time we want to replace the \"unknown\" sample with silence\n",

    "        if class_id == unknown_class_id and rn < 0.03:\n",

    "            original_sample_rate = frontend_settings.sample_rate_hz\n",

    "            sample = np.zeros((original_sample_rate,), dtype=np.float32)\n",

    "            audio_path = 'silence.wav'.encode('utf-8')\n",

    "        else:\n",

    "            # Read the audio file\n",

    "            try:\n",

    "                sample, original_sample_rate = audio_utils.read_audio_file(audio_path, return_numpy=True, return_sample_rate=True)\n",

    "            except Exception as e:\n",

    "                raise RuntimeError(f'Failed to read: {audio_path}, err: {e}')\n",

    "\n",

    "        # Create a buffer to hold the padded sample\n",

    "        padding_length = int((original_sample_rate * padding_length_ms) / 1000)\n",

    "        padded_sample_length = int((original_sample_rate * padded_frontend_settings.sample_length_ms) / 1000)\n",

    "        padded_sample = np.zeros((padded_sample_length,), dtype=np.float32)\n",

    "\n",

    "\n",

    "        # Adjust the audio clip to the length defined in the frontend_settings\n",

    "        out_length = int((original_sample_rate * frontend_settings.sample_length_ms) / 1000)\n",

    "        sample = audio_utils.adjust_length(\n",

    "            sample,\n",

    "            out_length=out_length,\n",

    "            trim_threshold_db=30,\n",

    "            offset=np.random.uniform(0, 1)\n",

    "        )\n",

    "        padded_sample[padding_length:padding_length+len(sample)] += sample\n",

    "\n",

    "\n",

    "\n",

    "        # Initialize the global audio augmentations instance\n",

    "        # NOTE: We want this to be global so that we only initialize it once per subprocess\n",

    "        audio_augmentations = globals().get('audio_augmentations', None)\n",

    "        if audio_augmentations is None:\n",

    "            audio_augmentations = audiomentations.Compose(\n",

    "                p=1.0,\n",

    "                transforms=[\n",

    "                audiomentations.Gain(min_gain_in_db=0.95, max_gain_in_db=1.2, p=1.0),\n",

    "                audiomentations.AddBackgroundNoise(\n",

    "                    f'{dataset_dir}/_background_noise_/ambient',\n",

    "                    min_snr_in_db=-1, # The lower the SNR, the louder the background noise\n",

    "                    max_snr_in_db=35,\n",

    "                    noise_rms=\"relative\",\n",

    "                    lru_cache_size=50,\n",

    "                    p=0.80\n",

    "                ),\n",

    "                audiomentations.AddBackgroundNoise(\n",

    "                    f'{dataset_dir}/_background_noise_/pacman',\n",

    "                    min_absolute_rms_in_db=-60,\n",

    "                    max_absolute_rms_in_db=-35,\n",

    "                    noise_rms=\"absolute\",\n",

    "                    lru_cache_size=50,\n",

    "                    p=0.50\n",

    "                ),\n",

    "                audiomentations.AddBackgroundNoise(\n",

    "                    f'{dataset_dir}/_background_noise_/brd2601',\n",

    "                    min_absolute_rms_in_db=-75.0,\n",

    "                    max_absolute_rms_in_db=-60.0,\n",

    "                    noise_rms=\"absolute\",\n",

    "                    lru_cache_size=50,\n",

    "                    p=1.0\n",

    "                ),\n",

    "                #audiomentations.AddGaussianSNR(min_snr_in_db=25, max_snr_in_db=40, p=0.25),\n",

    "            ])\n",

    "            globals()['audio_augmentations'] = audio_augmentations\n",

    "\n",

    "        # Apply random augmentations to the audio sample\n",

    "        augmented_sample = audio_augmentations(padded_sample, original_sample_rate)\n",

    "\n",

    "        # Convert the sample rate (if necessary)\n",

    "        if original_sample_rate != frontend_settings.sample_rate_hz:\n",

    "            augmented_sample = audio_utils.resample(\n",

    "                augmented_sample,\n",

    "                orig_sr=original_sample_rate,\n",

    "                target_sr=frontend_settings.sample_rate_hz\n",

    "            )\n",

    "\n",

    "        # Ensure the sample values are within (-1,1)\n",

    "        augmented_sample = np.clip(augmented_sample, -1.0, 1.0)\n",

    "\n",

    "        # Generate a spectrogram from the augmented audio sample\n",

    "        spectrogram = audio_utils.apply_frontend(\n",

    "            sample=augmented_sample,\n",

    "            settings=padded_frontend_settings,\n",

    "            dtype=np.int8\n",

    "        )\n",

    "\n",

    "        # The input audio sample was padded with padding_length_ms of background noise\n",

    "        # Drop the padded background noise from the final spectrogram used for training\n",

    "        spectrogram = spectrogram[-height:, :]\n",

    "        # The output spectrogram is 2D, add a channel dimension to make it 3D:\n",

    "        # (height, width, channels=1)\n",

    "\n",

    "        # Convert the spectrogram dimension from\n",

    "        # <time, features> to\n",

    "        # <time, 1, features>\n",

    "        spectrogram = np.expand_dims(spectrogram, axis=-2)\n",

    "\n",

    "        x_batch[i] = spectrogram\n",

    "\n",

    "    return x_batch"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Profiling the model\n",

    "\n",

    "Before training a machine learning model, it is important to know how efficiently the model will execute on the embedded target. This is especially true when using keyword spotting to control a Pac-Man (a keyword that takes > 1s to detect will not be useful when trying to avoid the ghosts).\n",

    "\n",

    "If the model inference takes too long to execute on the embedded target, then the model parameters need to be decreased to reduce the model's computational complexity. The desired model parameters should be known before the model is fully trained.\n",

    "\n",

    "To help determine the best model parameters, the MLTK features a [Model Profiler](../../docs/guides/model_profiler.md) command:"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 3,

   "metadata": {},

   "outputs": [

    {

     "name": "stdout",

     "output_type": "stream",

     "text": [

      "Profiling ML model on device ...\n",

      "\n",

      "Profiling Summary\n",

      "Name: my_model\n",

      "Accelerator: MVP\n",

      "Input Shape: 1x98x1x104\n",

      "Input Data Type: int8\n",

      "Output Shape: 1x7\n",

      "Output Data Type: int8\n",

      "Flash, Model File Size (bytes): 446.5k\n",

      "RAM, Runtime Memory Size (bytes): 76.7k\n",

      "Operation Count: 12.4M\n",

      "Multiply-Accumulate Count: 6.0M\n",

      "Layer Count: 90\n",

      "Unsupported Layer Count: 0\n",

      "Accelerator Cycle Count: 5.3M\n",

      "CPU Cycle Count: 954.1k\n",

      "CPU Utilization (%): 16.7\n",

      "Clock Rate (hz): 78.0M\n",

      "Time (s): 73.3m\n",

      "Ops/s: 169.2M\n",

      "MACs/s: 82.1M\n",

      "Inference/s: 13.7\n",

      "\n",

      "Model Layers\n",

      "+-------+-------------------+--------+--------+------------+------------+----------+--------------------------+--------------+------------------------------------------------------+\n",

      "| Index | OpCode            | # Ops  | # MACs | Acc Cycles | CPU Cycles | Time (s) | Input Shape              | Output Shape | Options                                              |\n",

      "+-------+-------------------+--------+--------+------------+------------+----------+--------------------------+--------------+------------------------------------------------------+\n",

      "| 0     | conv_2d           | 2.5M   | 1.2M   | 928.9k     | 11.3k      | 11.8m    | 1x98x1x104,40x3x1x104,40 | 1x98x1x40    | Padding:Same stride:1x1 activation:None              |\n",

      "| 1     | conv_2d           | 976.1k | 470.4k | 390.2k     | 5.2k       | 5.0m     | 1x98x1x40,120x1x1x40,120 | 1x98x1x120   | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 2     | depthwise_conv_2d | 123.5k | 52.9k  | 96.4k      | 78.7k      | 1.6m     | 1x98x1x120,1x9x1x120,120 | 1x49x1x120   | Multiplier:1 padding:Same stride:2x2 activation:Relu |\n",

      "| 3     | conv_2d           | 472.4k | 235.2k | 185.3k     | 5.3k       | 2.4m     | 1x49x1x120,40x1x1x120,40 | 1x49x1x40    | Padding:Valid stride:1x1 activation:None             |\n",

      "| 4     | conv_2d           | 162.7k | 78.4k  | 66.7k      | 5.2k       | 870.0u   | 1x98x1x40,40x1x1x40,40   | 1x49x1x40    | Padding:Same stride:2x2 activation:Relu              |\n",

      "| 5     | add               | 2.0k   | 0      | 6.9k       | 2.7k       | 120.0u   | 1x49x1x40,1x49x1x40      | 1x49x1x40    | Activation:Relu                                      |\n",

      "| 6     | conv_2d           | 488.0k | 235.2k | 195.2k     | 5.3k       | 2.5m     | 1x49x1x40,120x1x1x40,120 | 1x49x1x120   | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 7     | depthwise_conv_2d | 123.5k | 52.9k  | 94.5k      | 78.5k      | 1.6m     | 1x49x1x120,1x9x1x120,120 | 1x49x1x120   | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 8     | conv_2d           | 472.4k | 235.2k | 185.3k     | 5.3k       | 2.4m     | 1x49x1x120,40x1x1x120,40 | 1x49x1x40    | Padding:Valid stride:1x1 activation:None             |\n",

      "| 9     | add               | 2.0k   | 0      | 6.9k       | 2.6k       | 120.0u   | 1x49x1x40,1x49x1x40      | 1x49x1x40    | Activation:Relu                                      |\n",

      "| 10    | conv_2d           | 488.0k | 235.2k | 195.2k     | 5.3k       | 2.5m     | 1x49x1x40,120x1x1x40,120 | 1x49x1x120   | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 11    | depthwise_conv_2d | 123.5k | 52.9k  | 94.5k      | 78.5k      | 1.6m     | 1x49x1x120,1x9x1x120,120 | 1x49x1x120   | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 12    | conv_2d           | 472.4k | 235.2k | 185.3k     | 5.3k       | 2.4m     | 1x49x1x120,40x1x1x120,40 | 1x49x1x40    | Padding:Valid stride:1x1 activation:None             |\n",

      "| 13    | add               | 2.0k   | 0      | 6.9k       | 2.6k       | 120.0u   | 1x49x1x40,1x49x1x40      | 1x49x1x40    | Activation:Relu                                      |\n",

      "| 14    | conv_2d           | 488.0k | 235.2k | 195.2k     | 5.3k       | 2.5m     | 1x49x1x40,120x1x1x40,120 | 1x49x1x120   | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 15    | depthwise_conv_2d | 123.5k | 52.9k  | 94.5k      | 78.5k      | 1.6m     | 1x49x1x120,1x9x1x120,120 | 1x49x1x120   | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 16    | conv_2d           | 472.4k | 235.2k | 185.3k     | 5.3k       | 2.4m     | 1x49x1x120,40x1x1x120,40 | 1x49x1x40    | Padding:Valid stride:1x1 activation:None             |\n",

      "| 17    | add               | 2.0k   | 0      | 6.9k       | 2.6k       | 120.0u   | 1x49x1x40,1x49x1x40      | 1x49x1x40    | Activation:Relu                                      |\n",

      "| 18    | conv_2d           | 488.0k | 235.2k | 195.5k     | 5.3k       | 2.5m     | 1x49x1x40,120x1x1x40,120 | 1x49x1x120   | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 19    | depthwise_conv_2d | 63.0k  | 27.0k  | 47.9k      | 40.6k      | 810.0u   | 1x49x1x120,1x9x1x120,120 | 1x25x1x120   | Multiplier:1 padding:Same stride:2x2 activation:Relu |\n",

      "| 20    | conv_2d           | 241.0k | 120.0k | 95.3k      | 5.3k       | 1.3m     | 1x25x1x120,40x1x1x120,40 | 1x25x1x40    | Padding:Valid stride:1x1 activation:None             |\n",

      "| 21    | conv_2d           | 83.0k  | 40.0k  | 34.4k      | 5.2k       | 480.0u   | 1x49x1x40,40x1x1x40,40   | 1x25x1x40    | Padding:Same stride:2x2 activation:Relu              |\n",

      "| 22    | add               | 1.0k   | 0      | 3.5k       | 2.6k       | 90.0u    | 1x25x1x40,1x25x1x40      | 1x25x1x40    | Activation:Relu                                      |\n",

      "| 23    | conv_2d           | 249.0k | 120.0k | 99.9k      | 5.3k       | 1.3m     | 1x25x1x40,120x1x1x40,120 | 1x25x1x120   | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 24    | depthwise_conv_2d | 63.0k  | 27.0k  | 46.4k      | 40.5k      | 810.0u   | 1x25x1x120,1x9x1x120,120 | 1x25x1x120   | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 25    | conv_2d           | 241.0k | 120.0k | 95.3k      | 5.3k       | 1.2m     | 1x25x1x120,40x1x1x120,40 | 1x25x1x40    | Padding:Valid stride:1x1 activation:None             |\n",

      "| 26    | add               | 1.0k   | 0      | 3.5k       | 2.6k       | 90.0u    | 1x25x1x40,1x25x1x40      | 1x25x1x40    | Activation:Relu                                      |\n",

      "| 27    | conv_2d           | 249.0k | 120.0k | 99.7k      | 5.3k       | 1.3m     | 1x25x1x40,120x1x1x40,120 | 1x25x1x120   | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 28    | depthwise_conv_2d | 63.0k  | 27.0k  | 46.4k      | 40.5k      | 810.0u   | 1x25x1x120,1x9x1x120,120 | 1x25x1x120   | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 29    | conv_2d           | 241.0k | 120.0k | 95.3k      | 5.3k       | 1.2m     | 1x25x1x120,40x1x1x120,40 | 1x25x1x40    | Padding:Valid stride:1x1 activation:None             |\n",

      "| 30    | add               | 1.0k   | 0      | 3.5k       | 2.6k       | 60.0u    | 1x25x1x40,1x25x1x40      | 1x25x1x40    | Activation:Relu                                      |\n",

      "| 31    | conv_2d           | 249.0k | 120.0k | 99.9k      | 5.3k       | 1.3m     | 1x25x1x40,120x1x1x40,120 | 1x25x1x120   | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 32    | depthwise_conv_2d | 63.0k  | 27.0k  | 46.4k      | 40.5k      | 810.0u   | 1x25x1x120,1x9x1x120,120 | 1x25x1x120   | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 33    | conv_2d           | 241.0k | 120.0k | 95.3k      | 5.3k       | 1.3m     | 1x25x1x120,40x1x1x120,40 | 1x25x1x40    | Padding:Valid stride:1x1 activation:None             |\n",

      "| 34    | add               | 1.0k   | 0      | 3.5k       | 2.6k       | 60.0u    | 1x25x1x40,1x25x1x40      | 1x25x1x40    | Activation:Relu                                      |\n",

      "| 35    | conv_2d           | 249.0k | 120.0k | 99.9k      | 5.3k       | 1.3m     | 1x25x1x40,120x1x1x40,120 | 1x25x1x120   | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 36    | depthwise_conv_2d | 32.8k  | 14.0k  | 23.8k      | 21.6k      | 420.0u   | 1x25x1x120,1x9x1x120,120 | 1x13x1x120   | Multiplier:1 padding:Same stride:2x2 activation:Relu |\n",

      "| 37    | conv_2d           | 125.3k | 62.4k  | 49.6k      | 5.3k       | 660.0u   | 1x13x1x120,40x1x1x120,40 | 1x13x1x40    | Padding:Valid stride:1x1 activation:None             |\n",

      "| 38    | conv_2d           | 43.2k  | 20.8k  | 18.0k      | 5.2k       | 270.0u   | 1x25x1x40,40x1x1x40,40   | 1x13x1x40    | Padding:Same stride:2x2 activation:Relu              |\n",

      "| 39    | add               | 520.0  | 0      | 1.8k       | 2.6k       | 60.0u    | 1x13x1x40,1x13x1x40      | 1x13x1x40    | Activation:Relu                                      |\n",

      "| 40    | conv_2d           | 129.5k | 62.4k  | 52.0k      | 5.3k       | 720.0u   | 1x13x1x40,120x1x1x40,120 | 1x13x1x120   | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 41    | depthwise_conv_2d | 32.8k  | 14.0k  | 22.4k      | 21.6k      | 420.0u   | 1x13x1x120,1x9x1x120,120 | 1x13x1x120   | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 42    | conv_2d           | 125.3k | 62.4k  | 49.6k      | 5.3k       | 660.0u   | 1x13x1x120,40x1x1x120,40 | 1x13x1x40    | Padding:Valid stride:1x1 activation:None             |\n",

      "| 43    | add               | 520.0  | 0      | 1.8k       | 2.6k       | 30.0u    | 1x13x1x40,1x13x1x40      | 1x13x1x40    | Activation:Relu                                      |\n",

      "| 44    | conv_2d           | 129.5k | 62.4k  | 52.0k      | 5.3k       | 720.0u   | 1x13x1x40,120x1x1x40,120 | 1x13x1x120   | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 45    | depthwise_conv_2d | 32.8k  | 14.0k  | 22.4k      | 21.6k      | 420.0u   | 1x13x1x120,1x9x1x120,120 | 1x13x1x120   | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 46    | conv_2d           | 125.3k | 62.4k  | 49.6k      | 5.3k       | 690.0u   | 1x13x1x120,40x1x1x120,40 | 1x13x1x40    | Padding:Valid stride:1x1 activation:None             |\n",

      "| 47    | add               | 520.0  | 0      | 1.8k       | 2.6k       | 60.0u    | 1x13x1x40,1x13x1x40      | 1x13x1x40    | Activation:Relu                                      |\n",

      "| 48    | conv_2d           | 129.5k | 62.4k  | 52.0k      | 5.3k       | 720.0u   | 1x13x1x40,120x1x1x40,120 | 1x13x1x120   | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 49    | depthwise_conv_2d | 32.8k  | 14.0k  | 22.4k      | 21.6k      | 420.0u   | 1x13x1x120,1x9x1x120,120 | 1x13x1x120   | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 50    | conv_2d           | 125.3k | 62.4k  | 49.6k      | 5.3k       | 660.0u   | 1x13x1x120,40x1x1x120,40 | 1x13x1x40    | Padding:Valid stride:1x1 activation:None             |\n",

      "| 51    | add               | 520.0  | 0      | 1.8k       | 2.6k       | 60.0u    | 1x13x1x40,1x13x1x40      | 1x13x1x40    | Activation:Relu                                      |\n",

      "| 52    | conv_2d           | 129.5k | 62.4k  | 52.0k      | 5.3k       | 720.0u   | 1x13x1x40,120x1x1x40,120 | 1x13x1x120   | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 53    | depthwise_conv_2d | 17.6k  | 7.6k   | 11.8k      | 12.1k      | 240.0u   | 1x13x1x120,1x9x1x120,120 | 1x7x1x120    | Multiplier:1 padding:Same stride:2x2 activation:Relu |\n",

      "| 54    | conv_2d           | 67.5k  | 33.6k  | 26.7k      | 5.3k       | 390.0u   | 1x7x1x120,40x1x1x120,40  | 1x7x1x40     | Padding:Valid stride:1x1 activation:None             |\n",

      "| 55    | conv_2d           | 23.2k  | 11.2k  | 9.7k       | 5.2k       | 180.0u   | 1x13x1x40,40x1x1x40,40   | 1x7x1x40     | Padding:Same stride:2x2 activation:Relu              |\n",

      "| 56    | add               | 280.0  | 0      | 992.0      | 2.6k       | 30.0u    | 1x7x1x40,1x7x1x40        | 1x7x1x40     | Activation:Relu                                      |\n",

      "| 57    | conv_2d           | 69.7k  | 33.6k  | 28.1k      | 5.3k       | 420.0u   | 1x7x1x40,120x1x1x40,120  | 1x7x1x120    | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 58    | depthwise_conv_2d | 17.6k  | 7.6k   | 10.3k      | 12.1k      | 210.0u   | 1x7x1x120,1x9x1x120,120  | 1x7x1x120    | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 59    | conv_2d           | 67.5k  | 33.6k  | 26.7k      | 5.3k       | 390.0u   | 1x7x1x120,40x1x1x120,40  | 1x7x1x40     | Padding:Valid stride:1x1 activation:None             |\n",

      "| 60    | add               | 280.0  | 0      | 992.0      | 2.6k       | 30.0u    | 1x7x1x40,1x7x1x40        | 1x7x1x40     | Activation:Relu                                      |\n",

      "| 61    | conv_2d           | 69.7k  | 33.6k  | 28.1k      | 5.3k       | 420.0u   | 1x7x1x40,120x1x1x40,120  | 1x7x1x120    | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 62    | depthwise_conv_2d | 17.6k  | 7.6k   | 10.3k      | 12.1k      | 210.0u   | 1x7x1x120,1x9x1x120,120  | 1x7x1x120    | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 63    | conv_2d           | 67.5k  | 33.6k  | 26.7k      | 5.3k       | 390.0u   | 1x7x1x120,40x1x1x120,40  | 1x7x1x40     | Padding:Valid stride:1x1 activation:None             |\n",

      "| 64    | add               | 280.0  | 0      | 992.0      | 2.6k       | 30.0u    | 1x7x1x40,1x7x1x40        | 1x7x1x40     | Activation:Relu                                      |\n",

      "| 65    | conv_2d           | 69.7k  | 33.6k  | 28.1k      | 5.3k       | 420.0u   | 1x7x1x40,120x1x1x40,120  | 1x7x1x120    | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 66    | depthwise_conv_2d | 17.6k  | 7.6k   | 10.3k      | 12.1k      | 210.0u   | 1x7x1x120,1x9x1x120,120  | 1x7x1x120    | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 67    | conv_2d           | 67.5k  | 33.6k  | 26.7k      | 5.3k       | 420.0u   | 1x7x1x120,40x1x1x120,40  | 1x7x1x40     | Padding:Valid stride:1x1 activation:None             |\n",

      "| 68    | add               | 280.0  | 0      | 992.0      | 2.6k       | 30.0u    | 1x7x1x40,1x7x1x40        | 1x7x1x40     | Activation:Relu                                      |\n",

      "| 69    | conv_2d           | 69.7k  | 33.6k  | 28.1k      | 5.3k       | 420.0u   | 1x7x1x40,120x1x1x40,120  | 1x7x1x120    | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 70    | depthwise_conv_2d | 10.1k  | 4.3k   | 5.8k       | 7.4k       | 150.0u   | 1x7x1x120,1x9x1x120,120  | 1x4x1x120    | Multiplier:1 padding:Same stride:2x2 activation:Relu |\n",

      "| 71    | conv_2d           | 38.6k  | 19.2k  | 15.3k      | 5.3k       | 240.0u   | 1x4x1x120,40x1x1x120,40  | 1x4x1x40     | Padding:Valid stride:1x1 activation:None             |\n",

      "| 72    | conv_2d           | 13.3k  | 6.4k   | 5.6k       | 5.2k       | 150.0u   | 1x7x1x40,40x1x1x40,40    | 1x4x1x40     | Padding:Same stride:2x2 activation:Relu              |\n",

      "| 73    | add               | 160.0  | 0      | 572.0      | 2.6k       | 30.0u    | 1x4x1x40,1x4x1x40        | 1x4x1x40     | Activation:Relu                                      |\n",

      "| 74    | conv_2d           | 39.8k  | 19.2k  | 16.1k      | 5.3k       | 270.0u   | 1x4x1x40,120x1x1x40,120  | 1x4x1x120    | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 75    | depthwise_conv_2d | 10.1k  | 4.3k   | 4.3k       | 7.3k       | 120.0u   | 1x4x1x120,1x9x1x120,120  | 1x4x1x120    | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 76    | conv_2d           | 38.6k  | 19.2k  | 15.3k      | 5.3k       | 270.0u   | 1x4x1x120,40x1x1x120,40  | 1x4x1x40     | Padding:Valid stride:1x1 activation:None             |\n",

      "| 77    | add               | 160.0  | 0      | 572.0      | 2.6k       | 30.0u    | 1x4x1x40,1x4x1x40        | 1x4x1x40     | Activation:Relu                                      |\n",

      "| 78    | conv_2d           | 39.8k  | 19.2k  | 16.1k      | 5.3k       | 270.0u   | 1x4x1x40,120x1x1x40,120  | 1x4x1x120    | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 79    | depthwise_conv_2d | 10.1k  | 4.3k   | 4.3k       | 7.3k       | 120.0u   | 1x4x1x120,1x9x1x120,120  | 1x4x1x120    | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 80    | conv_2d           | 38.6k  | 19.2k  | 15.3k      | 5.3k       | 240.0u   | 1x4x1x120,40x1x1x120,40  | 1x4x1x40     | Padding:Valid stride:1x1 activation:None             |\n",

      "| 81    | add               | 160.0  | 0      | 572.0      | 2.6k       | 30.0u    | 1x4x1x40,1x4x1x40        | 1x4x1x40     | Activation:Relu                                      |\n",

      "| 82    | conv_2d           | 39.8k  | 19.2k  | 16.1k      | 5.3k       | 270.0u   | 1x4x1x40,120x1x1x40,120  | 1x4x1x120    | Padding:Valid stride:1x1 activation:Relu             |\n",

      "| 83    | depthwise_conv_2d | 10.1k  | 4.3k   | 4.3k       | 7.3k       | 120.0u   | 1x4x1x120,1x9x1x120,120  | 1x4x1x120    | Multiplier:1 padding:Same stride:1x1 activation:Relu |\n",

      "| 84    | conv_2d           | 38.6k  | 19.2k  | 15.3k      | 5.3k       | 240.0u   | 1x4x1x120,40x1x1x120,40  | 1x4x1x40     | Padding:Valid stride:1x1 activation:None             |\n",

      "| 85    | add               | 160.0  | 0      | 572.0      | 2.6k       | 60.0u    | 1x4x1x40,1x4x1x40        | 1x4x1x40     | Activation:Relu                                      |\n",

      "| 86    | average_pool_2d   | 200.0  | 0      | 154.0      | 3.8k       | 60.0u    | 1x4x1x40                 | 1x1x1x40     | Padding:Valid stride:1x4 filter:1x4 activation:None  |\n",

      "| 87    | reshape           | 0      | 0      | 0          | 640.0      | 0        | 1x1x1x40,2               | 1x40         | Type=none                                            |\n",

      "| 88    | fully_connected   | 567.0  | 280.0  | 477.0      | 2.1k       | 30.0u    | 1x40,7x40,7              | 1x7          | Activation:None                                      |\n",

      "| 89    | softmax           | 35.0   | 0      | 0          | 5.5k       | 90.0u    | 1x7                      | 1x7          | Type=softmaxoptions                                  |\n",

      "+-------+-------------------+--------+--------+------------+------------+----------+--------------------------+--------------+------------------------------------------------------+\n",

      "Profiling time: 115.656699 seconds\n"

     ]

    }

   ],

   "source": [

    "!mltk profile keyword_spotting_pacman_v3 --device --build --accelerator MVP"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "This command builds the model then profiles it on the development board using the [MVP](https://docs.silabs.com/gecko-platform/latest/machine-learning/tensorflow/mvp-accelerator) hardware accelerator."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Training the model\n",

    "\n",

    "Once the [model specification](https://github.com/SiliconLabs/mltk/tree/master/mltk/models/siliconlabs/keyword_spotting_pacman_v3.py) is ready, it can be [trained](../../docs/guides/model_training.md) with the command:"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "metadata": {},

   "outputs": [],

   "source": [

    "!mltk train keyword_spotting_pacman_v3"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Train in cloud\n",

    "\n",

    "Alternatively, you can _vastly_ improve the model training time by training this model in the \"cloud\".  \n",

    "See the tutorial: [Cloud Training with vast.ai](../../mltk/tutorials/cloud_training_with_vast_ai.md) for more details."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "After training completes, a [model archive](https://github.com/SiliconLabs/mltk/tree/master/mltk/models/siliconlabs/keyword_spotting_pacman_v3.mltk.zip) file is generated containing the quantized `.tflite` model file. This is the file that is built into the firmware application."

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Creating the Firmware Application\n",

    "\n",

    "The [BLE Audio Classifier](../../docs/cpp_development/examples/ble_audio_classifier.md) C++ example application may be used with the train model.\n",

    "\n",

    "The application uses the [Audio Feature Generator](https://siliconlabs.github.io/mltk/docs/audio/audio_feature_generator.html#gecko-sdk-component) library to generate spectrograms from the streaming microphone audio.\n",

    "The spectrograms are then passed to the [Tensorflow-Lite Micro](https://github.com/tensorflow/tflite-micro) inference engine which uses the trained model from above to make predictions on if a keyword is found in the spectrogram.\n",

    "If a keyword is detected, a connected BLE client is sent a notification containing the detected class ID of the keyword and prediction probability."

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Creating the Pac-Man Webpage\n",

    "\n",

    "A [Pac-Man Webpage](https://github.com/SiliconLabs/mltk/blob/master/cpp/shared/apps/ble_audio_classifier/web/pacman) is available that allows for playing the game \"Pac-Man\" using\n",

    "the keywords detected by the firmware application described above.\n",

    "\n",

    "This webpage was adapted from a game created by Lucio Panpinto, view original source code on [GitHub](https://github.com/luciopanepinto/pacman).\n",

    "\n",

    "The webpage was modified to use the [p5.ble.js](https://itpnyu.github.io/p5ble-website/) library for communicating with the firmware application via Bluetooth Low Energy.\n"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Running the Demo\n",

    "\n",

    "With the following components complete:  \n",

    "- Keyword spotting machine learning model\n",

    "- Firmware application with Audio Feature Generator, Tensorflow-Lite Micro, and Bluetooth libraries\n",

    "- Pac-Man webpage with Bluetooth\n",

    "\n",

    "We can now run the demo.\n",

    "\n",

    "A live demo may be found at: [https://mltk-pacman.web.app](https://mltk-pacman.web.app).\n",

    "\n",

    "Alternatively, you can build the firmware application from source and run the webpage locally:"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Build firmware application from source\n",

    "\n",

    "The MLTK supports building [C++ Applications](../../docs/cpp_development/index.md).\n",

    "\n",

    "It also features a [ble_audio_classifier](../../docs/cpp_development/examples/ble_audio_classifier.md) C++ application\n",

    "which can be built using:  \n",

    "- [Visual Studio Code](../../docs/cpp_development/vscode.md) \n",

    "- [Simplicity Studio](../../docs/cpp_development/simplicity_studio.md)\n",

    "- [Command Line](../../docs/cpp_development/command_line.md)\n",

    "\n",

    "Refer to the [ble_audio_classifier](../../docs/cpp_development/examples/ble_audio_classifier.md) application's documentation\n",

    "for how include your model into the built application."

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Run webpage locally\n",

    "\n",

    "The demo's webpage uses \"vanilla\" javascript+css+html. No special build systems are required.\n",

    "\n",

    "To run the webpage locally, simply open [index.html](https://github.com/SiliconLabs/mltk/blob/master/cpp/shared/apps/ble_audio_classifier/web/pacman/index.html) in your web browser (NOTE: double-click the __locally cloned__ `index.html` on your PC, not the one on Github).\n",

    "\n",

    "When the webpage starts, follow the instructions but do _not_ program the `.s37`. The locally built firmware application should have already been programmed as described in the the previous section."

   ]

  }

 ],

 "metadata": {

  "kernelspec": {

   "display_name": ".venv",

   "language": "python",

   "name": "python3"

  },

  "language_info": {

   "codemirror_mode": {

    "name": "ipython",

    "version": 3

   },

   "file_extension": ".py",

   "mimetype": "text/x-python",

   "name": "python",

   "nbconvert_exporter": "python",

   "pygments_lexer": "ipython3",

   "version": "3.10.8"

  },

  "orig_nbformat": 4,

  "vscode": {

   "interpreter": {

    "hash": "1b794eb47024974fee893fdb7015f3d322c4012087fc39c73069299b7c169399"

   }

  }

 },

 "nbformat": 4,

 "nbformat_minor": 2

}

