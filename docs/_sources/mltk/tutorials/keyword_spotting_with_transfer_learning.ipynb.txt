{

 "cells": [

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "# Keyword Spotting with Transfer Learning\n",

    "\n",

    "This tutorial describes how to use the weights from the pre-trained trained model [keyword_spotting_mobilenetv2](https://github.com/siliconlabs/mltk/blob/master/mltk/models/siliconlabs/keyword_spotting_mobilenetv2.py),\n",

    "as a starting point for training a new model, [keyword_spotting_with_transfer_learning.py](https://github.com/siliconlabs/mltk/blob/master/mltk/models/siliconlabs/keyword_spotting_with_transfer_learning.py), to detect the keywords:\n",

    "\n",

    "- __one__\n",

    "- __two__\n",

    "- __three__ \n",

    "- __four__\n",

    "- __five__\n",

    "- __six__\n",

    "- __seven__\n",

    "- __eight__\n",

    "- __nine__\n",

    "\n",

    "This process is known as [Transfer Learning](https://en.wikipedia.org/wiki/Transfer_learning) and can greatly improve training times as\n",

    "the new model can leverage the knowledge learnt from another model.\n",

    "\n",

    "See the [Keras Documentation](https://keras.io/guides/transfer_learning) for more details about how to use\n",

    "the Keras API to enable transfer learning."

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Quick Links\n",

    "\n",

    "- [GitHub Source](https://github.com/SiliconLabs/mltk/blob/master/mltk/tutorials/keyword_spotting_with_transfer_learning.ipynb) - View this tutorial on Github\n",

    "- [Run on Colab](https://colab.research.google.com/github/siliconlabs/mltk/blob/master/mltk/tutorials/keyword_spotting_with_transfer_learning.ipynb) - Run this tutorial on Google Colab\n",

    "- [C++ Example Application](../../docs/cpp_development/examples/audio_classifier.md) - View this tutorial's associated C++ example application\n",

    "- [Machine Learning Model](../../docs/python_api/models/siliconlabs/keyword_spotting_with_transfer_learning.md) - View this tutorial's associated machine learning model"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Running this Tutorial\n",

    " \n",

    "- This tutorial assumes the MLTK has been [installed](../../docs/installation.md) and is available on the [command-line](../../docs/command_line.md)\n",

    "- All commands below should run from a local terminal\n",

    "- In your local terminal, replace the `!mltk` command with `mltk` (i.e. remove the `!` character to run the command)"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Recommended Reading\n",

    "\n",

    "Before getting started, it is recommended to review the following documentation:\n",

    "\n",

    "- [MLTK Overview](../../docs/overview.md) - An overview of the core concepts used by the MLTK\n",

    "- [Keyword Spotting Overview](../../docs/audio/keyword_spotting_overview.md) - An overview of how keyword spotting works\n",

    "- [Keyword Spotting Tutorial](../../mltk/tutorials/keyword_spotting_on_off.md) - Detailed tutorial describing how to create a Keyword Spotting model using the MLTK\n",

    "- [Model Optimization Tutorial](../../mltk/tutorials/model_optimization.md) - Describes how to develop the [keyword_spotting_mobilenetv2](https://github.com/siliconlabs/mltk/blob/master/mltk/models/siliconlabs/keyword_spotting_mobilenetv2.py) model used by this tutorial"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Basic Concept\n",

    "\n",

    "Before continuing, it helps to understand the basic idea of how a Convolutional Neural Network (CNN) works.\n",

    "\n",

    "The following diagram illustrates a typical CNN model:  \n",

    "![CNN Diagram](https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png)  \n",

    "(By Aphex34 - Own work, [CC BY-SA 4.0](https://commons.wikimedia.org/w/index.php?curid=45679374))\n",

    "\n",

    "There are two basic parts:  \n",

    "- __Feature Extraction__ - Convolutional filters extract meaningful information (e.g. lines, shapes, textures, colors) from the input image. Multiple layers of filters are used to convert the raw image into a more abstract and compressed representation\n",

    "- __Classifier__ - The final Convolutional layer is feed into a Fully Connected layer. The Fully Connected layer converts the abstract, compressed representation generated by the Convolutional layers into a probability distribution of the possible \"class\" or object type (e.g. dog, cat, fish) to which the input image belongs.\n",

    "\n",

    "\n",

    "(Of course, it can get way more complex than this, but that is basically what is going on).\n",

    "\n",

    "So how does the CNN model learn to extract the features and generate the probability distribution?  \n",

    "Each \"layer\" of the model has \"trainable\" parameters such as weights and filters. Initially, these weights and filters are set to [random values](https://keras.io/api/layers/initializers/).\n",

    "During model training, the weights and filters are adjusted so that the model's predictions are as accurate as possible for the given parameters and dataset.\n",

    "At the end of training, the weights and filters that gave the best predictions are saved to a file and ultimately programmed to the embedded device.\n",

    "\n",

    "Since the weights and filters are initially random, it can take a long time to train a model from scratch.  \n",

    "The idea behind transfer learning is to train a new model starting with the best weights and filters of a previously trained model. \n",

    "If the datasets are similar then the new model does not need to relearn how to extract the features (e.g. lines, shapes, textures, colors).  \n",

    "It just needs to learn how to map the abstract, compressed representation of the convolutional layers to the new \"classes\" or objects (e.g. car, truck, bike)."

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Base Model Overview\n",

    "\n",

    "The pre-trained, base model, whose weights are transferred to the new model, may be found on Github: [keyword_spotting_mobilenetv2](https://github.com/siliconlabs/mltk/blob/master/mltk/models/siliconlabs/keyword_spotting_mobilenetv2.py).\n",

    "\n",

    "This model is built using [MobileNetv2](https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html), an industry-standard classification model developed by Google.\n",

    "MobileNetV2 is a useful model because it is generic enough that it can be applied to most classification tasks but still runs efficiently on embedded devices.\n",

    "\n",

    "This model is designed to detect the following keywords:  \n",

    "- left\n",

    "- right\n",

    "- up\n",

    "- down\n",

    "- stop\n",

    "- go"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Test model using PC microphone\n",

    "\n",

    "If you have a microphone connected to your computer, you can optionally run the following command to see the model detect the keywords:"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "metadata": {},

   "outputs": [],

   "source": [

    "# Test the keyword_spotting_mobilenetv2 using the PC's microphone\n",

    "# We simulate the latency to be 130ms as that's the approximate latency\n",

    "# that would be seen on the development board\n",

    "!mltk classify_audio keyword_spotting_mobilenetv2 --latency 130"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Test model using development board\n",

    "\n",

    "If you have a supported development board (currently just the BRD2601), you can optionally run the following command to see the model detected keywords using the development board's microphone:"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "metadata": {},

   "outputs": [],

   "source": [

    "# Test the keyword_spotting_mobilenetv2 using the development board's microphone\n",

    "# The red LED will turn on when a keyword is detected\n",

    "# The green LED will turn on when there's audio activity\n",

    "# NOTE: Your mouth should be ~2 inches from the microphone\n",

    "!mltk classify_audio keyword_spotting_mobilenetv2 --device --accelerator mvp"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Configure Model Specification with Transfer Learning\n",

    "\n",

    "The completed model specification used by this tutorial may be found on Github: [keyword_spotting_with_transfer_learning.py](https://github.com/siliconlabs/mltk/blob/master/mltk/models/siliconlabs/keyword_spotting_with_transfer_learning.py)\n",

    "\n",

    "This model specification is very similar to the base model specification, [keyword_spotting_mobilenetv2](https://github.com/siliconlabs/mltk/blob/master/mltk/models/siliconlabs/keyword_spotting_mobilenetv2.py), with the following key differences:"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Update description\n",

    "\n",

    "Update the model's description to help keep track of it in the field."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "metadata": {},

   "outputs": [],

   "source": [

    "my_model.description = 'Keyword spotting classifier using transfer learning to detect: \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"'"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Set epochs to small value\n",

    "\n",

    "Since this model is leveraging the knowledge of the base model, it does not need many epochs to tune the model parameters (e.g. filters and weights)"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "metadata": {},

   "outputs": [],

   "source": [

    "# Since we're using transfer learning, we should only need\n",

    "# a small number of epochs to get a well trained model\n",

    "my_model.epochs = 10"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Use LearningRateScheduler with small initial value\n",

    "\n",

    "To have better control of the learning rate, we use a [LearningRateScheduler](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler). We also set the initial value to a small value."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "metadata": {},

   "outputs": [],

   "source": [

    "def lr_schedule(epoch):\n",

    "    # When using transfer learning, the initial learning rate should start at a fairly small value\n",

    "    initial_learning_rate = 0.0005\n",

    "    decay_per_epoch = 0.95\n",

    "    lrate = initial_learning_rate * (decay_per_epoch ** epoch)\n",

    "    return lrate\n",

    "\n",

    "my_model.lr_schedule = dict(\n",

    "    schedule = lr_schedule,\n",

    "    verbose = 1\n",

    ")"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Update the keywords to detect\n",

    "\n",

    "Update the keywords we want this model to detect.\n",

    "\n",

    "While the keywords \"one\" through \"nine\" are supported, this can greatly increase the training time as each keyword adds ~3k samples to the training dataset.  \n",

    "\n",

    "__If you want to quickly train a model, it is recommended to only use a few keywords.__"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "metadata": {},

   "outputs": [],

   "source": [

    "# Using all 9 keywords may take a LONG TIME to train\n",

    "# my_model.classes = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', '_unknown_', '_silence_']\n",

    "\n",

    "# Using 3 keywords should take a much SHORTER TIME to train\n",

    "my_model.classes = ['one', 'two', 'three', '_unknown_', '_silence_']"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Reduce the \"unknown_class_percentage\"\n",

    "\n",

    "To help reduce training time, we reduce the size of the dataset by decreasing the size of the \"unknown\" data samples dynamically generated by the [ParallelAudioDataGenerator](../../docs/python_api/data_preprocessing/audio_data_generator.md)"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "metadata": {},

   "outputs": [],

   "source": [

    "unknown_class_percentage=1.0"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Load the weights from the base model\n",

    "\n",

    "The most important change is actually loading the weights from the base model into this model.\n",

    "\n",

    "To loads the weights:\n",

    "1. Instantiate a [MobileNetV2()](https://siliconlabs.github.io/mltk/docs/python_api/models/common_models.html#mobilenet-v2) instance\n",

    "2. Load the keyword_spotting_mobilenetv2 [MtlkObject](../../docs/python_api/core/mltk_model.md)\n",

    "3. Retrieve the path to the model's .h5 file in the model's [archive](../../docs/guides/model_archive.md). The .h5 model file contains the trained weights\n",

    "4. [Load](https://keras.io/api/models/model_saving_apis/#loadweights-method) the trained weights into the MobileNetV2() instance\n",

    "5. Compile and return the model instance with the weights from the base model\n",

    "\n",

    "__NOTE:__ Keras also describes ways of \"freezing\" layers of the model so that the weights do not change during training. More details [here](https://keras.io/guides/transfer_learning)."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "metadata": {},

   "outputs": [],

   "source": [

    "def my_model_builder(model: MyModel):\n",

    "    # Create an instance of the MobileNetV2\n",

    "    # NOTE: This should have similar parameters to the keyword_spotting_mobilenetv2 model\n",

    "    #       since we're transferring weights from it\n",

    "    keras_model = MobileNetV2( \n",

    "        input_shape=model.input_shape,\n",

    "        classes=model.n_classes,\n",

    "        alpha=0.15, \n",

    "        last_block_filters=384,\n",

    "        include_top=True,\n",

    "        weights=None\n",

    "    )\n",

    "\n",

    "    # Load the \"keyword_spotting_mobilenetv2\" model\n",

    "    # We want to transfer its weights to this model\n",

    "    # In this way, this new model can start with the knowledge\n",

    "    # that the keyword_spotting_mobilenetv2 model already knows\n",

    "    # NOTE: This step is not needed if you already have a .h5 file\n",

    "    base_mltk_model = load_mltk_model('keyword_spotting_mobilenetv2')\n",

    "\n",

    "    # Get the file path to the .h5 file found in the keyword_spotting_mobilenetv2 model archive\n",

    "    # The .h5 file contains the trained weights we want to transfer to this model\n",

    "    base_model_h5_path = base_mltk_model.h5_archive_path\n",

    "\n",

    "    # Load the keyword_spotting_mobilenetv2 weights into this model\n",

    "    keras_model.load_weights(\n",

    "        base_model_h5_path, \n",

    "        by_name=True, \n",

    "        skip_mismatch=True # We need to skip mismatches in case the number of classes is different\n",

    "    )\n",

    "\n",

    "    # NOTE: The https://keras.io/guides/transfer_learning recommends\n",

    "    #       \"freezing\" layers of the base model during training, however, in this instance,\n",

    "    #       it was found that making all layers trainable gave better performance.\n",

    "\n",

    "    keras_model.compile(\n",

    "        loss=model.loss, \n",

    "        optimizer=model.optimizer, \n",

    "        metrics=model.metrics\n",

    "    )\n",

    "    return keras_model"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Train the Model\n",

    "\n",

    "With the model specification complete, invoke the model training.\n",

    "Since we're only training for 10 epochs, this should complete relatively quickly."

   ]

  },

  {

   "cell_type": "code",

   "execution_count": 1,

   "metadata": {},

   "outputs": [

    {

     "ename": "SyntaxError",

     "evalue": "invalid syntax (1014115173.py, line 1)",

     "output_type": "error",

     "traceback": [

      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    mltk train keyword_spotting_with_transfer_learning\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"

     ]

    }

   ],

   "source": [

    "!mltk train keyword_spotting_with_transfer_learning"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Test the model\n",

    "\n",

    "With the model trained, we can see how well it runs on the development board by issuing the command:"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "metadata": {},

   "outputs": [],

   "source": [

    "# Test the keyword_spotting_with_transfer_learning using the development board's microphone\n",

    "# The red LED will turn on when a keyword is detected\n",

    "# The green LED will turn on when there's audio activity\n",

    "# NOTE: Your mouth must be ~2 inches for the board's microphone\n",

    "!mltk classify_audio keyword_spotting_with_transfer_learning --device --accelerator mvp"

   ]

  }

 ],

 "metadata": {

  "interpreter": {

   "hash": "d2cfb25ea30f37ddda2085817c91f6bd2a4a914387b5b179eb21bf4600b69cf8"

  },

  "kernelspec": {

   "display_name": "Python 3.9.7 ('.venv': venv)",

   "language": "python",

   "name": "python3"

  },

  "language_info": {

   "codemirror_mode": {

    "name": "ipython",

    "version": 3

   },

   "file_extension": ".py",

   "mimetype": "text/x-python",

   "name": "python",

   "nbconvert_exporter": "python",

   "pygments_lexer": "ipython3",

   "version": "3.9.7"

  },

  "orig_nbformat": 4

 },

 "nbformat": 4,

 "nbformat_minor": 2

}

