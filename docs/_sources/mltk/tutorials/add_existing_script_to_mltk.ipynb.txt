{

 "cells": [

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "# Add an Existing Script to the MLTK\n",

    "\n",

    "The MLTK is a collection of tools to aid the development of machine learning models for Silicon Labs embedded devices.\n",

    "As part of its offering, the MLTK features an _optional_ Python [API](../../docs/python_api/index.md) to simplify ML model development using the [Tensorflow](https://tensorflow.org/api_docs/python/tf) framework. The MLTK Python [API](../../docs/python_api/index.md) combines and simplifies several different ML model development tasks including:  \n",

    "- [Data preprocessing](../../docs/python_api/data_preprocessing/index.md)\n",

    "- [Model training](../../docs/guides/model_training.md)\n",

    "- [Model quantization](../../docs/guides/model_quantization.md)\n",

    "- [Model evaluation](../../docs/guides/model_evaluation.md)\n",

    "- [Model profiling](../../docs/guides/model_profiler.md)\n",

    "\n",

    "The MLTK Python [API](../../docs/python_api/index.md) is effectively a light wrapper around the [Tensorflow API](https://www.tensorflow.org/api_docs/python/tf).\n",

    "As such, very little needs to be done to convert an existing Tensorflow model training script to support the MLTK, and, thus, gain the additional MLTK features (quantization, evaluation, profiling).\n",

    "\n",

    "__NOTE:__ Converting your existing model script to the MLTK is __not required__. The only hard requirement to use your model on a Silicon Labs embedded device is that your model be converted to the [.tflite](https://www.tensorflow.org/lite/models/convert/convert_models) format with `int8` quantization."

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Setup\n",

    "\n",

    "Before continuing with this tutorial, ensure you have the MLTK Python package [installed](https://siliconlabs.github.io/mltk/docs/installation.html#standard-python-package) and that the virtual environment is activated (if applicable)."

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Existing Script\n",

    "\n",

    "For this tutorial, we use the Keras example: [Simple MNIST convnet](https://keras.io/examples/vision/mnist_convnet/)\n",

    "\n",

    "The source code for the model training script may be found on [GitHub](https://github.com/keras-team/keras-io/blob/master/examples/vision/mnist_convnet.py).\n",

    "\n",

    "It has also been copy & pasted as follows:"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Original mnist_convnet.py\n",

    "\n",

    "```python\n",

    "import numpy as np\n",

    "from tensorflow import keras\n",

    "from tensorflow.keras import layers\n",

    "\n",

    "\"\"\"\n",

    "## Prepare the data\n",

    "\"\"\"\n",

    "\n",

    "# Model / data parameters\n",

    "num_classes = 10\n",

    "input_shape = (28, 28, 1)\n",

    "\n",

    "# Load the data and split it between train and test sets\n",

    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",

    "\n",

    "# Scale images to the [0, 1] range\n",

    "x_train = x_train.astype(\"float32\") / 255\n",

    "x_test = x_test.astype(\"float32\") / 255\n",

    "# Make sure images have shape (28, 28, 1)\n",

    "x_train = np.expand_dims(x_train, -1)\n",

    "x_test = np.expand_dims(x_test, -1)\n",

    "print(\"x_train shape:\", x_train.shape)\n",

    "print(x_train.shape[0], \"train samples\")\n",

    "print(x_test.shape[0], \"test samples\")\n",

    "\n",

    "\n",

    "# convert class vectors to binary class matrices\n",

    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",

    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",

    "\n",

    "\"\"\"\n",

    "## Build the model\n",

    "\"\"\"\n",

    "\n",

    "model = keras.Sequential(\n",

    "    [\n",

    "        keras.Input(shape=input_shape),\n",

    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",

    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",

    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",

    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",

    "        layers.Flatten(),\n",

    "        layers.Dropout(0.5),\n",

    "        layers.Dense(num_classes, activation=\"softmax\"),\n",

    "    ]\n",

    ")\n",

    "\n",

    "model.summary()\n",

    "\n",

    "\"\"\"\n",

    "## Train the model\n",

    "\"\"\"\n",

    "\n",

    "batch_size = 128\n",

    "epochs = 15\n",

    "\n",

    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",

    "\n",

    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",

    "\n",

    "\"\"\"\n",

    "## Evaluate the trained model\n",

    "\"\"\"\n",

    "\n",

    "score = model.evaluate(x_test, y_test, verbose=0)\n",

    "print(\"Test loss:\", score[0])\n",

    "print(\"Test accuracy:\", score[1])\n",

    "```"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## MLTK Modifications\n",

    "\n",

    "Download the [mnist_convnet.py](https://raw.githubusercontent.com/keras-team/keras-io/master/examples/vision/mnist_convnet.py) script to your local PC, and add the following modifications to it so that it we can execute it in the MLTK.\n",

    "\n",

    "All modifications are prefixed with the comment: `# Modified for the MLTK`\n",

    "\n",

    "Copy & paste the following into the `mnist_convnet.py` script on your local PC:"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Modified mnist_convnet.py\n",

    "\n",

    "```python\n",

    "import numpy as np\n",

    "from tensorflow import keras\n",

    "from tensorflow.keras import layers\n",

    "\n",

    "\"\"\"\n",

    "## Prepare the data\n",

    "\"\"\"\n",

    "\n",

    "# Model / data parameters\n",

    "num_classes = 10\n",

    "input_shape = (28, 28, 1)\n",

    "\n",

    "# Load the data and split it between train and test sets\n",

    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",

    "\n",

    "# Scale images to the [0, 1] range\n",

    "x_train = x_train.astype(\"float32\") / 255\n",

    "x_test = x_test.astype(\"float32\") / 255\n",

    "# Make sure images have shape (28, 28, 1)\n",

    "x_train = np.expand_dims(x_train, -1)\n",

    "x_test = np.expand_dims(x_test, -1)\n",

    "print(\"x_train shape:\", x_train.shape)\n",

    "print(x_train.shape[0], \"train samples\")\n",

    "print(x_test.shape[0], \"test samples\")\n",

    "\n",

    "\n",

    "# convert class vectors to binary class matrices\n",

    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",

    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",

    "\n",

    "\"\"\"\n",

    "## Build the model\n",

    "\"\"\"\n",

    "\n",

    "model = keras.Sequential(\n",

    "    [\n",

    "        keras.Input(shape=input_shape),\n",

    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",

    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",

    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",

    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",

    "        layers.Flatten(),\n",

    "        layers.Dropout(0.5),\n",

    "        layers.Dense(num_classes, activation=\"softmax\"),\n",

    "    ]\n",

    ")\n",

    "\n",

    "model.summary()\n",

    "\n",

    "\"\"\"\n",

    "## Train the model\n",

    "\"\"\"\n",

    "\n",

    "batch_size = 128\n",

    "epochs = 15\n",

    "\n",

    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",

    "\n",

    "# Modified for the MLTK\n",

    "# We only want to train when calling the train_model() API\n",

    "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",

    "\n",

    "\"\"\"\n",

    "## Evaluate the trained model\n",

    "\"\"\"\n",

    "\n",

    "# Modified for the MLTK\n",

    "# We only want to evaluate when calling teh evaluate_model() API\n",

    "#score = model.evaluate(x_test, y_test, verbose=0)\n",

    "#print(\"Test loss:\", score[0])\n",

    "#print(\"Test accuracy:\", score[1])\n",

    "\n",

    "\n",

    "# Modified for the MLTK\n",

    "# The follow allows for running this existing model training script in the MLTK\n",

    "\n",

    "# Import the MLTK Python package\n",

    "import mltk.core as mltk_core \n",

    "import tensorflow as tf\n",

    "\n",

    "# Instantiate the MltkModel object\n",

    "class MyModel(\n",

    "    mltk_core.MltkModel,\n",

    "    mltk_core.TrainMixin,\n",

    "    mltk_core.DatasetMixin,\n",

    "    mltk_core.EvaluateClassifierMixin, \n",

    "):\n",

    "    pass\n",

    "my_model = MyModel()\n",

    "\n",

    "# This simply returns the previously built KerasModel\n",

    "def my_model_builder(mltk_model):\n",

    "    return model\n",

    "\n",

    "# This callback allows for loading the training or testing dataset\n",

    "def my_dataset_loader(\n",

    "    subset:str,\n",

    "    test:bool, \n",

    "    **kwargs\n",

    "):\n",

    "    if subset == 'training':\n",

    "        return x_train, y_train \n",

    "    else:\n",

    "        return x_test, y_test\n",

    "\n",

    "# This is used by the TfliteConverter when generating the quantized .tflite\n",

    "def my_representative_dataset_generator():\n",

    "    for input_value in tf.data.Dataset.from_tensor_slices(my_model.x).batch(1).take(100):\n",

    "        yield [input_value]\n",

    "\n",

    "\n",

    "# Populate the various MltkModel properties\n",

    "my_model.build_model_function = my_model_builder\n",

    "my_model.dataset = my_dataset_loader\n",

    "my_model.epochs = epochs\n",

    "my_model.batch_size = batch_size \n",

    "my_model.validation_split = 0.1\n",

    "my_model.classes = ['0','1','2','3','4','5','6','7','8','9']\n",

    "my_model.tflite_converter['representative_dataset'] = my_representative_dataset_generator\n",

    "\n",

    "# If this script is being invoked from the command-line then execute it now, e.g.:\n",

    "# python mnist_convnet.py\n",

    "if __name__ == '__main__':\n",

    "    from mltk import cli\n",

    "    cli.get_logger(verbose=False)\n",

    "\n",

    "    train_results = mltk_core.train_model(my_model, clean=True)\n",

    "    print(train_results)\n",

    "\n",

    "    tflite_eval_results = mltk_core.evaluate_model(my_model, verbose=True)\n",

    "    print(tflite_eval_results)\n",

    "\n",

    "    profiling_results = mltk_core.profile_model(my_model)\n",

    "    print(profiling_results)\n",

    "```"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Run the modified script\n",

    "\n",

    "If you invoke the modified script with the command:\n",

    "\n",

    "```shell\n",

    "python mnist_convnet.py\n",

    "```\n",

    "\n",

    "Then, instead of just using Tensorflow for model training like the original script used, the modified script will use the MLTK and perform the following:\n",

    "1. Train the model using Tensorflow\n",

    "2. Generate a quantized `.tflite` model file\n",

    "3. Evaluate the quantized `.tflite` model\n",

    "4. Profile the `.tflite` model in the Tensorflow-Lite Micro simulator\n"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Run from command-line\n",

    "\n",

    "Additionally, the model script can either directly invoke the model script from Python (e.g. `python mnist_convnet.py` ) or use the `mltk` command, e.g.:\n",

    "\n",

    "```shell\n",

    "# Train the model using the \"mltk train\" command\n",

    "mltk train mnist_convnet.py\n",

    "# Evaluate the model using the \"mltk evaluate\" command\n",

    "mltk evaluate mnist_convnet.py\n",

    "# Profile the model using the \"mltk profile\" command\n",

    "mltk profile mnist_convnet.py\n",

    "```"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Recommended Modifications\n",

    "\n",

    "While the above script works with minimal changes to the original script, it does have some deficiencies.\n",

    "\n",

    "For instance, the dataset is always loaded each time the model script is loaded. Ideally, the dataset would only be loaded when it is actually needed before training or evaluation.\n",

    "Similarly, the Keras model is always built each time the model script is loaded. Ideally, the model would only be built before training is invoked.\n",

    "\n",

    "To account for these issues, the MLTK features the callbacks:\n",

    "- [my_model.build_model_function](https://siliconlabs.github.io/mltk/docs/python_api/mltk_model/train_mixin.html#mltk.core.TrainMixin.build_model_function)\n",

    "- [my_model.dataset](https://siliconlabs.github.io/mltk/docs/python_api/mltk_model/dataset_mixin.html#mltk.core.DatasetMixin.dataset)\n",

    "\n",

    "The MLTK will invoke these callbacks at the necessary time for the given model task.\n",

    "\n",

    "These changes may be found in the [basic_example.py](../../docs/python_api/models/examples/basic_example.md) reference model. \n",

    "This model is based on [mnist_convnet.py](https://github.com/keras-team/keras-io/blob/master/examples/vision/mnist_convnet.py) but has been refactored to better align with the MLTK development."

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### basic_example.py\n",

    "\n",

    "```python\n",

    "from typing import Tuple\n",

    "import numpy as np\n",

    "import tensorflow as tf\n",

    "from mltk import core as mltk_core\n",

    "\n",

    "##########################################################################\n",

    "# Prepare the model parameters\n",

    "classes = ['0','1','2','3','4','5','6','7','8','9']\n",

    "num_classes = len(classes)\n",

    "input_shape = (28, 28, 1)\n",

    "batch_size = 128\n",

    "epochs = 15\n",

    "validation_split = 0.1\n",

    "\n",

    "##########################################################################\n",

    "# Prepare the dataset\n",

    "def my_dataset_loader(\n",

    "    subset:str,\n",

    "    test:bool, \n",

    "    **kwargs\n",

    ") -> Tuple[np.ndarray, np.ndarray]:\n",

    "    \"\"\"Load the dataset subset\n",

    "    \n",

    "    This is called automatically by the MLTK before training\n",

    "    or evaluation.\n",

    "    \n",

    "    Args:\n",

    "        subset: The dataset subset to return: 'training' or 'evaluation'\n",

    "        test: This is optional, it is used when invoking a training \"dryrun\", e.g.: mltk train basic_example-test\n",

    "            If this is true, then only return a small portion of the dataset for testing purposes\n",

    "\n",

    "    Return:\n",

    "        A tuple (x,y) for the subset\n",

    "    \"\"\"\n",

    "    # Load the data and split it between train and test sets\n",

    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",

    "\n",

    "    # Scale images to the [0, 1] range\n",

    "    x_train = x_train.astype(\"float32\") / 255\n",

    "    x_test = x_test.astype(\"float32\") / 255\n",

    "    # Make sure images have shape (28, 28, 1)\n",

    "    x_train = np.expand_dims(x_train, -1)\n",

    "    x_test = np.expand_dims(x_test, -1)\n",

    "\n",

    "    # If we're just testing, then truncate the dataset\n",

    "    if test:\n",

    "        x_test = x_test[:64]\n",

    "        x_train = x_train[:64]\n",

    "        y_train = y_train[:64]\n",

    "        y_test = y_test[:64]\n",

    "\n",

    "\n",

    "    # This is optional, but useful for automatically generating a summary of the dataset\n",

    "    if subset == 'training':\n",

    "        my_model.class_counts['training'] = { my_model.classes[class_id]: count for (class_id, count) in enumerate(np.bincount(y_train)) }\n",

    "    else:\n",

    "        my_model.class_counts['evaluation'] = { my_model.classes[class_id]: count for (class_id, count) in enumerate(np.bincount(y_test)) }\n",

    "\n",

    "    # Convert class vectors to binary class matrices\n",

    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",

    "    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",

    "\n",

    "    if subset == 'training':\n",

    "        return x_train, y_train\n",

    "    \n",

    "    else:\n",

    "        return x_test, y_test\n",

    " \n",

    "\n",

    "##########################################################################\n",

    "# Build the model\n",

    "def my_model_builder(my_model: mltk_core.MltkModel) -> tf.keras.Model:\n",

    "    \"\"\"Build the Keras model\n",

    "    \n",

    "    This is called by the MLTK just before training starts.\n",

    "\n",

    "    Arguments:\n",

    "        my_model: The MltkModel instance\n",

    "    \n",

    "    Returns:\n",

    "        Compiled Keras model instance\n",

    "    \"\"\"\n",

    "\n",

    "    model = tf.keras.Sequential([\n",

    "        tf.keras.Input(shape=input_shape),\n",

    "        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",

    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",

    "        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",

    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",

    "        tf.keras.layers.Flatten(),\n",

    "        tf.keras.layers.Dropout(0.5),\n",

    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",

    "    ])\n",

    "\n",

    "    model.compile(\n",

    "        loss=\"categorical_crossentropy\", \n",

    "        optimizer=\"adam\", \n",

    "        metrics=[\"accuracy\"]\n",

    "    )\n",

    "\n",

    "    return model\n",

    "\n",

    "\n",

    "##########################################################################\n",

    "# Create the MltkModel instance\n",

    "# and set the various properties\n",

    "\n",

    "# @mltk_model\n",

    "class MyModel(\n",

    "    mltk_core.MltkModel,    # We must inherit the MltkModel class\n",

    "    mltk_core.TrainMixin,   # We also inherit the TrainMixin since we want to train this model\n",

    "    mltk_core.DatasetMixin, # We also need the DatasetMixin mixin to provide the relevant dataset properties\n",

    "    mltk_core.EvaluateClassifierMixin,  # While not required, also inherit EvaluateClassifierMixin to help will generating evaluation for our classification model \n",

    "):\n",

    "    pass\n",

    "\n",

    "my_model = MyModel()\n",

    "\n",

    "# These properties are optional \n",

    "# but a useful for tracking the generated .tflite\n",

    "my_model.version = 1\n",

    "my_model.description = 'Basic model specification example'\n",

    "my_model.classes = classes\n",

    "my_model.class_weights = 'balanced' # Automatically generate balanced class weights for training\n",

    "\n",

    "\n",

    "# Required: Set the model build function\n",

    "my_model.build_model_function = my_model_builder\n",

    "\n",

    "# Set the other model properties\n",

    "# These values are passed directly to the model.fit() API\n",

    "# https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",

    "my_model.batch_size = batch_size\n",

    "my_model.epochs = epochs\n",

    "my_model.validation_split = validation_split\n",

    "\n",

    "# NOTE: All the other fit() arguments may also be set in the model, e.g.:\n",

    "# my_model.x = x_train\n",

    "# my_model.y = y_train\n",

    "# my_model.step_per_epoch = 60\n",

    "\n",

    "\n",

    "# Set the dataset\n",

    "my_model.dataset = my_dataset_loader\n",

    "# NOTE: Since my_dataset_loader() returns the x,y\n",

    "# We do not need to manually set the my_model.x, my_model.y properties.\n",

    "\n",

    "# NOTE: You can also add the various KerasCallbacks\n",

    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/\n",

    "my_model.train_callbacks = [\n",

    "    tf.keras.callbacks.TerminateOnNaN()\n",

    "]\n",

    "\n",

    "##########################################################################\n",

    "# Specify the .tflite conversion parameters\n",

    "# This is used to convert the float32 model to int8 model \n",

    "# that can run on the embedded device.\n",

    "\n",

    "def my_representative_dataset_generator():\n",

    "    \"\"\"A representative dataset is required generate the .tflite\n",

    "    In this example we just take the first 100 samples from the validation set.\n",

    "\n",

    "    For more details, see:\n",

    "    https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_using_integer-only_quantization\n",

    "\n",

    "    NOTE: Quantization is automatically done at the end of training.\n",

    "    It may also be invoked with:\n",

    "    mltk train basic_example\n",

    "    \"\"\"\n",

    "    for input_value in tf.data.Dataset.from_tensor_slices(my_model.x).batch(1).take(100):\n",

    "        yield [input_value]\n",

    "\n",

    "my_model.tflite_converter['inference_input_type'] = tf.float32\n",

    "my_model.tflite_converter['inference_output_type'] = tf.float32\n",

    "my_model.tflite_converter['representative_dataset'] = my_representative_dataset_generator\n",

    "\n",

    "\n",

    "\n",

    "##########################################################################################\n",

    "# (Optional) Configure model parameters\n",

    "#\n",

    "# While not required, user-defined parameters may be embedded into the .tflite model file.\n",

    "# These parameters may then be read by the embedded device at runtime.\n",

    "# \n",

    "# This is useful for syncing data preprocessing parameters between the model training\n",

    "# script and embedded device.\n",

    "\n",

    "# In my_dataset_loader() we scaled the image data by 1/255.\n",

    "# This same scaling must also happen on the embedded device.\n",

    "# Here, we're embedding the scaling value as \"metadata\" into the generated .tflite.\n",

    "# At runtime, the embedded device should read this value from the .tflite\n",

    "# and use it accordingly.\n",

    "my_model.model_parameters['samplewise_norm.rescale'] = 1/255.\n",

    "\n",

    "# Most standard Python data types may be embedded\n",

    "# See: https://siliconlabs.github.io/mltk/docs/guides/model_parameters.html \n",

    "my_model.model_parameters['my_boolean'] = True \n",

    "my_model.model_parameters['my_string'] = 'This string will be embedded into the .tflite' \n",

    "my_model.model_parameters['my_bytes'] = b'This byte string will be embedded also'\n",

    "my_model.model_parameters['my_float_list'] = [4.5, 2., 3.14]\n",

    "\n",

    "\n",

    "##########################################################################################\n",

    "# (Optional) The following allows for running this model training script directly, e.g.: \n",

    "# python basic_example.py\n",

    "#\n",

    "# Note that this has the similar functionality to:\n",

    "# mltk train basic_example\n",

    "#\n",

    "if __name__ == '__main__':\n",

    "    from mltk import cli\n",

    "    # Setup the CLI logger\n",

    "    cli.get_logger(verbose=False)\n",

    "\n",

    "    # If this is true then this will do a \"dry run\" of the model testing\n",

    "    # If this is false, then the model will be fully trained\n",

    "    test_mode_enabled = True\n",

    "\n",

    "    # Train the model\n",

    "    # This does the same as issuing the command: mltk train basic_example-test --clean\n",

    "    train_results = mltk_core.train_model(my_model, clean=True, test=test_mode_enabled)\n",

    "    print(train_results)\n",

    "\n",

    "    # Evaluate the model against the quantized .h5 (i.e. float32) model\n",

    "    # This does the same as issuing the command: mltk evaluate basic_example-test\n",

    "    tflite_eval_results = mltk_core.evaluate_model(my_model, verbose=True, test=test_mode_enabled)\n",

    "    print(tflite_eval_results)\n",

    "\n",

    "    # Profile the model in the simulator\n",

    "    # This does the same as issuing the command: mltk profile basic_example-test\n",

    "    profiling_results = mltk_core.profile_model(my_model, test=test_mode_enabled)\n",

    "    print(profiling_results)\n",

    "```"

   ]

  },

  {

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Next Steps\n",

    "\n",

    "This tutorial demonstrates how minimal modifications can be added to an existing model training script so that it can run in the MLTK.\n",

    "\n",

    "For complete, non-trivial models, please see the [Reference Models](../../docs/python_api/models/index.md) that come with the MLTK."

   ]

  }

 ],

 "metadata": {

  "kernelspec": {

   "display_name": "Python 3.9.7 ('.venv': venv)",

   "language": "python",

   "name": "python3"

  },

  "language_info": {

   "codemirror_mode": {

    "name": "ipython",

    "version": 3

   },

   "file_extension": ".py",

   "mimetype": "text/x-python",

   "name": "python",

   "nbconvert_exporter": "python",

   "pygments_lexer": "ipython3",

   "version": "3.10.1"

  },

  "orig_nbformat": 4,

  "vscode": {

   "interpreter": {

    "hash": "600e22ae316f8c315f552eaf99bb679bc9438a443c93affde9ac001991b79c8f"

   }

  }

 },

 "nbformat": 4,

 "nbformat_minor": 2

}

