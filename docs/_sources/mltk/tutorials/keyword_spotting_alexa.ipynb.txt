{

 "cells": [

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "# Keyword Spotting - Alexa\n",

    "\n",

    "This demonstrates how to use an embedded [development board](https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit) as the audio source/sink for the [Alexa Voice Services](https://developer.amazon.com/en-US/docs/alexa/alexa-voice-service/get-started-with-alexa-voice-service.html) backend.  \n",

    "With this demo, \"Alexa\" commands may be issued to the development board's microphone and the response will be played via the attached [speaker](https://www.adafruit.com/product/3885)."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Demo Video\n",

    "\n",

    "The following is a video of the demo described in this tutorial:\n",

    "\n",

    "<iframe width=\"281\" height=\"500\"\n",

    "src=\"https://www.youtube.com/embed/dLOIZSyYALo\" \n",

    "frameborder=\"0\" \n",

    "allow=\"autoplay; encrypted-media;\" \n",

    "allowfullscreen></iframe>"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Quick Links\n",

    "\n",

    "- [GitHub Source](https://github.com/SiliconLabs/mltk/blob/master/mltk/tutorials/keyword_spotting_alexa.ipynb) - View this tutorial on Github\n",

    "- [C++ Example Application](../../docs/cpp_development/examples/audio_classifier.md) - View this tutorial's associated C++ example application\n",

    "- [Machine Learning Model](../../docs/python_api/models/siliconlabs/keyword_spotting_alexa.md) - View this tutorial's associated machine learning model\n",

    "- [Alexa Voice Services Docs](https://developer.amazon.com/en-US/docs/alexa/alexa-voice-service/get-started-with-alexa-voice-service.html) - Alexa Voice Services (AVS) documentation\n",

    "- [Analog Speaker](https://www.adafruit.com/product/3885) - The recommended analog speaker used by this demo\n",

    "- [Synthetic Keyword Dataset Generation](../../mltk/tutorials/synthetic_audio_dataset_generation.md) - Describes how to generate an \"Alexa\" dataset using Amazon, Google, and Microsoft Clouds"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Quick start\n",

    "\n",

    "To quickly get this demo running, perform the following steps:\n",

    "\n",

    "1. [Install](https://siliconlabs.github.io/mltk/docs/installation.html#standard-python-package) the MLTK Python package \n",

    "2. Obtain a [BRD2601](https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit) development kit\n",

    "3. Obtain an analog [speaker](https://www.adafruit.com/product/3885) and connect the signal to the pin 9 of the development board (as well as the ground and power signals)\n",

    "4. Run the following MLTK command to program the firmware application and \"Alexa\" ML model to the development board:\n",

    "   \n",

    "   ```\n",

    "   mltk program_app mltk_audio_classifier-audio_io-brd2601-mvp --model keyword_spotting_alexa\n",

    "   ```\n",

    "\n",

    "5. Download the Python script [alexa_demo.py](https://raw.githubusercontent.com/SiliconLabs/mltk/master/cpp/shared/apps/audio_classifier/python/alexa_demo/alexa_demo.py) to your local PC\n",

    "6. From the MLTK Python environment, run the script and follow the instructions for obtaining the AVS cloud credentials:\n",

    "   \n",

    "   ```\n",

    "   python alexa_demo.py\n",

    "   ```\n",

    "\n",

    "7. At this point, you should now be able to speak commands into the develop board's microphone, which will be sent to the AVS cloud, and responses will be returned to the dev board's connected speaker.  \n",

    "   Try saying some of the commands:\n",

    "\n",

    "   - Alexa, what time is it?\n",

    "   - Alexa, tell me a joke\n",

    "   - Alexa, what's 1+1?\n",

    "\n",

    "\n",

    "__NOTE:__ By default, the UART BAUD rate is 115200 which is too slow for larger Alexa responses, see the [Increase Baud Rate](#increase-the-baud-rate) section below for details how to increase the BAUD rate so responses from AVS may be properly played."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Content\n",

    "\n",

    "This tutorial is divided into the following sections:\n",

    "\n",

    "- [System Overview](#system-overview) - Basic overview of the how the system is put together\n",

    "- [Prerequisite Reading](#prerequisite-reading) - Basic information about keyword spotting machine learning\n",

    "- [Required Hardware](#required-hardware) - Details about the hardware needed to run the demo\n",

    "- [Develop the ML Model](#develop-the-ml-model) - Details about how to create a machine learning model to detect the keyword: \"Alexa\"\n",

    "- [Running the demo](#run-the-demo) - Details about how to run the demo ML model and Python script\n",

    "- [Building the C++ application from source](#build-c-application-from-source) - Details about how to build the firmware application from source"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## System Overview\n",

    "\n",

    "The basic system overview is as follows:  \n",

    "![alexa_demo](../../docs/img/alexa_demo.png)\n",

    "\n",

    "1. A [Python script](https://github.com/SiliconLabs/mltk/tree/master/cpp/shared/apps/audio_classifier/python/alexa_demo/alexa_demo.py) runs on the local PC and communicates with the development board via UART\n",

    "1. A user says the keyword \"Alexa\" to the development board's microphone\n",

    "2. The [machine learning model](../../docs/python_api/models/siliconlabs/keyword_spotting_alexa.md) running on the development detects the keyword \n",

    "3. The development board begins compressing the microphone audio using the [Opus codec](https://github.com/SiliconLabs/mltk/tree/master/cpp/shared/opus) and streams the audio to the Python script via UART\n",

    "4. The development board uses a [Voice Activity Detection](https://github.com/SiliconLabs/mltk/tree/master/cpp/shared/voice_activity_detector) (VAD) library to determine when the user finishes the command\n",

    "5. The Python script sends the Opus-encoded audio to the AVS cloud\n",

    "6. The Python scripts receives the Alexa response from the AVS cloud and forwards to the development board via UART\n",

    "7. The development board decompresses the [MP3-encoded](https://github.com/SiliconLabs/mltk/tree/master/cpp/shared/minimp3) audio and streams to the locally connected [speaker](https://www.adafruit.com/product/3885) via [VDAC](https://docs.silabs.com/gecko-platform/latest/emlib/api/efr32xg24/group-vdac) peripheral"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Prerequisite Reading\n",

    "\n",

    "Before continuing with this tutorial, it is recommended to review the following documentation:\n",

    "- [Keyword Spotting Overview](../../docs/audio/keyword_spotting_overview.md) - Provides overview of how embedded keyword spotting works\n",

    "- [Keyword Spotting Tutorial](../../mltk/tutorials/keyword_spotting_on_off.md) - Provides an in-depth tutorial on how to create a keyword spotting model"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Required Hardware\n",

    "\n",

    "To run the full demonstration, the following hardware is required:\n",

    "\n",

    "### EFR32xG24 development kit\n",

    "\n",

    "[EFR32xG24 development kit product page](https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit)\n",

    "\n",

    "### Analog speaker with amplifier\n",

    "\n",

    "\"Alexa\" audio is played via [VDAC](https://docs.silabs.com/gecko-platform/latest/emlib/api/efr32xg24/group-vdac) peripheral of the development board.\n",

    "As such, an analog speaker is required. An amplifier is also necessary.\n",

    "\n",

    "While not required, this [speaker](https://www.adafruit.com/product/3885) provided by Adafruit is recommended.\n",

    "\n",

    "The speaker's analog signal should connect to pin 9 of the development board.\n",

    "\n",

    "### Pinout\n",

    "\n",

    "The following pin connections are required:\n",

    "\n",

    "| Pin header Number | Description                  |\n",

    "|-------------------|------------------------------|\n",

    "| 9                 | VDAC - Speaker analog signal |\n",

    "| 1                 | GND - Speaker ground         |\n",

    "| 20                | 3V3 - Speaker power          |\n",

    "\n",

    "![alexa_demo_pinout](../../docs/img/alexa_demo_pinout.png)"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Develop the ML Model\n",

    "\n",

    "This demo uses the keyword spotting ML model: [keyword_spotting_alexa](../../docs/python_api/models/siliconlabs/keyword_spotting_alexa.md) \n",

    "to detect the keyword \"Alexa\".\n",

    "\n",

    "This model is based on the [Temporal Efficient Neural Network (TENet)](https://arxiv.org/pdf/2010.09960.pdf) model architecture, a keyword spotting architecture with temporal and depthwise convolutions.\n",

    "\n",

    "The following describes how the model was developed."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Create the dataset\n",

    "\n",

    "The most important part of developing a machine learning model is acquiring a __representative__ dataset.  \n",

    "A good, __representative__ dataset should have numerous (e.g. 10k-1M) \"positive\" samples (i.e. audio clips of people saying the word \"Alexa\")\n",

    "and numerous \"negative\" samples (i.e. audio clips of people saying other words besides \"Alexa\").\n",

    "The larger and more diverse the dataset is, the better the model will likely perform in the field.\n",

    "\n",

    "\n",

    "#### Synthetic dataset generation\n",

    "\n",

    "Ideally, the dataset should contain samples of 10k+ people saying the word \"Alexa\". However, creating such a dataset can be very expensive and time-consuming.\n",

    "An alternative approach is to synthetically generate the dataset using cloud-based Text-to-Speech (TTS) services.  \n",

    "Refer to the [Synthetic Audio Dataset Generation](../../mltk/tutorials/synthetic_audio_dataset_generation.md) tutorial for how an \"Alexa\" dataset can be generated using the Google, Microsoft, and Amazon Clouds.\n",

    "\n",

    "#### \"Negative\" class\n",

    "\n",

    "While having a large and diverse \"positive\" class (i.e. samples of people saying \"Alexa\") is important, it is also important to have a large \"negative\" class.\n",

    "This way, the ML model learns not only how to detect the \"Alexa\" keyword, but also how to reject words and noises that sound similar to the \"Alexa\" keyword.\n",

    "\n",

    "Here [synthetic audio dataset generation](../../mltk/tutorials/synthetic_audio_dataset_generation.md) is useful as keywords that sound similar to \"alexa\" (e.g. alice, alexia, etc.) can be generated.\n",

    "\n",

    "Other large, publicly available audio datasets can also be used for the \"negative\" class samples:  \n",

    "- [MLCommons Multilingual Spoken Words](https://mlcommons.org/en/multilingual-spoken-words) - A large and growing audio dataset of spoken words in 50 languages for academic research and commercial applications in keyword spotting and spoken term search, licensed under CC-BY 4.0\n",

    "- [Mozilla Common Voice](https://commonvoice.mozilla.org/en/datasets) - An open source, multi-language dataset of voices that anyone can use to train speech-enabled applications\n",

    "\n",

    "\n",

    "#### Class balance\n",

    "\n",

    "The \"positive\" and \"negative\" classes should have approximately the same number of samples.\n",

    "If one class has substantially more samples then the model may not adequately learn the intricacies of each class.\n",

    "To help account for this, Tensorflow has the concept of [class weights](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights). The [MLTK Model](https://siliconlabs.github.io/mltk/docs/python_api/mltk_model/index.html) also provides a [class_weights](https://siliconlabs.github.io/mltk/docs/python_api/mltk_model/dataset_mixin.html#mltk.core.DatasetMixin.class_weights) property to automatically balance the classes during training.\n",

    "\n",

    "However, class weights may not work for substantially imbalanced (4x or more) datasets, which is the case for the [Keyword Spotting - Alexa](../../docs/python_api/models/siliconlabs/keyword_spotting_alexa.md) model.\n",

    "In this model's case, the \"negative\" class has 10x more samples than the \"positive\" class.\n",

    "\n",

    "To help account for this, the training script forces the \"negative\" class to have 2x the samples of the \"positive\" class. Then during training, all of the \"negative\" samples are randomly shuffled and only a subset is used for each epoch.\n",

    "In this way, the \"negative\" and \"positive\" classes are approximately balanced, yet the model \"sees\" all of the \"negative\" samples through the course of the full model training process."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Select the model architecture\n",

    "\n",

    "Acquiring a representative dataset is the most important step in ML model development.\n",

    "\n",

    "Another important step is defining a model architecture. A good model architecture should have the following characteristics:  \n",

    "- __Able to learn the dataset well__ - Ensures it will be robust in the field\n",

    "- __Able to fit within the target hardware's constraints__ - The model must be small enough to fit within the RAM/Flash memories\n",

    "- __Able to execute within the application's time requirements__ - The model's inference time (on the target hardware) must be low so that the application is responsive (i.e. The model should execute quickly so that there is little delay after saying \"Alexa\")\n",

    "\n",

    "For this application, we choose the [Temporal Efficient Neural Network (TENet)](https://arxiv.org/pdf/2010.09960.pdf) model architecture which has been shown to work well with keyword spotting applications, \n",

    "and, most importantly, is able to efficiently execute on our [embedded device](https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit)."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Determine the audio frontend parameters\n",

    "\n",

    "For this keyword spotting application, we convert raw audio into a spectrogram (gray-scale 2D image) and feed the spectrogram image into the classifier ML model ([TENet](https://arxiv.org/pdf/2010.09960.pdf)).\n",

    "See the [Keyword Spotting Overview](../../docs/audio/keyword_spotting_overview.md) for more details.\n",

    "\n",

    "We use the [Audio Feature Generator](../../docs/audio/audio_feature_generator.md) Python package to generate the spectrogram.\n",

    "\n",

    "This package has numerous [settings](../../docs/python_api/data_preprocessing/audio_feature_generator_settings.md) that determine how the audio is converted to a spectrogram image.\n",

    "We want to choose these settings so that the generated spectrogram has the best quality while at the same time the ML model executes efficiently on the embedded hardware.\n",

    "\n",

    "Typically, the larger the spectrogram's dimensions the better its quality. However, increasing the spectrogram dimensions also increases the input size to the ML model which increases the processing time on the embedded hardware.\n",

    "\n",

    "\n",

    "The MLTK features two tools to aid with the selection of the audio frontend parameters:  \n",

    "- [Audio Visualizer](https://siliconlabs.github.io/mltk/docs/audio/audio_utilities.html#audio-visualization-utility) - Allows for adjusting the AudioFeatureGenerator [settings](https://siliconlabs.github.io/mltk/docs/python_api/data_preprocessing/audio_feature_generator_settings.html) and viewing the resulting spectrogram in real-time\n",

    "- [Model Profiler](../../docs/guides/model_profiler.md) - Allows for running and profiling the ML model on the embedded device _before_ fully training the ML model"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "#### Audio visualizer command\n",

    "\n",

    "To run the Audio Visualizer, issue the command:\n",

    "\n",

    "```shell\n",

    "mltk view_audio\n",

    "```"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "#### AudioFeatureGenerator settings\n",

    "\n",

    "For the [keyword_spotting_alexa](../../docs/python_api/models/siliconlabs/keyword_spotting_alexa.md)  model, we use the following settings:\n",

    "\n",

    "\n",

    "```python\n",

    "frontend_settings = AudioFeatureGeneratorSettings()\n",

    "\n",

    "frontend_settings.sample_rate_hz = 16000\n",

    "frontend_settings.sample_length_ms = 1200                       # Use 1.2s audio clips to ensure the full \"alexa\" keyword is captured\n",

    "frontend_settings.window_size_ms = 30\n",

    "frontend_settings.window_step_ms = 10\n",

    "frontend_settings.filterbank_n_channels = 108                   # We want this value to be as large as possible\n",

    "                                                                # while still allowing for the ML model to execute efficiently on the hardware\n",

    "frontend_settings.filterbank_upper_band_limit = 7500.0\n",

    "frontend_settings.filterbank_lower_band_limit = 125.0           # The dev board mic seems to have a lot of noise at lower frequencies\n",

    "\n",

    "frontend_settings.noise_reduction_enable = True                 # Enable the noise reduction block to help ignore background noise in the field\n",

    "frontend_settings.noise_reduction_smoothing_bits = 10\n",

    "frontend_settings.noise_reduction_even_smoothing =  0.025\n",

    "frontend_settings.noise_reduction_odd_smoothing = 0.06\n",

    "frontend_settings.noise_reduction_min_signal_remaining = 0.40   # This value is fairly large (which makes the background noise reduction small)\n",

    "                                                                # But it has been found to still give good results\n",

    "                                                                # i.e. There is still some background noise reduction,\n",

    "                                                                # but the actual signal is still (mostly) untouched\n",

    "\n",

    "frontend_settings.dc_notch_filter_enable = True                 # Enable the DC notch filter, to help remove the DC signal from the dev board's mic\n",

    "frontend_settings.dc_notch_filter_coefficient = 0.95\n",

    "\n",

    "frontend_settings.quantize_dynamic_scale_enable = True          # Enable dynamic quantization, this dynamically converts the uint16 spectrogram to int8\n",

    "frontend_settings.quantize_dynamic_scale_range_db = 40.0\n",

    "\n",

    "\n",

    "# Add the Audio Feature generator settings to the model parameters\n",

    "# This way, they are included in the generated .tflite model file\n",

    "# See https://siliconlabs.github.io/mltk/docs/guides/model_parameters.html\n",

    "my_model.model_parameters.update(frontend_settings)\n",

    "```"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Profile the model\n",

    "\n",

    "Before spending the time and money to fully train the ML model, it is critical that we profile the model on the embedded device to ensure it is able to efficiently execute.\n",

    "\n",

    "This can be done using the [Model Profiler](../../docs/guides/model_profiler.md)."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "#### Model profiler command\n",

    "\n",

    "Plug the [development board](https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit) in your PC and issue the following command:\n",

    "\n",

    "```shell\n",

    "mltk profile keyword_spotting_alexa --build --device --accelerator mvp\n",

    "```\n",

    "\n",

    "This does the following:\n",

    "- The `--build` option generates a `.tflite` model file using dummy weights (i.e. it generates a non-fully trained ML model)\n",

    "- The `--device` option programs the generated `.tflite` to the locally connected [development board](https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit)\n",

    "- The `--accelerator mvp` option tells the model profiler application to accelerate the ML model with the embedded device's [MVP](https://docs.silabs.com/gecko-platform/latest/machine-learning/tensorflow/mvp-accelerator) hardware accelerator\n",

    "\n",

    "\n",

    "The pre-generated results of this profiling command may found [here](https://siliconlabs.github.io/mltk/docs/python_api/models/siliconlabs/keyword_spotting_alexa.html#model-profiling-report)."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Data augmentations\n",

    "\n",

    "Typically, the larger the dataset the more robust the ML model will be. Another way of increasing the size of the dataset without acquiring more samples is to dynamically augment the audio samples during training.\n",

    "\n",

    "The [keyword_spotting_alexa.py](https://github.com/siliconlabs/mltk/blob/master/mltk/models/siliconlabs/keyword_spotting_alexa.py) ML model augmentations are implemented in the `audio_pipeline_with_augmentations()` function. This function does the following:\n",

    "- Uses the [audiomentations](https://github.com/iver56/audiomentations) Python library to apply the various augmentations\n",

    "- Pads the samples with 1s of silence, this helps the AudioFeatureGenerator's noise reduction block to \"warm up\" when generating the spectrogram\n",

    "- Adds background audio recorded from the [development board's](https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit) microphone - This helps to make the dataset samples sound as if they were recorded by the dev board's mic\n",

    "- Randomly adds other background noises such as conferences, offices, restaurants, etc.\n",

    "- Randomly increases/decreases the sample volume\n",

    "- Randomly crops \"known\" samples and uses the crop sample as an \"unknown\" sample. This should help the model to not trigger on partially buffered keywords, the model should only trigger when the keyword has been fully buffered\n",

    "\n",

    "\n",

    "#### Dump augmentations\n",

    "\n",

    "In the [keyword_spotting_alexa.py](https://github.com/siliconlabs/mltk/blob/master/mltk/models/siliconlabs/keyword_spotting_alexa.py) model specification file, if you uncomment the line (near the middle of the script):\n",

    "\n",

    "```python\n",

    "# Uncomment this to dump the augmented audio samples to the log directory\n",

    "# DO NOT forget to disable this before training the model as it will generate A LOT of data\n",

    "#data_dump_dir = my_model.create_log_dir('dataset_dump')\n",

    "```\n",

    "\n",

    "Then, during training, the dynamically augmented audio samples and corresponding spectrograms will be dumped to the specified directory.  \n",

    "This way, you can listen to the augmented audio samples and view their corresponding spectrogram images.\n",

    "\n",

    "The dumped audio files and spectrogram images will appear in the dump directory similar to:  \n",

    "![audio_augmentations_dump](../../docs/img/audio_augmentations_dump.png)\n",

    "\n",

    "__HINT:__ See the [Model Debugging](../../mltk/tutorials/model_debugging.md) tutorial for how easily debug the model Python script and view the dumped samples."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Train the model\n",

    "\n",

    "Once the parameters are configured and it is determined that the profiled model runs efficiently on the embedded hardware, it is time to fully train the model.\n",

    "The model can either be trained locally or on a remote cloud machine.\n",

    "\n",

    "\n",

    "#### Local Training\n",

    "\n",

    "See the [Local Model Training](../../docs/guides/model_training.md) guide for details on how to train the model on your local machine.\n",

    "\n",

    "```shell\n",

    "mltk train keyword_spotting_alexa\n",

    "```\n",

    "\n",

    "#### Remote Training\n",

    "\n",

    "See the [Remote Model Training](../../docs/guides/model_training_via_ssh.md) guide for details on how to train the model on a remote cloud machine.\n",

    "\n",

    "```shell\n",

    "shell ssh train keyword_spotting_alexa\n",

    "```\n",

    "\n",

    "__HINT:__ This model can be trained in ~2hrs using [vast.ai](../../mltk/tutorials/cloud_training_with_vast_ai.md).  \n",

    "Be sure to select an instance with at least 48 CPUs (only 1 GPU is needed). Also be sure to update the `n_jobs` in [tf_dataset_utils.parallel_process()](https://siliconlabs.github.io/mltk/docs/python_api/data_preprocessing/tf_dataset.html#mltk.core.preprocess.utils.tf_dataset.parallel_process) in [keyword_spotting_alexa.py](https://github.com/siliconlabs/mltk/blob/master/mltk/models/siliconlabs/keyword_spotting_alexa.py), e.g.:\n",

    "\n",

    "```python\n",

    "features_ds, pool = tf_dataset_utils.parallel_process(\n",

    "    features_ds,\n",

    "    audio_pipeline_with_augmentations,\n",

    "    dtype=np.int8,\n",

    "    #n_jobs=84 if subset == 'training' else 32, # These are the settings for a 256 CPU core cloud machine\n",

    "    n_jobs=72 if subset == 'training' else 32, # These are the settings for a 128 CPU core cloud machine\n",

    "    #n_jobs=44 if subset == 'training' else 16, # These are the settings for a 96 CPU core cloud machine\n",

    "    #n_jobs=50 if subset == 'training' else 25, # These are the settings for a 84 CPU core cloud machine\n",

    "    #n_jobs=36 if subset == 'training' else 12, # These are the settings for a 64 CPU core cloud machine\n",

    "    #n_jobs=28 if subset == 'training' else 16, # These are the settings for a 48 CPU core cloud machine\n",

    "    #n_jobs=.65 if subset == 'training' else .35,\n",

    "    #n_jobs=1,\n",

    "    name=subset,\n",

    ")\n",

    "```"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Run the trained model\n",

    "\n",

    "After the model is trained, a [model archive](../../docs/guides/model_archive.md) file will be generated.\n",

    "This archive file contains the `.tflite` model file which should be programmed to the embedded device.\n",

    "\n",

    "The pre-trained model archive used by this tutorial may be found at: [keyword_spottong_alexa.mltk.zip](https://github.com/siliconlabs/mltk/raw/master/mltk/models/siliconlabs/keyword_spotting_alexa.mltk.zip)\n",

    "\n",

    "To verify that the model works, the [classify_audio](https://siliconlabs.github.io/mltk/docs/audio/audio_utilities.html#audio-classification-utility) MLTK command may be used.\n",

    "This will program the trained model to the [development board](https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit) and stream the dev board's microphone audio into the model. With this, you can issue keywords to the dev board and see the model's classification results in real-time.\n",

    "\n",

    "#### Classify audio command\n",

    "\n",

    "Issue the following command to run the [audio_classifier](../../docs/cpp_development/examples/audio_classifier.md) app with your trained ML model on the development board:\n",

    "\n",

    "```shell\n",

    "mltk classify_audio keyword_spotting_alexa --device --accelerator mvp --verbose\n",

    "```\n",

    "\n",

    "This does the following:\n",

    "- The `--device` option programs the trained `.tflite` to the locally connected [development board](https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit)\n",

    "- The `--accelerator mvp` option tells the classify_audio application to accelerate the ML model with the embedded device's [MVP](https://docs.silabs.com/gecko-platform/latest/machine-learning/tensorflow/mvp-accelerator) hardware accelerator\n",

    "- The `--verbose` option provides more verbose model classification results"

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Run the demo\n",

    "\n",

    "With the model fully trained, we can now run it in the demo application.\n",

    "\n",

    "See the [Quick Start](#quick-start) for more details.\n",

    "\n",

    "When you get to step 4), change the `model` argument to point to your newly trained model, e.g.:\n",

    "\n",

    "\n",

    "```shell\n",

    "mltk program_app mltk_audio_classifier-audio_io-brd2601-mvp --model ~/Desktop/my_model.mltk.zip\n",

    "```\n",

    "\n",

    "__NOTE:__ Internally, this demo uses the [UartStream](../../docs/python_api/utils/uart_stream/index.md) Python utility to communicate with the development board via UART."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "### Build C++ application from source\n",

    "\n",

    "This \"Alexa\" demo is based on the [audio_classifier](../../docs/cpp_development/examples/audio_classifier.md) example application.\n",

    "\n",

    "To build this application from source, execute the following steps:\n",

    "\n",

    "1. Configure the [Visual Studio Code](../../docs/cpp_development/vscode.md) _or_ [CMake Command Line](../../docs/cpp_development/command_line.md) development environment\n",  

    "   __NOTE:__ The application needs to be built for [embedded](https://siliconlabs.github.io/mltk/docs/cpp_development/vscode.html#build-for-embedded)\n",

    "2. Create/modify the file: `<mltk repo root>/user_options.cmake`\n",

    "3. Add the following to `<mltk repo root>/user_options.cmake`\n",

    "   \n",

    "   ```\n",

    "   mltk_set(MLTK_PLATFORM_NAME brd2601)\n",

    "   mltk_set(TFLITE_MICRO_ACCELERATOR mvp)\n",

    "   mltk_set(MLTK_STACK_SIZE 32768) # The Opus audio codec library requires a large stack\n",

    "   mltk_set(AUDIO_CLASSIFIER_ENABLE_AUDIO_IO ON)\n",

    "   mltk_set(AUDIO_CLASSIFIER_MODEL \"<path to your trained Alexa .mltk.zip model archive file>\")\n",

    "   ```\n",

    "4. Invoke the CMake target: `mltk_audio_classifier_download` (see [Visual Studio Code](../../docs/cpp_development/vscode.md) _or_ [CMake Command Line](../../docs/cpp_development/command_line.md) for more details)\n",

    "\n",

    "\n",

    "These steps will:\n",

    "1. Build the [audio_classifier](../../docs/cpp_development/examples/audio_classifier.md) \n",

    "   - For the [BRD2601](../../docs/other/supported_hardware.md#brd2601) platform\n",

    "   - With [MVP](https://docs.silabs.com/gecko-platform/latest/machine-learning/tensorflow/mvp-accelerator) hardware acceleration\n",

    "   - With your trained ML model\n",

    "   - Using the \"Audio I/O\" feature (used to communicate with the demo [Python script](https://raw.githubusercontent.com/SiliconLabs/mltk/master/cpp/shared/apps/audio_classifier/python/alexa_demo/alexa_demo.py))\n",

    "2. Program the built app with ML model to the development board\n",

    "\n",

    "\n",

    "After the app is programmed, run the demo [Python script](https://raw.githubusercontent.com/SiliconLabs/mltk/master/cpp/shared/apps/audio_classifier/python/alexa_demo/alexa_demo.py) which will communicate with the app via UART."

   ]

  },

  {

   "attachments": {},

   "cell_type": "markdown",

   "metadata": {},

   "source": [

    "## Increase the BAUD rate\n",

    "\n",

    "By default, the UART BAUD rate is set to `115200`. This rate is fast enough for simple Alexa responses.\n",

    "However, for longer responses such as music or stories this rate is not fast enough.\n",

    "\n",

    "Execute the following steps to increase the BAUD rate to `460800` which is fast enough for all Alexa responses.\n",

    "\n",

    "1. See [WSTK Virtual COM port baudrate setting](https://community.silabs.com/s/article/wstk-virtual-com-port-baudrate-setting) for how to configure the [development board's](https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit) VCOM port baud rate setting\n",

    "   - In the dev board's \"admin console\", issue the command: `serial vcom config speed 460800`\n",

    "\n",

    "2. Update the model archive with a new parameter: `baud_rate=460800`  using the command (change `keyword_spotting_alexa` to your model's `.mltk.zip` file path):\n",

    "\n",

    "   ```shell\n",

    "   mltk update_params keyword_spotting_alexa baud_rate=460800\n",

    "   ```\n",

    "\n",

    "3. Re-program the app (or rebuild the app from source):\n",

    "\n",

    "   ```shell\n",

    "   mltk program_app mltk_audio_classifier-audio_io-brd2601-mvp --model keyword_spotting_alexa\n",

    "   ```\n",

    "\n",

    "4. Run the demo Python script with the `--baud` argument, e.g.:\n",

    "\n",

    "   ```shell\n",

    "   python alexa_demo.py --baud 460800\n",

    "   ```"

   ]

  }

 ],

 "metadata": {

  "kernelspec": {

   "display_name": ".venv",

   "language": "python",

   "name": "python3"

  },

  "language_info": {

   "codemirror_mode": {

    "name": "ipython",

    "version": 3

   },

   "file_extension": ".py",

   "mimetype": "text/x-python",

   "name": "python",

   "nbconvert_exporter": "python",

   "pygments_lexer": "ipython3",

   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"

  },

  "orig_nbformat": 4,

  "vscode": {

   "interpreter": {

    "hash": "1b794eb47024974fee893fdb7015f3d322c4012087fc39c73069299b7c169399"

   }

  }

 },

 "nbformat": 4,

 "nbformat_minor": 2

}

