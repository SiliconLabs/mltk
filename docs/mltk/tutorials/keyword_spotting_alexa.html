
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="title" content="Machine Learning Toolkit">
<meta name="description" content="A Python package with command-line utilities and scripts to aid the development of machine learning models for Silicon Lab's embedded platforms">
<meta name="keywords" content="machine learning, machine-learning, machinelearning, ml, ai, iot, Internet of things, aiot, tinyml, tensorflow, tensorflow-lite, tensorflow-lite-micro, keras-tensorflow, keras, tflite, embedded, embedded-systems, mcu, Microcontrollers, hardware, python, c++, cmake, keras, numpy, silabs, silicon labs">
<meta name="robots" content="index, follow">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="language" content="English">
<meta name="author" content="Silicon Labs">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../../_static/javascripts/modernizr.js"></script>
  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HZ5MW943WF"></script>
<script>
    window.gTrackingId = 'G-HZ5MW943WF';
</script>
<meta name="google-site-verification" content="dsSsmnE2twOnfSAQk5zBBTrjMArsTJj809Bp-8mVlIw" />
  
  
    <title>Keyword Spotting - Alexa &#8212; MLTK 0.17.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/material.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/design-tabs.js"></script>
    <script src="../../_static/js/custom.js"></script>
    <script src="../../_static/js/apitoc.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Image Classification - Rock, Paper, Scissors" href="image_classification.html" />
    <link rel="prev" title="Keyword Spotting - Pac-Man" href="keyword_spotting_pacman.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=red data-md-color-accent=light-blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#mltk/tutorials/keyword_spotting_alexa" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../../index.html" title="MLTK 0.17.0 documentation"
           class="md-header-nav__button md-logo">
          
              <img src="../../_static/logo.png"
                   alt="MLTK 0.17.0 documentation logo">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">Machine Learning Toolkit</span>
          <span class="md-header-nav__topic"> Keyword Spotting - Alexa </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../../search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            <a href="https://github.com/siliconlabs/mltk" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    MLTK Github Repository
  </div>
</a>
          </div>
        </div>
      
      
    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
            
            <li class="md-tabs__item"><a href="https://docs.silabs.com/gecko-platform/latest/machine-learning/tensorflow/overview" class="md-tabs__link">Gecko SDK Documentation</a></li>
            
            <li class="md-tabs__item"><a href="https://github.com/tensorflow/tflite-micro" class="md-tabs__link">Tensorflow-Lite Micro Repository</a></li>
            
            <li class="md-tabs__item"><a href="https://www.tensorflow.org/learn" class="md-tabs__link">Tensorflow Documentation</a></li>
          <li class="md-tabs__item"><a href="../../docs/tutorials.html" class="md-tabs__link">Tutorials</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../../index.html" title="MLTK 0.17.0 documentation" class="md-nav__button md-logo">
      
        <img src="../../_static/logo.png" alt=" logo" width="48" height="48">
      
    </a>
    <a href="../../index.html"
       title="MLTK 0.17.0 documentation">Machine Learning Toolkit</a>
  </label>
    <div class="md-nav__source">
      <a href="https://github.com/siliconlabs/mltk" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    MLTK Github Repository
  </div>
</a>
    </div>
  
  

  
  <ul class="md-nav__list">
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">Basics</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/overview.html" class="md-nav__link">Overview</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/why_mltk.html" class="md-nav__link">Why MLTK?</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/installation.html" class="md-nav__link">Installation</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/command_line/index.html" class="md-nav__link">Command-Line</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/guides/index.html" class="md-nav__link">Modeling Guides</a>
      
    
    </li>
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">Usage</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/tutorials.html" class="md-nav__link">Tutorials</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="keyword_spotting_on_off.html" class="md-nav__link">Keyword Spotting - On/Off</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="keyword_spotting_pacman.html" class="md-nav__link">Keyword Spotting - Pac-Man</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    <label class="md-nav__link md-nav__link--active" for="__toc"> Keyword Spotting - Alexa </label>
    
      <a href="#" class="md-nav__link md-nav__link--active">Keyword Spotting - Alexa</a>
      
        
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">Contents</label>
  <ul class="md-nav__list" data-md-scrollfix="" id="localtoc">
        <li class="md-nav__item"><a href="#mltk-tutorials-keyword-spotting-alexa--page-root" class="md-nav__link">Keyword Spotting - Alexa</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#demo-video" class="md-nav__link">Demo Video</a>
        </li>
        <li class="md-nav__item"><a href="#quick-links" class="md-nav__link">Quick Links</a>
        </li>
        <li class="md-nav__item"><a href="#quick-start" class="md-nav__link">Quick start</a>
        </li>
        <li class="md-nav__item"><a href="#content" class="md-nav__link">Content</a>
        </li>
        <li class="md-nav__item"><a href="#system-overview" class="md-nav__link">System Overview</a>
        </li>
        <li class="md-nav__item"><a href="#prerequisite-reading" class="md-nav__link">Prerequisite Reading</a>
        </li>
        <li class="md-nav__item"><a href="#required-hardware" class="md-nav__link">Required Hardware</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#efr32xg24-development-kit" class="md-nav__link">EFR32xG24 development kit</a>
        </li>
        <li class="md-nav__item"><a href="#analog-speaker-with-amplifier" class="md-nav__link">Analog speaker with amplifier</a>
        </li>
        <li class="md-nav__item"><a href="#pinout" class="md-nav__link">Pinout</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#develop-the-ml-model" class="md-nav__link">Develop the ML Model</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#create-the-dataset" class="md-nav__link">Create the dataset</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#synthetic-dataset-generation" class="md-nav__link">Synthetic dataset generation</a>
        </li>
        <li class="md-nav__item"><a href="#negative-class" class="md-nav__link">“Negative” class</a>
        </li>
        <li class="md-nav__item"><a href="#class-balance" class="md-nav__link">Class balance</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#select-the-model-architecture" class="md-nav__link">Select the model architecture</a>
        </li>
        <li class="md-nav__item"><a href="#determine-the-audio-frontend-parameters" class="md-nav__link">Determine the audio frontend parameters</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#audio-visualizer-command" class="md-nav__link">Audio visualizer command</a>
        </li>
        <li class="md-nav__item"><a href="#audiofeaturegenerator-settings" class="md-nav__link">AudioFeatureGenerator settings</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#profile-the-model" class="md-nav__link">Profile the model</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#model-profiler-command" class="md-nav__link">Model profiler command</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#data-augmentations" class="md-nav__link">Data augmentations</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#dump-augmentations" class="md-nav__link">Dump augmentations</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#train-the-model" class="md-nav__link">Train the model</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#local-training" class="md-nav__link">Local Training</a>
        </li>
        <li class="md-nav__item"><a href="#remote-training" class="md-nav__link">Remote Training</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#run-the-trained-model" class="md-nav__link">Run the trained model</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#classify-audio-command" class="md-nav__link">Classify audio command</a>
        </li></ul>
            </nav>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#run-the-demo" class="md-nav__link">Run the demo</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#build-c-application-from-source" class="md-nav__link">Build C++ application from source</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#increase-the-baud-rate" class="md-nav__link">Increase the BAUD rate</a>
        </li></ul>
            </nav>
        </li>
      <script type="text/javascript" src=../../_static/js/apitoc.js></script>
  </ul>
</nav>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="image_classification.html" class="md-nav__link">Image Classification - Rock, Paper, Scissors</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="cloud_training_with_vast_ai.html" class="md-nav__link">Cloud Training with vast.ai</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="cloud_logging_with_wandb.html" class="md-nav__link">Cloud Logging with Weights & Biases</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="model_optimization.html" class="md-nav__link">Model Optimization for MVP Hardware Accelerator</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="keyword_spotting_with_transfer_learning.html" class="md-nav__link">Keyword Spotting with Transfer Learning</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="fingerprint_authentication.html" class="md-nav__link">Fingerprint Authentication</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="onnx_to_tflite.html" class="md-nav__link">ONNX to TF-Lite Model Conversion</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="model_debugging.html" class="md-nav__link">Model Debugging</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="add_existing_script_to_mltk.html" class="md-nav__link">Add an Existing Script to the MLTK</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="synthetic_audio_dataset_generation.html" class="md-nav__link">Synthetic Audio Dataset Generation</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/examples.html" class="md-nav__link">API Examples</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/python_api/index.html" class="md-nav__link">API Reference</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/python_api/models/index.html" class="md-nav__link">Reference Models</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/python_api/datasets/index.html" class="md-nav__link">Reference Datasets</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/cpp_development/index.html" class="md-nav__link">C++ Development</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/cpp_development/examples/index.html" class="md-nav__link">C++ Examples</a>
      
    
    </li>
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">Audio Related</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/audio/keyword_spotting_overview.html" class="md-nav__link">Keyword Spotting Overview</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/audio/audio_feature_generator.html" class="md-nav__link">Audio Feature Generator</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/audio/audio_utilities.html" class="md-nav__link">Audio Utilities</a>
      
    
    </li>
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">Other Information</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/faq/index.html" class="md-nav__link">Frequently Asked Questions</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/other/quick_reference.html" class="md-nav__link">Quick Reference</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/other/supported_hardware.html" class="md-nav__link">Supported Hardware</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/guides/notebook_examples_guide.html" class="md-nav__link">Notebook Examples Guide</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/other/settings_file.html" class="md-nav__link">Settings File</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/other/environment_variables.html" class="md-nav__link">Environment Variables</a>
      
    
    </li>
  </ul>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">Contents</label>
  <ul class="md-nav__list" data-md-scrollfix="" id="localtoc">
        <li class="md-nav__item"><a href="#mltk-tutorials-keyword-spotting-alexa--page-root" class="md-nav__link">Keyword Spotting - Alexa</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#demo-video" class="md-nav__link">Demo Video</a>
        </li>
        <li class="md-nav__item"><a href="#quick-links" class="md-nav__link">Quick Links</a>
        </li>
        <li class="md-nav__item"><a href="#quick-start" class="md-nav__link">Quick start</a>
        </li>
        <li class="md-nav__item"><a href="#content" class="md-nav__link">Content</a>
        </li>
        <li class="md-nav__item"><a href="#system-overview" class="md-nav__link">System Overview</a>
        </li>
        <li class="md-nav__item"><a href="#prerequisite-reading" class="md-nav__link">Prerequisite Reading</a>
        </li>
        <li class="md-nav__item"><a href="#required-hardware" class="md-nav__link">Required Hardware</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#efr32xg24-development-kit" class="md-nav__link">EFR32xG24 development kit</a>
        </li>
        <li class="md-nav__item"><a href="#analog-speaker-with-amplifier" class="md-nav__link">Analog speaker with amplifier</a>
        </li>
        <li class="md-nav__item"><a href="#pinout" class="md-nav__link">Pinout</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#develop-the-ml-model" class="md-nav__link">Develop the ML Model</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#create-the-dataset" class="md-nav__link">Create the dataset</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#synthetic-dataset-generation" class="md-nav__link">Synthetic dataset generation</a>
        </li>
        <li class="md-nav__item"><a href="#negative-class" class="md-nav__link">“Negative” class</a>
        </li>
        <li class="md-nav__item"><a href="#class-balance" class="md-nav__link">Class balance</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#select-the-model-architecture" class="md-nav__link">Select the model architecture</a>
        </li>
        <li class="md-nav__item"><a href="#determine-the-audio-frontend-parameters" class="md-nav__link">Determine the audio frontend parameters</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#audio-visualizer-command" class="md-nav__link">Audio visualizer command</a>
        </li>
        <li class="md-nav__item"><a href="#audiofeaturegenerator-settings" class="md-nav__link">AudioFeatureGenerator settings</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#profile-the-model" class="md-nav__link">Profile the model</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#model-profiler-command" class="md-nav__link">Model profiler command</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#data-augmentations" class="md-nav__link">Data augmentations</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#dump-augmentations" class="md-nav__link">Dump augmentations</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#train-the-model" class="md-nav__link">Train the model</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#local-training" class="md-nav__link">Local Training</a>
        </li>
        <li class="md-nav__item"><a href="#remote-training" class="md-nav__link">Remote Training</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#run-the-trained-model" class="md-nav__link">Run the trained model</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#classify-audio-command" class="md-nav__link">Classify audio command</a>
        </li></ul>
            </nav>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#run-the-demo" class="md-nav__link">Run the demo</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#build-c-application-from-source" class="md-nav__link">Build C++ application from source</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#increase-the-baud-rate" class="md-nav__link">Increase the BAUD rate</a>
        </li></ul>
            </nav>
        </li>
      <script type="text/javascript" src=../../_static/js/apitoc.js></script>
  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">

          
          <div class="breadcrumbs md-typeset">
            <ul class="breadcrumb">
              <li></li>
              <li><a href="../../index.html"><i class="md-icon">home</i></a></li>
                <li><a href="../../docs/tutorials.html" accesskey="U">Tutorials</a></li>

              <li class="activate"><a>Keyword Spotting - Alexa</a></li>
            </ul>
          </div>
          

          <article class="md-content__inner md-typeset" role="main">
            
  <section id="keyword-spotting-alexa">
<h1 id="mltk-tutorials-keyword-spotting-alexa--page-root">Keyword Spotting - Alexa<a class="headerlink" href="#mltk-tutorials-keyword-spotting-alexa--page-root" title="Permalink to this heading">¶</a></h1>
<p>This demonstrates how to use an embedded <a class="reference external" href="https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit">development board</a> as the audio source/sink for the <a class="reference external" href="https://developer.amazon.com/en-US/docs/alexa/alexa-voice-service/get-started-with-alexa-voice-service.html">Alexa Voice Services</a> backend.<br/>
With this demo, “Alexa” commands may be issued to the development board’s microphone and the response will be played via the attached <a class="reference external" href="https://www.adafruit.com/product/3885">speaker</a>.</p>
<section id="demo-video">
<h2 id="demo-video">Demo Video<a class="headerlink" href="#demo-video" title="Permalink to this heading">¶</a></h2>
<p>The following is a video of the demo described in this tutorial:</p>
<iframe allow="autoplay; encrypted-media;" allowfullscreen="" frameborder="0" height="500" src="https://www.youtube.com/embed/dLOIZSyYALo" width="281"></iframe></section>
<section id="quick-links">
<h2 id="quick-links">Quick Links<a class="headerlink" href="#quick-links" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/SiliconLabs/mltk/blob/master/mltk/tutorials/keyword_spotting_alexa.ipynb">GitHub Source</a> - View this tutorial on Github</p></li>
<li><p><a class="reference internal" href="../../docs/cpp_development/examples/audio_classifier.html"><span class="doc std std-doc">C++ Example Application</span></a> - View this tutorial’s associated C++ example application</p></li>
<li><p><a class="reference internal" href="../../docs/python_api/models/siliconlabs/keyword_spotting_alexa.html"><span class="doc std std-doc">Machine Learning Model</span></a> - View this tutorial’s associated machine learning model</p></li>
<li><p><a class="reference external" href="https://developer.amazon.com/en-US/docs/alexa/alexa-voice-service/get-started-with-alexa-voice-service.html">Alexa Voice Services Docs</a> - Alexa Voice Services (AVS) documentation</p></li>
<li><p><a class="reference external" href="https://www.adafruit.com/product/3885">Analog Speaker</a> - The recommended analog speaker used by this demo</p></li>
<li><p><a class="reference internal" href="synthetic_audio_dataset_generation.html"><span class="doc std std-doc">Synthetic Keyword Dataset Generation</span></a> - Describes how to generate an “Alexa” dataset using Amazon, Google, and Microsoft Clouds</p></li>
</ul>
</section>
<section id="quick-start">
<h2 id="quick-start">Quick start<a class="headerlink" href="#quick-start" title="Permalink to this heading">¶</a></h2>
<p>To quickly get this demo running, perform the following steps:</p>
<ol class="arabic">
<li><p><a class="reference external" href="../../docs/installation.html#standard-python-package">Install</a> the MLTK Python package</p></li>
<li><p>Obtain a <a class="reference external" href="https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit">BRD2601</a> development kit</p></li>
<li><p>Obtain an analog <a class="reference external" href="https://www.adafruit.com/product/3885">speaker</a> and connect the signal to the pin 9 of the development board (as well as the ground and power signals)</p></li>
<li><p>Install <a class="reference external" href="https://www.segger.com/downloads/jlink">Segger J-Link</a> (if necessary) which provides the drivers necessary for programming the development board</p></li>
<li><p>Run the following MLTK command to program the firmware application and “Alexa” ML model to the development board:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mltk</span> <span class="n">program_app</span> <span class="n">mltk_audio_classifier</span><span class="o">-</span><span class="n">audio_io</span><span class="o">-</span><span class="n">brd2601</span><span class="o">-</span><span class="n">mvp</span> <span class="o">--</span><span class="n">model</span> <span class="n">keyword_spotting_alexa</span>
</pre></div>
</div>
</li>
<li><p>Download the Python script <a class="reference external" href="https://raw.githubusercontent.com/SiliconLabs/mltk/master/cpp/shared/apps/audio_classifier/python/alexa_demo/alexa_demo.py">alexa_demo.py</a> to your local PC</p></li>
<li><p>From the MLTK Python environment, run the script and follow the instructions for obtaining the AVS cloud credentials:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">alexa_demo</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</li>
<li><p>At this point, you should now be able to speak commands into the develop board’s microphone, which will be sent to the AVS cloud, and responses will be returned to the dev board’s connected speaker.<br/>
Try saying some of the commands:</p>
<ul class="simple">
<li><p>Alexa, what time is it?</p></li>
<li><p>Alexa, tell me a joke</p></li>
<li><p>Alexa, what’s 1+1?</p></li>
</ul>
</li>
</ol>
<p><strong>NOTE:</strong> By default, the UART BAUD rate is 115200 which is too slow for larger Alexa responses, see the <a class="reference internal" href="#increase-the-baud-rate"><span class="std std-doc">Increase Baud Rate</span></a> section below for details how to increase the BAUD rate so responses from AVS may be properly played.</p>
</section>
<section id="content">
<h2 id="content">Content<a class="headerlink" href="#content" title="Permalink to this heading">¶</a></h2>
<p>This tutorial is divided into the following sections:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#system-overview"><span class="std std-doc">System Overview</span></a> - Basic overview of the how the system is put together</p></li>
<li><p><a class="reference internal" href="#prerequisite-reading"><span class="std std-doc">Prerequisite Reading</span></a> - Basic information about keyword spotting machine learning</p></li>
<li><p><a class="reference internal" href="#required-hardware"><span class="std std-doc">Required Hardware</span></a> - Details about the hardware needed to run the demo</p></li>
<li><p><a class="reference internal" href="#develop-the-ml-model"><span class="std std-doc">Develop the ML Model</span></a> - Details about how to create a machine learning model to detect the keyword: “Alexa”</p></li>
<li><p><a class="reference internal" href="#run-the-demo"><span class="std std-doc">Running the demo</span></a> - Details about how to run the demo ML model and Python script</p></li>
<li><p><a class="reference internal" href="#build-c-application-from-source"><span class="std std-doc">Building the C++ application from source</span></a> - Details about how to build the firmware application from source</p></li>
</ul>
</section>
<section id="system-overview">
<h2 id="system-overview">System Overview<a class="headerlink" href="#system-overview" title="Permalink to this heading">¶</a></h2>
<p>The basic system overview is as follows:<br/>
<img alt="alexa_demo" src="../../_images/alexa_demo.png"/></p>
<ol class="arabic simple">
<li><p>A <a class="reference external" href="https://github.com/SiliconLabs/mltk/tree/master/cpp/shared/apps/audio_classifier/python/alexa_demo/alexa_demo.py">Python script</a> runs on the local PC and communicates with the development board via UART</p></li>
<li><p>A user says the keyword “Alexa” to the development board’s microphone</p></li>
<li><p>The <a class="reference internal" href="../../docs/python_api/models/siliconlabs/keyword_spotting_alexa.html"><span class="doc std std-doc">machine learning model</span></a> running on the development detects the keyword</p></li>
<li><p>The development board begins compressing the microphone audio using the <a class="reference external" href="https://github.com/SiliconLabs/mltk/tree/master/cpp/shared/opus">Opus codec</a> and streams the audio to the Python script via UART</p></li>
<li><p>The development board uses a <a class="reference external" href="https://github.com/SiliconLabs/mltk/tree/master/cpp/shared/voice_activity_detector">Voice Activity Detection</a> (VAD) library to determine when the user finishes the command</p></li>
<li><p>The Python script sends the Opus-encoded audio to the AVS cloud</p></li>
<li><p>The Python scripts receives the Alexa response from the AVS cloud and forwards to the development board via UART</p></li>
<li><p>The development board decompresses the <a class="reference external" href="https://github.com/SiliconLabs/mltk/tree/master/cpp/shared/minimp3">MP3-encoded</a> audio and streams to the locally connected <a class="reference external" href="https://www.adafruit.com/product/3885">speaker</a> via <a class="reference external" href="https://docs.silabs.com/gecko-platform/latest/emlib/api/efr32xg24/group-vdac">VDAC</a> peripheral</p></li>
</ol>
</section>
<section id="prerequisite-reading">
<h2 id="prerequisite-reading">Prerequisite Reading<a class="headerlink" href="#prerequisite-reading" title="Permalink to this heading">¶</a></h2>
<p>Before continuing with this tutorial, it is recommended to review the following documentation:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../../docs/audio/keyword_spotting_overview.html"><span class="doc std std-doc">Keyword Spotting Overview</span></a> - Provides overview of how embedded keyword spotting works</p></li>
<li><p><a class="reference internal" href="keyword_spotting_on_off.html"><span class="doc std std-doc">Keyword Spotting Tutorial</span></a> - Provides an in-depth tutorial on how to create a keyword spotting model</p></li>
</ul>
</section>
<section id="required-hardware">
<h2 id="required-hardware">Required Hardware<a class="headerlink" href="#required-hardware" title="Permalink to this heading">¶</a></h2>
<p>To run the full demonstration, the following hardware is required:</p>
<section id="efr32xg24-development-kit">
<h3 id="efr32xg24-development-kit">EFR32xG24 development kit<a class="headerlink" href="#efr32xg24-development-kit" title="Permalink to this heading">¶</a></h3>
<p><a class="reference external" href="https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit">EFR32xG24 development kit product page</a></p>
</section>
<section id="analog-speaker-with-amplifier">
<h3 id="analog-speaker-with-amplifier">Analog speaker with amplifier<a class="headerlink" href="#analog-speaker-with-amplifier" title="Permalink to this heading">¶</a></h3>
<p>“Alexa” audio is played via <a class="reference external" href="https://docs.silabs.com/gecko-platform/latest/emlib/api/efr32xg24/group-vdac">VDAC</a> peripheral of the development board.
As such, an analog speaker is required. An amplifier is also necessary.</p>
<p>While not required, this <a class="reference external" href="https://www.adafruit.com/product/3885">speaker</a> provided by Adafruit is recommended.</p>
<p>The speaker’s analog signal should connect to pin 9 of the development board.</p>
</section>
<section id="pinout">
<h3 id="pinout">Pinout<a class="headerlink" href="#pinout" title="Permalink to this heading">¶</a></h3>
<p>The following pin connections are required:</p>
<table>
<thead>
<tr class="row-odd"><th class="head"><p>Pin header Number</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>9</p></td>
<td><p>VDAC - Speaker analog signal</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>GND - Speaker ground</p></td>
</tr>
<tr class="row-even"><td><p>20</p></td>
<td><p>3V3 - Speaker power</p></td>
</tr>
</tbody>
</table>
<p><img alt="alexa_demo_pinout" src="../../_images/alexa_demo_pinout.png"/></p>
</section>
</section>
<section id="develop-the-ml-model">
<h2 id="develop-the-ml-model">Develop the ML Model<a class="headerlink" href="#develop-the-ml-model" title="Permalink to this heading">¶</a></h2>
<p>This demo uses the keyword spotting ML model: <a class="reference internal" href="../../docs/python_api/models/siliconlabs/keyword_spotting_alexa.html"><span class="doc std std-doc">keyword_spotting_alexa</span></a>
to detect the keyword “Alexa”.</p>
<p>This model is based on the <a class="reference external" href="https://arxiv.org/pdf/2010.09960.pdf">Temporal Efficient Neural Network (TENet)</a> model architecture, a keyword spotting architecture with temporal and depthwise convolutions.</p>
<p>The following describes how the model was developed.</p>
<section id="create-the-dataset">
<h3 id="create-the-dataset">Create the dataset<a class="headerlink" href="#create-the-dataset" title="Permalink to this heading">¶</a></h3>
<p>The most important part of developing a machine learning model is acquiring a <strong>representative</strong> dataset.<br/>
A good, <strong>representative</strong> dataset should have numerous (e.g. 10k-1M) “positive” samples (i.e. audio clips of people saying the word “Alexa”)
and numerous “negative” samples (i.e. audio clips of people saying other words besides “Alexa”).
The larger and more diverse the dataset is, the better the model will likely perform in the field.</p>
<section id="synthetic-dataset-generation">
<h4 id="synthetic-dataset-generation">Synthetic dataset generation<a class="headerlink" href="#synthetic-dataset-generation" title="Permalink to this heading">¶</a></h4>
<p>Ideally, the dataset should contain samples of 10k+ people saying the word “Alexa”. However, creating such a dataset can be very expensive and time-consuming.
An alternative approach is to synthetically generate the dataset using cloud-based Text-to-Speech (TTS) services.<br/>
Refer to the <a class="reference internal" href="synthetic_audio_dataset_generation.html"><span class="doc std std-doc">Synthetic Audio Dataset Generation</span></a> tutorial for how an “Alexa” dataset can be generated using the Google, Microsoft, and Amazon Clouds.</p>
</section>
<section id="negative-class">
<h4 id="negative-class">“Negative” class<a class="headerlink" href="#negative-class" title="Permalink to this heading">¶</a></h4>
<p>While having a large and diverse “positive” class (i.e. samples of people saying “Alexa”) is important, it is also important to have a large “negative” class.
This way, the ML model learns not only how to detect the “Alexa” keyword, but also how to reject words and noises that sound similar to the “Alexa” keyword.</p>
<p>Here <a class="reference internal" href="synthetic_audio_dataset_generation.html"><span class="doc std std-doc">synthetic audio dataset generation</span></a> is useful as keywords that sound similar to “alexa” (e.g. alice, alexia, etc.) can be generated.</p>
<p>Other large, publicly available audio datasets can also be used for the “negative” class samples:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://mlcommons.org/en/multilingual-spoken-words">MLCommons Multilingual Spoken Words</a> - A large and growing audio dataset of spoken words in 50 languages for academic research and commercial applications in keyword spotting and spoken term search, licensed under CC-BY 4.0</p></li>
<li><p><a class="reference external" href="https://commonvoice.mozilla.org/en/datasets">Mozilla Common Voice</a> - An open source, multi-language dataset of voices that anyone can use to train speech-enabled applications</p></li>
</ul>
</section>
<section id="class-balance">
<h4 id="class-balance">Class balance<a class="headerlink" href="#class-balance" title="Permalink to this heading">¶</a></h4>
<p>The “positive” and “negative” classes should have approximately the same number of samples.
If one class has substantially more samples then the model may not adequately learn the intricacies of each class.
To help account for this, Tensorflow has the concept of <a class="reference external" href="https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights">class weights</a>. The <a class="reference external" href="../../docs/python_api/mltk_model/index.html">MLTK Model</a> also provides a <a class="reference external" href="../../docs/python_api/mltk_model/dataset_mixin.html#mltk.core.DatasetMixin.class_weights">class_weights</a> property to automatically balance the classes during training.</p>
<p>However, class weights may not work for substantially imbalanced (4x or more) datasets, which is the case for the <a class="reference internal" href="../../docs/python_api/models/siliconlabs/keyword_spotting_alexa.html"><span class="doc std std-doc">Keyword Spotting - Alexa</span></a> model.
In this model’s case, the “negative” class has 10x more samples than the “positive” class.</p>
<p>To help account for this, the training script forces the “negative” class to have 2x the samples of the “positive” class. Then during training, all of the “negative” samples are randomly shuffled and only a subset is used for each epoch.
In this way, the “negative” and “positive” classes are approximately balanced, yet the model “sees” all of the “negative” samples through the course of the full model training process.</p>
</section>
</section>
<section id="select-the-model-architecture">
<h3 id="select-the-model-architecture">Select the model architecture<a class="headerlink" href="#select-the-model-architecture" title="Permalink to this heading">¶</a></h3>
<p>Acquiring a representative dataset is the most important step in ML model development.</p>
<p>Another important step is defining a model architecture. A good model architecture should have the following characteristics:</p>
<ul class="simple">
<li><p><strong>Able to learn the dataset well</strong> - Ensures it will be robust in the field</p></li>
<li><p><strong>Able to fit within the target hardware’s constraints</strong> - The model must be small enough to fit within the RAM/Flash memories</p></li>
<li><p><strong>Able to execute within the application’s time requirements</strong> - The model’s inference time (on the target hardware) must be low so that the application is responsive (i.e. The model should execute quickly so that there is little delay after saying “Alexa”)</p></li>
</ul>
<p>For this application, we choose the <a class="reference external" href="https://arxiv.org/pdf/2010.09960.pdf">Temporal Efficient Neural Network (TENet)</a> model architecture which has been shown to work well with keyword spotting applications,
and, most importantly, is able to efficiently execute on our <a class="reference external" href="https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit">embedded device</a>.</p>
</section>
<section id="determine-the-audio-frontend-parameters">
<h3 id="determine-the-audio-frontend-parameters">Determine the audio frontend parameters<a class="headerlink" href="#determine-the-audio-frontend-parameters" title="Permalink to this heading">¶</a></h3>
<p>For this keyword spotting application, we convert raw audio into a spectrogram (gray-scale 2D image) and feed the spectrogram image into the classifier ML model (<a class="reference external" href="https://arxiv.org/pdf/2010.09960.pdf">TENet</a>).
See the <a class="reference internal" href="../../docs/audio/keyword_spotting_overview.html"><span class="doc std std-doc">Keyword Spotting Overview</span></a> for more details.</p>
<p>We use the <a class="reference internal" href="../../docs/audio/audio_feature_generator.html"><span class="doc std std-doc">Audio Feature Generator</span></a> Python package to generate the spectrogram.</p>
<p>This package has numerous <a class="reference internal" href="../../docs/python_api/data_preprocessing/audio_feature_generator_settings.html"><span class="doc std std-doc">settings</span></a> that determine how the audio is converted to a spectrogram image.
We want to choose these settings so that the generated spectrogram has the best quality while at the same time the ML model executes efficiently on the embedded hardware.</p>
<p>Typically, the larger the spectrogram’s dimensions the better its quality. However, increasing the spectrogram dimensions also increases the input size to the ML model which increases the processing time on the embedded hardware.</p>
<p>The MLTK features two tools to aid with the selection of the audio frontend parameters:</p>
<ul class="simple">
<li><p><a class="reference external" href="../../docs/audio/audio_utilities.html#audio-visualization-utility">Audio Visualizer</a> - Allows for adjusting the AudioFeatureGenerator <a class="reference external" href="../../docs/python_api/data_preprocessing/audio_feature_generator_settings.html">settings</a> and viewing the resulting spectrogram in real-time</p></li>
<li><p><a class="reference internal" href="../../docs/guides/model_profiler.html"><span class="doc std std-doc">Model Profiler</span></a> - Allows for running and profiling the ML model on the embedded device <em>before</em> fully training the ML model</p></li>
</ul>
<section id="audio-visualizer-command">
<h4 id="audio-visualizer-command">Audio visualizer command<a class="headerlink" href="#audio-visualizer-command" title="Permalink to this heading">¶</a></h4>
<p>To run the Audio Visualizer, issue the command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mltk<span class="w"> </span>view_audio
</pre></div>
</div>
</section>
<section id="audiofeaturegenerator-settings">
<h4 id="audiofeaturegenerator-settings">AudioFeatureGenerator settings<a class="headerlink" href="#audiofeaturegenerator-settings" title="Permalink to this heading">¶</a></h4>
<p>For the <a class="reference internal" href="../../docs/python_api/models/siliconlabs/keyword_spotting_alexa.html"><span class="doc std std-doc">keyword_spotting_alexa</span></a>  model, we use the following settings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">frontend_settings</span> <span class="o">=</span> <span class="n">AudioFeatureGeneratorSettings</span><span class="p">()</span>

<span class="n">frontend_settings</span><span class="o">.</span><span class="n">sample_rate_hz</span> <span class="o">=</span> <span class="mi">16000</span>
<span class="n">frontend_settings</span><span class="o">.</span><span class="n">sample_length_ms</span> <span class="o">=</span> <span class="mi">1200</span>                       <span class="c1"># Use 1.2s audio clips to ensure the full "alexa" keyword is captured</span>
<span class="n">frontend_settings</span><span class="o">.</span><span class="n">window_size_ms</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">frontend_settings</span><span class="o">.</span><span class="n">window_step_ms</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">frontend_settings</span><span class="o">.</span><span class="n">filterbank_n_channels</span> <span class="o">=</span> <span class="mi">108</span>                   <span class="c1"># We want this value to be as large as possible</span>
                                                                <span class="c1"># while still allowing for the ML model to execute efficiently on the hardware</span>
<span class="n">frontend_settings</span><span class="o">.</span><span class="n">filterbank_upper_band_limit</span> <span class="o">=</span> <span class="mf">7500.0</span>
<span class="n">frontend_settings</span><span class="o">.</span><span class="n">filterbank_lower_band_limit</span> <span class="o">=</span> <span class="mf">125.0</span>           <span class="c1"># The dev board mic seems to have a lot of noise at lower frequencies</span>

<span class="n">frontend_settings</span><span class="o">.</span><span class="n">noise_reduction_enable</span> <span class="o">=</span> <span class="kc">True</span>                 <span class="c1"># Enable the noise reduction block to help ignore background noise in the field</span>
<span class="n">frontend_settings</span><span class="o">.</span><span class="n">noise_reduction_smoothing_bits</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">frontend_settings</span><span class="o">.</span><span class="n">noise_reduction_even_smoothing</span> <span class="o">=</span>  <span class="mf">0.025</span>
<span class="n">frontend_settings</span><span class="o">.</span><span class="n">noise_reduction_odd_smoothing</span> <span class="o">=</span> <span class="mf">0.06</span>
<span class="n">frontend_settings</span><span class="o">.</span><span class="n">noise_reduction_min_signal_remaining</span> <span class="o">=</span> <span class="mf">0.40</span>   <span class="c1"># This value is fairly large (which makes the background noise reduction small)</span>
                                                                <span class="c1"># But it has been found to still give good results</span>
                                                                <span class="c1"># i.e. There is still some background noise reduction,</span>
                                                                <span class="c1"># but the actual signal is still (mostly) untouched</span>

<span class="n">frontend_settings</span><span class="o">.</span><span class="n">dc_notch_filter_enable</span> <span class="o">=</span> <span class="kc">True</span>                 <span class="c1"># Enable the DC notch filter, to help remove the DC signal from the dev board's mic</span>
<span class="n">frontend_settings</span><span class="o">.</span><span class="n">dc_notch_filter_coefficient</span> <span class="o">=</span> <span class="mf">0.95</span>

<span class="n">frontend_settings</span><span class="o">.</span><span class="n">quantize_dynamic_scale_enable</span> <span class="o">=</span> <span class="kc">True</span>          <span class="c1"># Enable dynamic quantization, this dynamically converts the uint16 spectrogram to int8</span>
<span class="n">frontend_settings</span><span class="o">.</span><span class="n">quantize_dynamic_scale_range_db</span> <span class="o">=</span> <span class="mf">40.0</span>


<span class="c1"># Add the Audio Feature generator settings to the model parameters</span>
<span class="c1"># This way, they are included in the generated .tflite model file</span>
<span class="c1"># See https://siliconlabs.github.io/mltk/docs/guides/model_parameters.html</span>
<span class="n">my_model</span><span class="o">.</span><span class="n">model_parameters</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">frontend_settings</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="profile-the-model">
<h3 id="profile-the-model">Profile the model<a class="headerlink" href="#profile-the-model" title="Permalink to this heading">¶</a></h3>
<p>Before spending the time and money to fully train the ML model, it is critical that we profile the model on the embedded device to ensure it is able to efficiently execute.</p>
<p>This can be done using the <a class="reference internal" href="../../docs/guides/model_profiler.html"><span class="doc std std-doc">Model Profiler</span></a>.</p>
<section id="model-profiler-command">
<h4 id="model-profiler-command">Model profiler command<a class="headerlink" href="#model-profiler-command" title="Permalink to this heading">¶</a></h4>
<p>Plug the <a class="reference external" href="https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit">development board</a> in your PC and issue the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mltk<span class="w"> </span>profile<span class="w"> </span>keyword_spotting_alexa<span class="w"> </span>--build<span class="w"> </span>--device<span class="w"> </span>--accelerator<span class="w"> </span>mvp
</pre></div>
</div>
<p>This does the following:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">--build</span></code> option generates a <code class="docutils literal notranslate"><span class="pre">.tflite</span></code> model file using dummy weights (i.e. it generates a non-fully trained ML model)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">--device</span></code> option programs the generated <code class="docutils literal notranslate"><span class="pre">.tflite</span></code> to the locally connected <a class="reference external" href="https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit">development board</a></p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">--accelerator</span> <span class="pre">mvp</span></code> option tells the model profiler application to accelerate the ML model with the embedded device’s <a class="reference external" href="https://docs.silabs.com/gecko-platform/latest/machine-learning/tensorflow/mvp-accelerator">MVP</a> hardware accelerator</p></li>
</ul>
<p>The pre-generated results of this profiling command may found <a class="reference external" href="../../docs/python_api/models/siliconlabs/keyword_spotting_alexa.html#model-profiling-report">here</a>.</p>
</section>
</section>
<section id="data-augmentations">
<h3 id="data-augmentations">Data augmentations<a class="headerlink" href="#data-augmentations" title="Permalink to this heading">¶</a></h3>
<p>Typically, the larger the dataset the more robust the ML model will be. Another way of increasing the size of the dataset without acquiring more samples is to dynamically augment the audio samples during training.</p>
<p>The <a class="reference external" href="https://github.com/siliconlabs/mltk/blob/master/mltk/models/siliconlabs/keyword_spotting_alexa.py">keyword_spotting_alexa.py</a> ML model augmentations are implemented in the <code class="docutils literal notranslate"><span class="pre">audio_pipeline_with_augmentations()</span></code> function. This function does the following:</p>
<ul class="simple">
<li><p>Uses the <a class="reference external" href="https://github.com/iver56/audiomentations">audiomentations</a> Python library to apply the various augmentations</p></li>
<li><p>Pads the samples with 1s of silence, this helps the AudioFeatureGenerator’s noise reduction block to “warm up” when generating the spectrogram</p></li>
<li><p>Adds background audio recorded from the <a class="reference external" href="https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit">development board’s</a> microphone - This helps to make the dataset samples sound as if they were recorded by the dev board’s mic</p></li>
<li><p>Randomly adds other background noises such as conferences, offices, restaurants, etc.</p></li>
<li><p>Randomly increases/decreases the sample volume</p></li>
<li><p>Randomly crops “known” samples and uses the crop sample as an “unknown” sample. This should help the model to not trigger on partially buffered keywords, the model should only trigger when the keyword has been fully buffered</p></li>
</ul>
<section id="dump-augmentations">
<h4 id="dump-augmentations">Dump augmentations<a class="headerlink" href="#dump-augmentations" title="Permalink to this heading">¶</a></h4>
<p>In the <a class="reference external" href="https://github.com/siliconlabs/mltk/blob/master/mltk/models/siliconlabs/keyword_spotting_alexa.py">keyword_spotting_alexa.py</a> model specification file, if you uncomment the line (near the middle of the script):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Uncomment this to dump the augmented audio samples to the log directory</span>
<span class="c1"># DO NOT forget to disable this before training the model as it will generate A LOT of data</span>
<span class="c1">#data_dump_dir = my_model.create_log_dir('dataset_dump')</span>
</pre></div>
</div>
<p>Then, during training, the dynamically augmented audio samples and corresponding spectrograms will be dumped to the specified directory.<br/>
This way, you can listen to the augmented audio samples and view their corresponding spectrogram images.</p>
<p>The dumped audio files and spectrogram images will appear in the dump directory similar to:<br/>
<img alt="audio_augmentations_dump" src="../../_images/audio_augmentations_dump.png"/></p>
<p><strong>HINT:</strong> See the <a class="reference internal" href="model_debugging.html"><span class="doc std std-doc">Model Debugging</span></a> tutorial for how easily debug the model Python script and view the dumped samples.</p>
</section>
</section>
<section id="train-the-model">
<h3 id="train-the-model">Train the model<a class="headerlink" href="#train-the-model" title="Permalink to this heading">¶</a></h3>
<p>Once the parameters are configured and it is determined that the profiled model runs efficiently on the embedded hardware, it is time to fully train the model.
The model can either be trained locally or on a remote cloud machine.</p>
<section id="local-training">
<h4 id="local-training">Local Training<a class="headerlink" href="#local-training" title="Permalink to this heading">¶</a></h4>
<p>See the <a class="reference internal" href="../../docs/guides/model_training.html"><span class="doc std std-doc">Local Model Training</span></a> guide for details on how to train the model on your local machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mltk<span class="w"> </span>train<span class="w"> </span>keyword_spotting_alexa
</pre></div>
</div>
</section>
<section id="remote-training">
<h4 id="remote-training">Remote Training<a class="headerlink" href="#remote-training" title="Permalink to this heading">¶</a></h4>
<p>See the <a class="reference internal" href="../../docs/guides/model_training_via_ssh.html"><span class="doc std std-doc">Remote Model Training</span></a> guide for details on how to train the model on a remote cloud machine.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mltk<span class="w"> </span>ssh<span class="w"> </span>train<span class="w"> </span>keyword_spotting_alexa
</pre></div>
</div>
<p><strong>HINT:</strong> This model can be trained in ~2hrs using <a class="reference internal" href="cloud_training_with_vast_ai.html"><span class="doc std std-doc">vast.ai</span></a>.<br/>
Be sure to select an instance with at least 48 CPUs (only 1 GPU is needed). Also be sure to update the <code class="docutils literal notranslate"><span class="pre">n_jobs</span></code> in <a class="reference external" href="../../docs/python_api/data_preprocessing/tf_dataset.html#mltk.core.preprocess.utils.tf_dataset.parallel_process">tf_dataset_utils.parallel_process()</a> in <a class="reference external" href="https://github.com/siliconlabs/mltk/blob/master/mltk/models/siliconlabs/keyword_spotting_alexa.py">keyword_spotting_alexa.py</a>, e.g.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">features_ds</span><span class="p">,</span> <span class="n">pool</span> <span class="o">=</span> <span class="n">tf_dataset_utils</span><span class="o">.</span><span class="n">parallel_process</span><span class="p">(</span>
    <span class="n">features_ds</span><span class="p">,</span>
    <span class="n">audio_pipeline_with_augmentations</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
    <span class="c1">#n_jobs=84 if subset == 'training' else 32, # These are the settings for a 256 CPU core cloud machine</span>
    <span class="n">n_jobs</span><span class="o">=</span><span class="mi">72</span> <span class="k">if</span> <span class="n">subset</span> <span class="o">==</span> <span class="s1">'training'</span> <span class="k">else</span> <span class="mi">32</span><span class="p">,</span> <span class="c1"># These are the settings for a 128 CPU core cloud machine</span>
    <span class="c1">#n_jobs=44 if subset == 'training' else 16, # These are the settings for a 96 CPU core cloud machine</span>
    <span class="c1">#n_jobs=50 if subset == 'training' else 25, # These are the settings for a 84 CPU core cloud machine</span>
    <span class="c1">#n_jobs=36 if subset == 'training' else 12, # These are the settings for a 64 CPU core cloud machine</span>
    <span class="c1">#n_jobs=28 if subset == 'training' else 16, # These are the settings for a 48 CPU core cloud machine</span>
    <span class="c1">#n_jobs=.65 if subset == 'training' else .35,</span>
    <span class="c1">#n_jobs=1,</span>
    <span class="n">name</span><span class="o">=</span><span class="n">subset</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="run-the-trained-model">
<h3 id="run-the-trained-model">Run the trained model<a class="headerlink" href="#run-the-trained-model" title="Permalink to this heading">¶</a></h3>
<p>After the model is trained, a <a class="reference internal" href="../../docs/guides/model_archive.html"><span class="doc std std-doc">model archive</span></a> file will be generated.
This archive file contains the <code class="docutils literal notranslate"><span class="pre">.tflite</span></code> model file which should be programmed to the embedded device.</p>
<p>The pre-trained model archive used by this tutorial may be found at: <a class="reference external" href="https://github.com/siliconlabs/mltk/raw/master/mltk/models/siliconlabs/keyword_spotting_alexa.mltk.zip">keyword_spottong_alexa.mltk.zip</a></p>
<p>To verify that the model works, the <a class="reference external" href="../../docs/audio/audio_utilities.html#audio-classification-utility">classify_audio</a> MLTK command may be used.
This will program the trained model to the <a class="reference external" href="https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit">development board</a> and stream the dev board’s microphone audio into the model. With this, you can issue keywords to the dev board and see the model’s classification results in real-time.</p>
<section id="classify-audio-command">
<h4 id="classify-audio-command">Classify audio command<a class="headerlink" href="#classify-audio-command" title="Permalink to this heading">¶</a></h4>
<p>Issue the following command to run the <a class="reference internal" href="../../docs/cpp_development/examples/audio_classifier.html"><span class="doc std std-doc">audio_classifier</span></a> app with your trained ML model on the development board:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mltk<span class="w"> </span>classify_audio<span class="w"> </span>keyword_spotting_alexa<span class="w"> </span>--device<span class="w"> </span>--accelerator<span class="w"> </span>mvp<span class="w"> </span>--verbose
</pre></div>
</div>
<p>This does the following:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">--device</span></code> option programs the trained <code class="docutils literal notranslate"><span class="pre">.tflite</span></code> to the locally connected <a class="reference external" href="https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit">development board</a></p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">--accelerator</span> <span class="pre">mvp</span></code> option tells the classify_audio application to accelerate the ML model with the embedded device’s <a class="reference external" href="https://docs.silabs.com/gecko-platform/latest/machine-learning/tensorflow/mvp-accelerator">MVP</a> hardware accelerator</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">--verbose</span></code> option provides more verbose model classification results</p></li>
</ul>
</section>
</section>
</section>
<section id="run-the-demo">
<h2 id="run-the-demo">Run the demo<a class="headerlink" href="#run-the-demo" title="Permalink to this heading">¶</a></h2>
<p>With the model fully trained, we can now run it in the demo application.</p>
<p>See the <a class="reference internal" href="#quick-start"><span class="std std-doc">Quick Start</span></a> for more details.</p>
<p>When you get to step 4), change the <code class="docutils literal notranslate"><span class="pre">model</span></code> argument to point to your newly trained model, e.g.:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mltk<span class="w"> </span>program_app<span class="w"> </span>mltk_audio_classifier-audio_io-brd2601-mvp<span class="w"> </span>--model<span class="w"> </span>~/Desktop/my_model.mltk.zip
</pre></div>
</div>
<p><strong>NOTE:</strong> Internally, this demo uses the <a class="reference internal" href="../../docs/python_api/utils/uart_stream/index.html"><span class="doc std std-doc">UartStream</span></a> Python utility to communicate with the development board via UART.</p>
<section id="build-c-application-from-source">
<h3 id="build-c-application-from-source">Build C++ application from source<a class="headerlink" href="#build-c-application-from-source" title="Permalink to this heading">¶</a></h3>
<p>This “Alexa” demo is based on the <a class="reference internal" href="../../docs/cpp_development/examples/audio_classifier.html"><span class="doc std std-doc">audio_classifier</span></a> example application.</p>
<p>To build this application from source, execute the following steps:</p>
<ol class="arabic">
<li><p>Configure the <a class="reference internal" href="../../docs/cpp_development/vscode.html"><span class="doc std std-doc">Visual Studio Code</span></a> <em>or</em> <a class="reference internal" href="../../docs/cpp_development/command_line.html"><span class="doc std std-doc">CMake Command Line</span></a> development environment
<strong>NOTE:</strong> The application needs to be built for <a class="reference external" href="../../docs/cpp_development/vscode.html#build-for-embedded">embedded</a></p></li>
<li><p>Create/modify the file: <code class="docutils literal notranslate"><span class="pre">&lt;mltk</span> <span class="pre">repo</span> <span class="pre">root&gt;/user_options.cmake</span></code></p></li>
<li><p>Add the following to <code class="docutils literal notranslate"><span class="pre">&lt;mltk</span> <span class="pre">repo</span> <span class="pre">root&gt;/user_options.cmake</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mltk_set</span><span class="p">(</span><span class="n">MLTK_PLATFORM_NAME</span> <span class="n">brd2601</span><span class="p">)</span>
<span class="n">mltk_set</span><span class="p">(</span><span class="n">TFLITE_MICRO_ACCELERATOR</span> <span class="n">mvp</span><span class="p">)</span>
<span class="n">mltk_set</span><span class="p">(</span><span class="n">MLTK_STACK_SIZE</span> <span class="mi">32768</span><span class="p">)</span> <span class="c1"># The Opus audio codec library requires a large stack</span>
<span class="n">mltk_set</span><span class="p">(</span><span class="n">AUDIO_CLASSIFIER_ENABLE_AUDIO_IO</span> <span class="n">ON</span><span class="p">)</span>
<span class="n">mltk_set</span><span class="p">(</span><span class="n">AUDIO_CLASSIFIER_MODEL</span> <span class="s2">"&lt;path to your trained Alexa .mltk.zip model archive file&gt;"</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Invoke the CMake target: <code class="docutils literal notranslate"><span class="pre">mltk_audio_classifier_download</span></code> (see <a class="reference internal" href="../../docs/cpp_development/vscode.html"><span class="doc std std-doc">Visual Studio Code</span></a> <em>or</em> <a class="reference internal" href="../../docs/cpp_development/command_line.html"><span class="doc std std-doc">CMake Command Line</span></a> for more details)</p></li>
</ol>
<p>These steps will:</p>
<ol class="arabic simple">
<li><p>Build the <a class="reference internal" href="../../docs/cpp_development/examples/audio_classifier.html"><span class="doc std std-doc">audio_classifier</span></a></p>
<ul class="simple">
<li><p>For the <a class="reference internal" href="../../docs/other/supported_hardware.html#brd2601"><span class="std std-doc">BRD2601</span></a> platform</p></li>
<li><p>With <a class="reference external" href="https://docs.silabs.com/gecko-platform/latest/machine-learning/tensorflow/mvp-accelerator">MVP</a> hardware acceleration</p></li>
<li><p>With your trained ML model</p></li>
<li><p>Using the “Audio I/O” feature (used to communicate with the demo <a class="reference external" href="https://raw.githubusercontent.com/SiliconLabs/mltk/master/cpp/shared/apps/audio_classifier/python/alexa_demo/alexa_demo.py">Python script</a>)</p></li>
</ul>
</li>
<li><p>Program the built app with ML model to the development board</p></li>
</ol>
<p>After the app is programmed, run the demo <a class="reference external" href="https://raw.githubusercontent.com/SiliconLabs/mltk/master/cpp/shared/apps/audio_classifier/python/alexa_demo/alexa_demo.py">Python script</a> which will communicate with the app via UART.</p>
</section>
</section>
<section id="increase-the-baud-rate">
<h2 id="increase-the-baud-rate">Increase the BAUD rate<a class="headerlink" href="#increase-the-baud-rate" title="Permalink to this heading">¶</a></h2>
<p>By default, the UART BAUD rate is set to <code class="docutils literal notranslate"><span class="pre">115200</span></code>. This rate is fast enough for simple Alexa responses.
However, for longer responses such as music or stories this rate is not fast enough.</p>
<p>Execute the following steps to increase the BAUD rate to <code class="docutils literal notranslate"><span class="pre">460800</span></code> which is fast enough for all Alexa responses.</p>
<ol class="arabic">
<li><p>See <a class="reference external" href="https://community.silabs.com/s/article/wstk-virtual-com-port-baudrate-setting">WSTK Virtual COM port baudrate setting</a> for how to configure the <a class="reference external" href="https://www.silabs.com/development-tools/wireless/efr32xg24-dev-kit">development board’s</a> VCOM port baud rate setting.<br/>
In the dev board’s “admin console”, issue the command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>serial<span class="w"> </span>vcom<span class="w"> </span>config<span class="w"> </span>speed<span class="w"> </span><span class="m">460800</span>
</pre></div>
</div>
</li>
<li><p>Update the model archive with a new parameter: <code class="docutils literal notranslate"><span class="pre">baud_rate=460800</span></code>  using the command (change <code class="docutils literal notranslate"><span class="pre">keyword_spotting_alexa</span></code> to your model’s <code class="docutils literal notranslate"><span class="pre">.mltk.zip</span></code> file path):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mltk<span class="w"> </span>update_params<span class="w"> </span>keyword_spotting_alexa<span class="w"> </span><span class="nv">baud_rate</span><span class="o">=</span><span class="m">460800</span>
</pre></div>
</div>
</li>
<li><p>Re-program the app (or rebuild the app from source):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mltk<span class="w"> </span>program_app<span class="w"> </span>mltk_audio_classifier-audio_io-brd2601-mvp<span class="w"> </span>--model<span class="w"> </span>keyword_spotting_alexa
</pre></div>
</div>
</li>
<li><p>Run the demo Python script with the <code class="docutils literal notranslate"><span class="pre">--baud</span></code> argument, e.g.:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>alexa_demo.py<span class="w"> </span>--baud<span class="w"> </span><span class="m">460800</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>


          </article>
        </div>
      </div>
      <a href="#" class="go-top"><i class="md-icon">arrow_upward</i>Back to Top</a>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
            <a href="keyword_spotting_pacman.html" title="Keyword Spotting - Pac-Man"
               class="md-flex md-footer-nav__link md-footer-nav__link--prev"
               rel="prev">
              <div class="md-flex__cell md-flex__cell--shrink">
                <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
              </div>
              <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
                <span class="md-flex__ellipsis">
                  <span
                      class="md-footer-nav__direction"> Previous </span> Keyword Spotting - Pac-Man </span>
              </div>
            </a>
          
          
            <a href="image_classification.html" title="Image Classification - Rock, Paper, Scissors"
               class="md-flex md-footer-nav__link md-footer-nav__link--next"
               rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span
                class="md-flex__ellipsis"> <span
                class="md-footer-nav__direction"> Next </span> Image Classification - Rock, Paper, Scissors </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink"><i
                class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2023, Silicon Labs.
              
          </div>
            Last updated on
              Jun 19, 2023.
            <br/>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
        <button id="survey-link" class="feedback-button">Feedback</button>
      </div>
    </div>
  </footer>
  <div class="privacy-banner">
    <div class="privacy-banner-wrapper">
      <p>
        <b>Important:</b> We use cookies only for functional and traffic analytics. <br />
        We DO NOT use cookies for any marketing purposes. By using our site you acknowledge you have read and understood our <a class="privacy-policy" href="https://www.silabs.com/about-us/legal/cookie-policy" target="_blank">Cookie Policy</a>.
      </p>
      <a class="privacy-banner-accept" href="#">Got it</a>
    </div>
</div>
  
<div class="survey-container" id="dlg-survey"> 
    <div class="close" id="dlg-survey-close"><i class="md-icon">close</i></div>
    <div class="msg">Please click the <b>submit</b> button at the end even if you do not answer all of the questions</div>
    <iframe id="iframe-survey" style="width: 100%; height: 100%;"></iframe>
</div>
  
  <script src="../../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>