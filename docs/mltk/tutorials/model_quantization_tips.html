
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="title" content="Machine Learning Toolkit">
<meta name="description" content="A Python package with command-line utilities and scripts to aid the development of machine learning models for Silicon Lab's embedded platforms">
<meta name="keywords" content="machine learning, machine-learning, machinelearning, ml, ai, iot, Internet of things, aiot, tinyml, tensorflow, tensorflow-lite, tensorflow-lite-micro, keras-tensorflow, keras, tflite, embedded, embedded-systems, mcu, Microcontrollers, hardware, python, c++, cmake, keras, numpy, silabs, silicon labs">
<meta name="robots" content="index, follow">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="language" content="English">
<meta name="author" content="Silicon Labs">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../../_static/javascripts/modernizr.js"></script>
  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HZ5MW943WF"></script>
<script>
    window.gTrackingId = 'G-HZ5MW943WF';
</script>
<meta name="google-site-verification" content="dsSsmnE2twOnfSAQk5zBBTrjMArsTJj809Bp-8mVlIw" />
  
  
    <title>Model Quantization Tips &#8212; MLTK 0.20.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/material.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/design-tabs.js"></script>
    <script src="../../_static/js/custom.js"></script>
    <script src="../../_static/js/apitoc.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Quantized LSTM" href="quantized_lstm.html" />
    <link rel="prev" title="Synthetic Audio Dataset Generation" href="synthetic_audio_dataset_generation.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=red data-md-color-accent=light-blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#mltk/tutorials/model_quantization_tips" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../../index.html" title="MLTK 0.20.0 documentation"
           class="md-header-nav__button md-logo">
          
              <img src="../../_static/logo.png"
                   alt="MLTK 0.20.0 documentation logo">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">Machine Learning Toolkit</span>
          <span class="md-header-nav__topic"> Model Quantization Tips </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../../search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            <a href="https://github.com/siliconlabs/mltk" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    MLTK Github Repository
  </div>
</a>
          </div>
        </div>
      
      
    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
            
            <li class="md-tabs__item"><a href="https://docs.silabs.com/gecko-platform/latest/machine-learning/tensorflow/overview" class="md-tabs__link">Gecko SDK Documentation</a></li>
            
            <li class="md-tabs__item"><a href="https://github.com/tensorflow/tflite-micro" class="md-tabs__link">Tensorflow-Lite Micro Repository</a></li>
            
            <li class="md-tabs__item"><a href="https://www.tensorflow.org/learn" class="md-tabs__link">Tensorflow Documentation</a></li>
          <li class="md-tabs__item"><a href="../../docs/tutorials.html" class="md-tabs__link">Tutorials</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../../index.html" title="MLTK 0.20.0 documentation" class="md-nav__button md-logo">
      
        <img src="../../_static/logo.png" alt=" logo" width="48" height="48">
      
    </a>
    <a href="../../index.html"
       title="MLTK 0.20.0 documentation">Machine Learning Toolkit</a>
  </label>
    <div class="md-nav__source">
      <a href="https://github.com/siliconlabs/mltk" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    MLTK Github Repository
  </div>
</a>
    </div>
  
  

  
  <ul class="md-nav__list">
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">Basics</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/overview.html" class="md-nav__link">Overview</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/why_mltk.html" class="md-nav__link">Why MLTK?</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/installation.html" class="md-nav__link">Installation</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/command_line/index.html" class="md-nav__link">Command-Line</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/guides/index.html" class="md-nav__link">Modeling Guides</a>
      
    
    </li>
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">Usage</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/tutorials.html" class="md-nav__link">Tutorials</a>
      <ul class="md-nav__list"> 
    <li class="md-nav__item">
    
    
      <a href="keyword_spotting_on_off.html" class="md-nav__link">Keyword Spotting - On/Off</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="keyword_spotting_pacman.html" class="md-nav__link">Keyword Spotting - Pac-Man</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="keyword_spotting_alexa.html" class="md-nav__link">Keyword Spotting - Alexa</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="image_classification.html" class="md-nav__link">Image Classification - Rock, Paper, Scissors</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="cloud_training_with_vast_ai.html" class="md-nav__link">Cloud Training with vast.ai</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="cloud_logging_with_wandb.html" class="md-nav__link">Cloud Logging with Weights & Biases</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="model_optimization.html" class="md-nav__link">Model Optimization for MVP Hardware Accelerator</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="keyword_spotting_with_transfer_learning.html" class="md-nav__link">Keyword Spotting with Transfer Learning</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="fingerprint_authentication.html" class="md-nav__link">Fingerprint Authentication</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="onnx_to_tflite.html" class="md-nav__link">ONNX to TF-Lite Model Conversion</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="model_debugging.html" class="md-nav__link">Model Debugging</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="add_existing_script_to_mltk.html" class="md-nav__link">Add an Existing Script to the MLTK</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="synthetic_audio_dataset_generation.html" class="md-nav__link">Synthetic Audio Dataset Generation</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    <label class="md-nav__link md-nav__link--active" for="__toc"> Model Quantization Tips </label>
    
      <a href="#" class="md-nav__link md-nav__link--active">Model Quantization Tips</a>
      
        
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">Contents</label>
  <ul class="md-nav__list" data-md-scrollfix="" id="localtoc">
        <li class="md-nav__item"><a href="#mltk-tutorials-model-quantization-tips--page-root" class="md-nav__link">Model Quantization Tips</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#contents" class="md-nav__link">Contents</a>
        </li>
        <li class="md-nav__item"><a href="#quantization-report" class="md-nav__link">Quantization Report</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#enabling-the-quantization-report" class="md-nav__link">Enabling the Quantization Report</a>
        </li>
        <li class="md-nav__item"><a href="#analyzing-the-report" class="md-nav__link">Analyzing the Report</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#input-data-normalization" class="md-nav__link">Input Data Normalization</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#scale-by-a-constant" class="md-nav__link">Scale by a constant</a>
        </li>
        <li class="md-nav__item"><a href="#center-about-mean-and-scale-by-std" class="md-nav__link">Center about mean and scale by STD</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#normalization-layers" class="md-nav__link">Normalization Layers</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#batch-normalization" class="md-nav__link">Batch Normalization</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#conv2d-batch-normalization" class="md-nav__link">Conv2D + Batch Normalization</a>
        </li>
        <li class="md-nav__item"><a href="#fully-connected-batch-normalization" class="md-nav__link">Fully Connected + Batch Normalization</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#layernormalization" class="md-nav__link">LayerNormalization</a>
        </li></ul>
            </nav>
        </li></ul>
            </nav>
        </li>
      <script type="text/javascript" src=../../_static/js/apitoc.js></script>
  </ul>
</nav>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="quantized_lstm.html" class="md-nav__link">Quantized LSTM</a>
      
    
    </li></ul>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/examples.html" class="md-nav__link">API Examples</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/python_api/index.html" class="md-nav__link">API Reference</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/python_api/models/index.html" class="md-nav__link">Reference Models</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/python_api/datasets/index.html" class="md-nav__link">Reference Datasets</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/cpp_development/index.html" class="md-nav__link">C++ Development</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/cpp_development/examples/index.html" class="md-nav__link">C++ Examples</a>
      
    
    </li>
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">Audio Related</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/audio/keyword_spotting_overview.html" class="md-nav__link">Keyword Spotting Overview</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/audio/audio_feature_generator.html" class="md-nav__link">Audio Feature Generator</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/audio/audio_utilities.html" class="md-nav__link">Audio Utilities</a>
      
    
    </li>
    <li class="md-nav__item">
    
      <span class="md-nav__link caption"><span class="caption-text">Other Information</span></span>
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/faq/index.html" class="md-nav__link">Frequently Asked Questions</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/other/quick_reference.html" class="md-nav__link">Quick Reference</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/other/supported_hardware.html" class="md-nav__link">Supported Hardware</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/guides/notebook_examples_guide.html" class="md-nav__link">Notebook Examples Guide</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/other/settings_file.html" class="md-nav__link">Settings File</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../docs/other/environment_variables.html" class="md-nav__link">Environment Variables</a>
      
    
    </li>
  </ul>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">Contents</label>
  <ul class="md-nav__list" data-md-scrollfix="" id="localtoc">
        <li class="md-nav__item"><a href="#mltk-tutorials-model-quantization-tips--page-root" class="md-nav__link">Model Quantization Tips</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#contents" class="md-nav__link">Contents</a>
        </li>
        <li class="md-nav__item"><a href="#quantization-report" class="md-nav__link">Quantization Report</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#enabling-the-quantization-report" class="md-nav__link">Enabling the Quantization Report</a>
        </li>
        <li class="md-nav__item"><a href="#analyzing-the-report" class="md-nav__link">Analyzing the Report</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#input-data-normalization" class="md-nav__link">Input Data Normalization</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#scale-by-a-constant" class="md-nav__link">Scale by a constant</a>
        </li>
        <li class="md-nav__item"><a href="#center-about-mean-and-scale-by-std" class="md-nav__link">Center about mean and scale by STD</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#normalization-layers" class="md-nav__link">Normalization Layers</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#batch-normalization" class="md-nav__link">Batch Normalization</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#conv2d-batch-normalization" class="md-nav__link">Conv2D + Batch Normalization</a>
        </li>
        <li class="md-nav__item"><a href="#fully-connected-batch-normalization" class="md-nav__link">Fully Connected + Batch Normalization</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#layernormalization" class="md-nav__link">LayerNormalization</a>
        </li></ul>
            </nav>
        </li></ul>
            </nav>
        </li>
      <script type="text/javascript" src=../../_static/js/apitoc.js></script>
  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">

          
          <div class="breadcrumbs md-typeset">
            <ul class="breadcrumb">
              <li></li>
              <li><a href="../../index.html"><i class="md-icon">home</i></a></li>
                <li><a href="../../docs/tutorials.html" accesskey="U">Tutorials</a></li>

              <li class="activate"><a>Model Quantization Tips</a></li>
            </ul>
          </div>
          

          <article class="md-content__inner md-typeset" role="main">
            
  <section id="model-quantization-tips">
<h1 id="mltk-tutorials-model-quantization-tips--page-root">Model Quantization Tips<a class="headerlink" href="#mltk-tutorials-model-quantization-tips--page-root" title="Permalink to this heading">¶</a></h1>
<p>This tutorial provides several tips for how to better quantize your model.</p>
<p>Model quantization uses the <a class="reference external" href="https://www.tensorflow.org/lite/models/convert">TfliteConverter</a> to convert a model with <strong>float32</strong> tensors into a model with <strong>int8</strong> tensors. A model with <strong>int8</strong> tensors typically reduces the memory requirements by 4x and also allows for better <a class="reference external" href="https://docs.silabs.com/gecko-platform/4.2/machine-learning/tensorflow/mvp-accelerator">hardware acceleration</a>.</p>
<p>While model quantization can greatly reduce memory and computational overhead, it can also reduce model accuracy. This is because converting from <strong>float32</strong> (32-bits) to <strong>int8</strong> (8-bits) looses information. So, for quantization to be effective, it must accurately represent the original 32-bit data in 8-bits as efficiently as possible. While the <a class="reference external" href="https://www.tensorflow.org/lite/models/convert">TfliteConverter</a> does most of the work for you, the following provides some tips on how you can help the model quantize even better.</p>
<section id="contents">
<h2 id="contents">Contents<a class="headerlink" href="#contents" title="Permalink to this heading">¶</a></h2>
<p>This tutorial is divided into the following sections:</p>
<ol class="arabic simple">
<li><p><a class="reference internal" href="#quantization-report"><span class="std std-doc">Quantization Report</span></a> - Describes how to use the MLTK to generate a quantization report to aid with debugging quantization errors</p></li>
<li><p><a class="reference internal" href="#input-data-normalization"><span class="std std-doc">Input Data Normalization</span></a> - Describes how to normalize the input data so that it can be better quantized</p></li>
<li><p><a class="reference internal" href="#normalization-layers"><span class="std std-doc">Normalization Layers</span></a> - Describes some of the ML layers that allow for better model quantization</p></li>
</ol>
</section>
<section id="quantization-report">
<h2 id="quantization-report">Quantization Report<a class="headerlink" href="#quantization-report" title="Permalink to this heading">¶</a></h2>
<p>Tensorflow-Lite comes with an experimental <a class="reference external" href="https://www.tensorflow.org/lite/performance/quantization_debugger">Quantization Debugger</a>:</p>
<blockquote>
<div><p>Although full-integer quantization provides improved model size and latency, the quantized model won’t always work as expected. It’s usually expected for the model quality (e.g. accuracy, mAP, WER) to be slightly lower than the original float model. However, there are cases where the model quality can go below your expectation or generated completely wrong results.</p>
<p>When this problem happens, it’s tricky and painful to spot the root cause of the quantization error, and it’s even more difficult to fix the quantization error. To assist this model inspection process, quantization debugger can be used to identify problematic layers, and selective quantization can leave those problematic layers in float so that the model accuracy can be recovered at the cost of reduced benefit from quantization.</p>
<p>Quantization debugger makes it possible to do quantization quality metric analysis in the existing model. Quantization debugger can automate processes for running model with a debug dataset, and collecting quantization quality metrics for each tensors.</p>
</div></blockquote>
<section id="enabling-the-quantization-report">
<h3 id="enabling-the-quantization-report">Enabling the Quantization Report<a class="headerlink" href="#enabling-the-quantization-report" title="Permalink to this heading">¶</a></h3>
<p>The MLTK will automatically generate a quantization report during <a class="reference external" href="../../docs/guides/model_quantization.html">model quantization</a> by setting the <a class="reference external" href="../../docs/python_api/mltk_model/train_mixin.html#mltk.core.TrainMixin.tflite_converter">tflite_converter</a> setting:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">my_model</span><span class="o">.</span><span class="n">tflite_converter</span><span class="p">[</span><span class="s1">'generate_quantization_report'</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<p>The quantization report is a standard <code class="docutils literal notranslate"><span class="pre">.csv</span></code> file and is added to the <a class="reference internal" href="../../docs/guides/model_archive.html"><span class="doc std std-doc">Model Archive</span></a> file (it is also generated in the same directory as the <code class="docutils literal notranslate"><span class="pre">.tflite</span></code>, typically <code class="docutils literal notranslate"><span class="pre">~/.mtlk/models/&lt;model</span> <span class="pre">name&gt;/quantization_report.csv</span></code>).</p>
</section>
<section id="analyzing-the-report">
<h3 id="analyzing-the-report">Analyzing the Report<a class="headerlink" href="#analyzing-the-report" title="Permalink to this heading">¶</a></h3>
<p>For each row in the report, the op name and index comes first, followed by quantization parameters and error metrics.</p>
<p>Additionally, per the <a class="reference external" href="https://www.tensorflow.org/lite/performance/quantization_debugger#step_3_data_analysis">Data Analysis</a> section, two additional metrics are calculated:</p>
<ul class="simple">
<li><p><strong>Range</strong> - <code class="docutils literal notranslate"><span class="pre">scale</span> <span class="pre">*</span> <span class="pre">255.0</span></code>, this provides the range of input values that the given layer should accept. Ideally, this value should be less than 255. Larger values could cause quantization problems at runtime. (Recall that the quantized values must fit within an <code class="docutils literal notranslate"><span class="pre">int8</span></code> data type)</p></li>
<li><p><strong>RMSE / scale</strong> - <code class="docutils literal notranslate"><span class="pre">sqrt(mean_squared_error)</span> <span class="pre">/</span> <span class="pre">scale</span></code>, this value is close to <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">sqrt(12)</span></code> (~ 0.289) when quantized distribution is similar to the original float distribution, indicating a good quantized model. The larger the value is, the more likely the layer is not being quantized well.</p></li>
</ul>
<p>So, when viewing a quantization report, if a layer has large values in the <code class="docutils literal notranslate"><span class="pre">range</span></code> and/or <code class="docutils literal notranslate"><span class="pre">rmse/scale</span></code> columns then it could be that the model is not quantizing well. Refer to the next sections for how to fix these issues.</p>
</section>
</section>
<section id="input-data-normalization">
<h2 id="input-data-normalization">Input Data Normalization<a class="headerlink" href="#input-data-normalization" title="Permalink to this heading">¶</a></h2>
<p>For the best quantization, the range of input values should be evenly distributed around -1.0 to 1.0. While this is not a hard rule, in practice is was found that <code class="docutils literal notranslate"><span class="pre">rmse/scale</span></code> has a lower value when the input values are within this range.</p>
<p>The following are methods to normalize the input data:</p>
<section id="scale-by-a-constant">
<h3 id="scale-by-a-constant">Scale by a constant<a class="headerlink" href="#scale-by-a-constant" title="Permalink to this heading">¶</a></h3>
<p>Scaling by a constant is a computationally efficient method for data normalization:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">normalized_input_sample</span> <span class="o">=</span> <span class="n">input_sample</span> <span class="o">/</span> <span class="o">&lt;</span><span class="n">scaler</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>In the training Python scripts, the following may be used:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># -----------------------------------------</span>
<span class="c1"># Define the input scaling value</span>
<span class="c1"># This value should be near the upper limit of the input data</span>
<span class="n">input_scaling_value</span> <span class="o">=</span> <span class="mf">255.0</span>

<span class="c1"># -----------------------------------------</span>
<span class="c1"># Add the scaling value to the model parameters.</span>
<span class="c1"># This allows the embedded device to access the scaling value at runtime</span>
<span class="c1"># Note, we save the scaler *reciprocal*, as multiplication is a more efficient op than division on embedded</span>
<span class="n">my_model</span><span class="o">.</span><span class="n">model_parameters</span><span class="p">[</span><span class="s1">'samplewise_norm.rescale'</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">input_scaling_value</span>

<span class="c1"># -----------------------------------------</span>
<span class="c1"># Ensure the input/output data types of the quantized model are float32</span>
<span class="n">my_model</span><span class="o">.</span><span class="n">tflite_converter</span><span class="p">[</span><span class="s1">'inference_input_type'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span>
<span class="n">my_model</span><span class="o">.</span><span class="n">tflite_converter</span><span class="p">[</span><span class="s1">'inference_output_type'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span>
<span class="c1"># Generate a quantization report to help with debugging quantization errors</span>
<span class="n">my_model</span><span class="o">.</span><span class="n">tflite_converter</span><span class="p">[</span><span class="s1">'generate_quantization_report'</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>


<span class="c1"># -----------------------------------------</span>
<span class="c1"># Later, in the training data pipeline, convert the data to float32 and scale the input data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x</span> <span class="o">/=</span> <span class="n">input_scaling_value</span>
</pre></div>
</div>
<p>At runtime on the embedded device, the input data must also be converted to float32 and scaled.
The following may be used:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">"tflite_micro_model/tflite_micro_model.hpp"</span>

<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">mltk</span><span class="p">;</span>

<span class="c1">// Assume the source input data is in uint16 format</span>
<span class="k">extern</span><span class="w"> </span><span class="kt">uint16_t</span><span class="w"> </span><span class="o">*</span><span class="n">source_input_data</span><span class="p">;</span>
<span class="c1">// This is defined by the build scripts</span>
<span class="c1">// which converts the specified .tflite to a C array</span>
<span class="k">extern</span><span class="w"> </span><span class="s">"C"</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">uint8_t</span><span class="w"> </span><span class="n">sl_tflite_model_array</span><span class="p">[];</span>


<span class="kt">void</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
<span class="w">    </span><span class="n">TfliteMicroModel</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Load the quantized .tflite model</span>
<span class="w">    </span><span class="n">model</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">sl_tflite_model_array</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Retrieve the input scaler from the .tflite</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">input_scaler</span><span class="p">;</span>
<span class="w">    </span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"samplewise_norm.rescale"</span><span class="p">,</span><span class="w"> </span><span class="n">input_scaler</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Obtain a pointer to the input tensor which is in float32 format</span>
<span class="w">    </span><span class="n">TfliteTensorView</span><span class="w"> </span><span class="o">*</span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">inputs</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// Scale the input data</span>
<span class="w">    </span><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">input</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">().</span><span class="n">flat_size</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="n">input</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">.</span><span class="n">f</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">source_input_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">input_scaler</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Run inference on the scaled input data</span>
<span class="w">    </span><span class="n">model</span><span class="p">.</span><span class="n">invoke</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// Do something with the results</span>
<span class="w">    </span><span class="n">TfliteTensorView</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">output</span><span class="p">();</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="center-about-mean-and-scale-by-std">
<h3 id="center-about-mean-and-scale-by-std">Center about mean and scale by STD<a class="headerlink" href="#center-about-mean-and-scale-by-std" title="Permalink to this heading">¶</a></h3>
<p>A more robust normalization method is to center the data about the mean and scale by the standard deviation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">normalized_input_sample</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_sample</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">input_sample</span><span class="p">))</span> <span class="o">/</span> <span class="n">std</span><span class="p">(</span><span class="n">input_sample</span><span class="p">)</span>
</pre></div>
</div>
<p>In the training Python scripts, the following may be used:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># -----------------------------------------</span>
<span class="c1"># This tells the embedded device to normalized by the mean and STD</span>
<span class="n">my_model</span><span class="o">.</span><span class="n">model_parameters</span><span class="p">[</span><span class="s1">'samplewise_norm.mean_and_std'</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># -----------------------------------------</span>
<span class="c1"># Ensure the input/output data types of the quantized model are float32</span>
<span class="n">my_model</span><span class="o">.</span><span class="n">tflite_converter</span><span class="p">[</span><span class="s1">'inference_input_type'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span>
<span class="n">my_model</span><span class="o">.</span><span class="n">tflite_converter</span><span class="p">[</span><span class="s1">'inference_output_type'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span>
<span class="c1"># Generate a quantization report to help with debugging quantization errors</span>
<span class="n">my_model</span><span class="o">.</span><span class="n">tflite_converter</span><span class="p">[</span><span class="s1">'generate_quantization_report'</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>


<span class="c1"># -----------------------------------------</span>
<span class="c1"># Later, in the training data pipeline, convert the data to float32 and normalize</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x</span> <span class="o">/=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>

</pre></div>
</div>
<p>At runtime on the embedded device, the input data must also be converted to float32 and normalized.
The following may be used:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">"tflite_micro_model/tflite_micro_model.hpp"</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">"tflite_micro_model/tflite_micro_utils.hpp"</span>

<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">mltk</span><span class="p">;</span>

<span class="c1">// Assume the source input data is in uint16 format</span>
<span class="k">extern</span><span class="w"> </span><span class="kt">uint16_t</span><span class="w"> </span><span class="o">*</span><span class="n">source_input_data</span><span class="p">;</span>
<span class="c1">// This is defined by the build scripts</span>
<span class="c1">// which converts the specified .tflite to a C array</span>
<span class="k">extern</span><span class="w"> </span><span class="s">"C"</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">uint8_t</span><span class="w"> </span><span class="n">sl_tflite_model_array</span><span class="p">[];</span>


<span class="kt">void</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
<span class="w">    </span><span class="n">TfliteMicroModel</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Load the quantized .tflite model</span>
<span class="w">    </span><span class="n">model</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">sl_tflite_model_array</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Retrieve the input scaler from the .tflite</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">mean_and_std_enabled</span><span class="p">;</span>
<span class="w">    </span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"samplewise_norm.mean_and_std"</span><span class="p">,</span><span class="w"> </span><span class="n">mean_and_std_enabled</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Obtain a pointer to the input tensor which is in float32 format</span>
<span class="w">    </span><span class="n">TfliteTensorView</span><span class="w"> </span><span class="o">*</span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">inputs</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// Use the helper function to normalize the input buffer</span>
<span class="w">    </span><span class="n">samplewise_mean_std_tensor</span><span class="p">(</span><span class="n">source_input_data</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">.</span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">().</span><span class="n">flat_size</span><span class="p">());</span>

<span class="w">    </span><span class="c1">// Run inference on the scaled input data</span>
<span class="w">    </span><span class="n">model</span><span class="p">.</span><span class="n">invoke</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// Do something with the results</span>
<span class="w">    </span><span class="n">TfliteTensorView</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">output</span><span class="p">();</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="normalization-layers">
<h2 id="normalization-layers">Normalization Layers<a class="headerlink" href="#normalization-layers" title="Permalink to this heading">¶</a></h2>
<p>For the best quantization, we want the input data to be evenly distributed around 0.0. The same is true for the inputs to each of the layers of the model.</p>
<p>To help with this, <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers">Tensorflow</a> / <a class="reference external" href="https://keras.io/api/layers/#normalization-layers">Keras</a> offer several layers that will normalize the outputs of the preceding layers:</p>
<section id="batch-normalization">
<h3 id="batch-normalization">Batch Normalization<a class="headerlink" href="#batch-normalization" title="Permalink to this heading">¶</a></h3>
<p>The <a class="reference external" href="https://keras.io/api/layers/normalization_layers/batch_normalization/">Batch Normalization</a> applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.</p>
<p>This layer is commonly used in many model architectures. Additionally, if used properly, it can be fused with other layers so that there is minimal runtime overhead.</p>
<p>The following are some examples of how to use the <a class="reference external" href="https://keras.io/api/layers/normalization_layers/batch_normalization/">Batch Normalization</a> layer so that it fused with other layers. In this way:</p>
<ol class="arabic simple">
<li><p>The input to the following layer is centered about 0</p></li>
<li><p>The qunatized layer is fused and introduces minimal runtime overhead on the embedded device</p></li>
</ol>
<p><strong>NOTE:</strong> The key for BatchNorm fusion is to invoke the activation <em>after</em> the BatchNorm layer.</p>
<section id="conv2d-batch-normalization">
<h4 id="conv2d-batch-normalization">Conv2D + Batch Normalization<a class="headerlink" href="#conv2d-batch-normalization" title="Permalink to this heading">¶</a></h4>
<p>The following provides an example of how to use batch normalization with a Conv2D layer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="c1"># Define the Conv2D layer *without* an activation</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
    <span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Apply batch normalization</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Apply the ReLU activation *after* the batch norm</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="fully-connected-batch-normalization">
<h4 id="fully-connected-batch-normalization">Fully Connected + Batch Normalization<a class="headerlink" href="#fully-connected-batch-normalization" title="Permalink to this heading">¶</a></h4>
<p>The following provides an example of how to use batch normalization with a Dense (aka Fully Connected) layer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="c1"># Define the Dense layer *without* an activation</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Apply batch normalization</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Apply the ReLU activation *after* the batch norm</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="layernormalization">
<h3 id="layernormalization">LayerNormalization<a class="headerlink" href="#layernormalization" title="Permalink to this heading">¶</a></h3>
<p><a class="reference external" href="https://keras.io/api/layers/normalization_layers/layer_normalization/">LayerNormalization</a> normalizes the activations of the previous layer for each given example in a batch independently, rather than across a batch like Batch Normalization. i.e. applies a transformation that maintains the mean activation within each example close to 0 and the activation standard deviation close to 1.</p>
<p>While this layer introduces more overhead than <a class="reference external" href="https://keras.io/api/layers/normalization_layers/batch_normalization/">Batch Normalization</a>, it is applied on a per-sample basis (as opposed to a per-batch like BatchNorm) which is useful for models that maintain a memory (e.g. recurrent networks).</p>
<p>This layer should be applied <em>before</em> the activation, e.g.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">n_classes</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


          </article>
        </div>
      </div>
      <a href="#" class="go-top"><i class="md-icon">arrow_upward</i>Back to Top</a>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
            <a href="synthetic_audio_dataset_generation.html" title="Synthetic Audio Dataset Generation"
               class="md-flex md-footer-nav__link md-footer-nav__link--prev"
               rel="prev">
              <div class="md-flex__cell md-flex__cell--shrink">
                <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
              </div>
              <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
                <span class="md-flex__ellipsis">
                  <span
                      class="md-footer-nav__direction"> Previous </span> Synthetic Audio Dataset Generation </span>
              </div>
            </a>
          
          
            <a href="quantized_lstm.html" title="Quantized LSTM"
               class="md-flex md-footer-nav__link md-footer-nav__link--next"
               rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span
                class="md-flex__ellipsis"> <span
                class="md-footer-nav__direction"> Next </span> Quantized LSTM </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink"><i
                class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2025, Silicon Labs.
              
          </div>
            Last updated on
              Jan 30, 2025.
            <br/>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <div class="privacy-banner">
    <div class="privacy-banner-wrapper">
      <p>
        <b>Important:</b> We use cookies only for functional and traffic analytics. <br />
        We DO NOT use cookies for any marketing purposes. By using our site you acknowledge you have read and understood our <a class="privacy-policy" href="https://www.silabs.com/about-us/legal/cookie-policy" target="_blank">Cookie Policy</a>.
      </p>
      <a class="privacy-banner-accept" href="#">Got it</a>
    </div>
</div>
  <script src="../../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>